<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 45]
- [stat.ML](#stat.ML) [Total: 6]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [ASCIIBench: Evaluating Language-Model-Based Understanding of Visually-Oriented Text](https://arxiv.org/abs/2512.04125)
*Kerry Luo,Michael Fu,Joshua Peguero,Husnain Malik,Anvay Patil,Joyce Lin,Megan Van Overborg,Ryan Sarmiento,Kevin Zhu*

Main category: cs.LG

TL;DR: ASCIIBench提出一个包含5,315个带标签的ASCII图像的数据集，并提供一个适配ASCII结构的CLIP微调模型，用以评估LLMs生成的ASCII艺术。结果发现基于CLIP嵌入的余弦相似度对大多数ASCII类别难以区分，表现接近随机；仅在内部均值相似度较高的类别上具有可辨度，表明瓶颈在表示而非生成方差。这一工作将ASCII艺术作为多模态表征的压力测试，推动开发针对符号化视觉模态的新嵌入与评估指标。资源见GitHub。


<details>
  <summary>Details</summary>
Motivation: 探测大规模语言模型在需要精确空间/位置推理的任务上的局限性；通过符号化的ASCII艺术作为一个独特的视觉模态来评估和挑战多模态嵌入能力；并寻找更适合符号化视觉模态的嵌入与评估方法。

Method: 构建ASCIIBench数据集，筛选并标注5,315张ASCII图像；发布一个微调后的、适应ASCII结构的CLIP模型，用以评估LLM生成的ASCII艺术；通过对比嵌入向量的余弦相似度，评估不同类别的可分离性，并分析生成变异性与表征瓶颈之间的关系。

Result: 大多数ASCII类别的CLIP嵌入余弦相似度接近随机猜测，表现为机会水平；但在内部均值相似度较高的类别上呈现明确的可区分性，显示主要瓶颈在于表示层而非生成方差。

Conclusion: ASCII艺术可作为多模态表示的压力测试，促使开发专门针对符号化视觉模态的嵌入和评估指标；并提供资源以支持后续研究。

Abstract: Large language models (LLMs) have demonstrated several emergent behaviors with scale, including reasoning and fluency in long-form text generation. However, they continue to struggle with tasks requiring precise spatial and positional reasoning. ASCII art, a symbolic medium where characters encode structure and form, provides a unique probe of this limitation. We introduce ASCIIBench, a novel benchmark for evaluating both the generation and classification of ASCII-text images. ASCIIBench consists of a filtered dataset of 5,315 class-labeled ASCII images and is, to our knowledge, the first publicly available benchmark of its kind. Alongside the dataset, we release weights for a fine-tuned CLIP model adapted to capture ASCII structure, enabling the evaluation of LLM-generated ASCII art. Our analysis shows that cosine similarity over CLIP embeddings fails to separate most ASCII categories, yielding chance-level performance even for low-variance classes. In contrast, classes with high internal mean similarity exhibit clear discriminability, revealing that the bottleneck lies in representation rather than generational variance. These findings position ASCII art as a stress test for multimodal representations and motivate the development of new embedding methods or evaluation metrics tailored to symbolic visual modalities. All resources are available at https://github.com/ASCIIBench/ASCIIBench.

</details>


### [2] [Decoding Large Language Diffusion Models with Foreseeing Movement](https://arxiv.org/abs/2512.04135)
*Yichuan Mo,Quan Chen,Mingjie Li,Zeming Wei,Yisen Wang*

Main category: cs.LG

TL;DR: 提出基于搜索的前瞻解码方法FDM及其加速变体FDM-A，解决LLDM解码中局部与全局影响的冲突问题，通过离散空间的优化实现更高效的解码。


<details>
  <summary>Details</summary>
Motivation: LLDMs的灵活解码提高并行推理与可控性，但解码顺序对性能影响显著，现有启发式方法多关注局部效应，忽略长期影响，需一套同时考虑局部与全局的系统优化方法。

Method: 提出FDM，结合局部与全局考量，采用基于搜索的策略在离散空间中进行有效优化；通过分析在完整解码过程中的token一致性，提出FDM-A，在关键步骤（探查与平衡）进行深度探索的限制以提升效率。

Result: 在多种基准与模型架构上进行广泛实验，验证FDM的可扩展性，并显示FDM-A在效率-性能权衡上的优越性。

Conclusion: 本文为LLDM的解码方法提供一个有原则性的推进方向，提升解码的灵活性与性能。

Abstract: Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance becomes highly sensitive to the decoding order of tokens. Existing heuristic methods, however, focus mainly on local effects while overlooking long-term impacts. To address this limitation, we propose the Foreseeing Decoding Method (FDM), a novel approach that integrates both local and global considerations to unlock the full potential, employing a search-based strategy to enable effective optimization in discrete spaces. Furthermore, by analyzing the consistency of chosen tokens in the full decoding process, we develop a variant, FDM with Acceleration (FDM-A), which restricts deep exploration to critical steps identified as the exploration and balance circumantences. Extensive experiments across diverse benchmarks and model architectures validate the scalability of FDM and demonstrate the superior efficiency-performance trade-off achieved by FDM-A. Our work might potentially provide a principled step toward more powerful decoding methods for LLDMs.

</details>


### [3] [MechDetect: Detecting Data-Dependent Errors](https://arxiv.org/abs/2512.04138)
*Philipp Jung,Nicholas Chandler,Sebastian Jäger,Felix Biessmann*

Main category: cs.LG

TL;DR: MechDetect estimates whether data errors depend on the data to infer error-generation mechanisms, extending missing-value mechanism detection to other error types via an error mask and validated on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Understanding how errors are generated helps trace and fix data quality problems. Merely detecting errors does not reveal their root causes; knowing the mechanism aids remediation.

Method: Given a tabular dataset and an error mask, train machine learning models to predict error occurrence from the data and test whether errors exhibit data-dependency. Builds on literature for missing-value mechanisms and generalizes to other error types with an error mask.

Result: Empirical experiments on benchmark datasets show that MechDetect can identify data-dependent error mechanisms and is adaptable to different error types when an error mask is available.

Conclusion: MechDetect offers a simple, extensible approach to uncover error-generation mechanisms, enabling targeted data-cleaning and more robust data-quality pipelines across diverse error types.

Abstract: Data quality monitoring is a core challenge in modern information processing systems. While many approaches to detect data errors or shifts have been proposed, few studies investigate the mechanisms governing error generation. We argue that knowing how errors were generated can be key to tracing and fixing them. In this study, we build on existing work in the statistics literature on missing values and propose MechDetect, a simple algorithm to investigate error generation mechanisms. Given a tabular data set and a corresponding error mask, the algorithm estimates whether or not the errors depend on the data using machine learning models. Our work extends established approaches to detect mechanisms underlying missing values and can be readily applied to other error types, provided that an error mask is available. We demonstrate the effectiveness of MechDetect in experiments on established benchmark datasets.

</details>


### [4] [ActVAE: Modelling human activity schedules with a deep conditional generative approach](https://arxiv.org/abs/2512.04223)
*Fred Shone,Tim Hillel*

Main category: cs.LG

TL;DR: 提出基于条件变分自编码器的深度生成模型，用于在输入标签下快速生成现实的日程，并与其他模型对比验证有效性。


<details>
  <summary>Details</summary>
Motivation: 人类活动调度行为高度复杂且具有随机性，需要可控且多样性的生成模型以提高需求建模的准确性和灵活性。

Method: 提出一种将结构化潜在变量模型与条件学习相结合的 Conditional VAE；输入年龄、就业状况等标签，生成日程；通过联合密度估计框架评估，并进行多案例分析；比较与纯生成模型、纯条件模型的表现。

Result: 模型能够在给定标签下快速生成精确且接近真实的活动日程；在联合密度估计和案例研究中表现良好；具备实际数据与计算资源需求，易于在新旧需求建模框架中部署；强调随机性建模的重要性。

Conclusion: 将条件生成与结构化潜在变量结合的CVAE在建模复杂人类行为方面优于单一方向的模型，提供了可控且多样的生成能力，适用于需求建模领域。

Abstract: Modelling the complexity and diversity of human activity scheduling behaviour is inherently challenging. We demonstrate a deep conditional-generative machine learning approach for the modelling of realistic activity schedules depending on input labels such as an individual's age, employment status, or other information relevant to their scheduling. We combine (i) a structured latent generative approach, with (ii) a conditional approach, through a novel Conditional VAE architecture. This allows for the rapid generation of precise and realistic schedules for different input labels. We extensively evaluate model capabilities using a joint density estimation framework and several case studies. We additionally show that our approach has practical data and computational requirements, and can be deployed within new and existing demand modelling frameworks. We evaluate the importance of generative capability more generally, by comparing our combined approach to (i) a purely generative model without conditionality, and (ii) a purely conditional model which outputs the most likely schedule given the input labels. This comparison highlights the usefulness of explicitly modelling the randomness of complex and diverse human behaviours using deep generative approaches.

</details>


### [5] [Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness](https://arxiv.org/abs/2512.04264)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Jing Lin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we study the ML model robustness using ten different activation functions through adversarial training in centralized environments and explore the ML model robustness in federal learning environments. In the centralized environment, we first propose an advanced adversarial training approach to improving the ML model robustness by incorporating model architecture change, soft labeling, simplified data augmentation, and varying learning rates. Then, we conduct extensive experiments on ten well-known activation functions in addition to ReLU to better understand how they impact the ML model robustness. Furthermore, we extend the proposed adversarial training approach to the federal learning environment, where both independent and identically distributed (IID) and non-IID data settings are considered. Our proposed centralized adversarial training approach achieves a natural and robust accuracy of 77.08% and 67.96%, respectively on CIFAR-10 against the fast gradient sign attacks. Experiments on ten activation functions reveal ReLU usually performs best. In the federated learning environment, however, the robust accuracy decreases significantly, especially on non-IID data. To address the significant performance drop in the non-IID data case, we introduce data sharing and achieve the natural and robust accuracy of 70.09% and 54.79%, respectively, surpassing the CalFAT algorithm, when 40% data sharing is used. That is, a proper percentage of data sharing can significantly improve the ML model robustness, which is useful to some real-world applications.

</details>


### [6] [The Initialization Determines Whether In-Context Learning Is Gradient Descent](https://arxiv.org/abs/2512.04268)
*Shifeng Xie,Rui Yuan,Simone Rossi,Thomas Hannagan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions with zero-mean Gaussian priors and zero initialization for GD. However, subsequent studies have challenged this simplified view by highlighting its overly restrictive assumptions, demonstrating instead that under conditions such as multi-layer or nonlinear attention, self-attention performs optimization-like inference, akin to but distinct from GD. We investigate how multi-head LSA approximates GD under more realistic conditions specifically when incorporating non-zero Gaussian prior means in linear regression formulations of ICL. We first extend multi-head LSA embedding matrix by introducing an initial estimation of the query, referred to as the initial guess. We prove an upper bound on the number of heads needed for ICL linear regression setup. Our experiments confirm this result and further observe that a performance gap between one-step GD and multi-head LSA persists. To address this gap, we introduce yq-LSA, a simple generalization of single-head LSA with a trainable initial guess yq. We theoretically establish the capabilities of yq-LSA and provide experimental validation on linear regression tasks, thereby extending the theory that bridges ICL and GD. Finally, inspired by our findings in the case of linear regression, we consider widespread LLMs augmented with initial guess capabilities, and show that their performance is improved on a semantic similarity task.

</details>


### [7] [Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order](https://arxiv.org/abs/2512.04277)
*Prakhar Gupta,Vaibhav Gupta*

Main category: cs.LG

TL;DR: 通过混合奖励的强化学习后训练，在数独任务中将求解顺序的粗粒度信号引入，提高模型在随机顺序求解时的表现，接近使用求解器顺序时的性能。


<details>
  <summary>Details</summary>
Motivation: 探究是否仅使用一个标量目标在 RL 的后训练阶段就能利用粗粒度的求解顺序信号来改善性能，即便监督数据是随机顺序的。

Method: 在数独任务上，先对 Transformer 进行标准的随机顺序微调，然后用 Group Relative Policy Optimization (GRPO) 进行后训练，使用两种奖励：单元格准确度和一个排序奖励，其在模型的输出顺序与求解器给定的顺序对齐时增强。为了比较信号，通过固定混合方式将两者合并，并在初始化时通过自举缩放来对齐两者的量纲。

Result: 混合奖励通常优于仅用单元格目标的优化；最佳混合在测试集上的准确度显著高于仅在随机顺序微调的模型，且接近在求解器顺序上微调的模型的准确度。

Conclusion: 粗粒度的排序信号可以在不改变监督数据或模型结构的前提下，引导 RL 后训练朝向求解器顺序的轨迹。

Abstract: Post-training with reinforcement learning (RL) typically optimizes a single scalar objective and ignores structure in how solutions are produced. We ask whether a scalar hint toward a canonical solver ordering, used only during RL post-training, improves performance even when fine-tuned on randomized solution sequences. On Sudoku, we train a Transformer with standard fine-tuning on randomized solving orders, then post-train it with Group Relative Policy Optimization (GRPO) with two rewards: cell accuracy and an ordering reward that increases when the model's emission order aligns with the solver order. To compare signals cleanly, we combine them via fixed mixtures and use a simple bootstrapped scaling to equalize component magnitudes at initialization. Mixed rewards generally outperform cell-only optimization--the best mixture yields substantially higher test accuracy than the fine-tuned-only model trained on random-order and approaches the fine-tuned-only model trained on solver-order sequences in accuracy. These results suggest that coarse ordering signals can steer RL post-training toward solver-order trajectories without modifying supervised data or architecture.

</details>


### [8] [GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers](https://arxiv.org/abs/2512.04296)
*Malyaban Bal,Abhronil Sengupta*

Main category: cs.LG

TL;DR: GRASP introduces grouped activation parameterization for PEFT, achieving substantial parameter savings; StochGRASP extends to probabilistic weight perturbations for robustness on noisy edge hardware. Both perform competitively on GLUE and E2E NLG with fewer trainable params than LoRA/BitFit.


<details>
  <summary>Details</summary>
Motivation: The need to reduce trainable parameters in fine-tuning large pre-trained models while maintaining performance; address hardware variability and energy efficiency for edge deployment.

Method: Split token representations in selected layers into K groups; apply shared per-group scaling and shifting (grouped modulation) to achieve parameter efficiency. StochGRASP models perturbations as Gaussian distributions over pre-trained weights with a noise-aware loss to capture hardware variability.

Result: GRASP matches or surpasses established PEFT methods on GLUE (RoBERTa-base/large) and E2E NLG (GPT-2 Medium) with an order of magnitude fewer trainable parameters than LoRA and BitFit. StochGRASP improves robustness under noise compared to deterministic variants.

Conclusion: GRASP and StochGRASP offer effective, compact fine-tuning with robustness benefits, suitable for energy-efficient edge AI hardware; stochastic variant further enhances noise resilience.

Abstract: Parameter-efficient fine-tuning (PEFT) provides a scalable alternative to full-model adaptation by updating only a small subset of parameters in large pre-trained models. We introduce GRASP - GRouped Activation Shared Parameterization - a lightweight PEFT framework that partitions the D-dimensional token representations of selected layers into K << D groups and learns a shared scaling and shifting vector for each group. This grouped modulation reduces the number of trainable parameters significantly while preserving the ability of the model to learn task-specific features. Building on this formulation, we further propose StochGRASP, which learns Gaussian distributions as perturbations to the pre-trained weights rather than deterministic values. This probabilistic parameterization along with a noise-aware loss function formulation enables modelling hardware-level variability in programmed weights and significantly improves robustness under non-ideal inference conditions-an important requirement for deployment on edge-based emerging AI hardware. Across GLUE (RoBERTa-base & RoBERTa-large) and E2E NLG (GPT-2 Medium), GRASP matches or exceeds the performance of established PEFT methods while achieving an order of magnitude reduction in trainable parameters compared to LoRA and BitFit. Under varying levels of noise, StochGRASP consistently outperforms deterministic variants, demonstrating its suitability for energy-efficient and noise-prone hardware platforms.

</details>


### [9] [GraphBench: Next-generation graph learning benchmarking](https://arxiv.org/abs/2512.04475)
*Timo Stoll,Chendi Qian,Ben Finkelshtein,Ali Parviz,Darius Weber,Fabrizio Frasca,Hadar Shavit,Antoine Siraudin,Arman Mielke,Marie Anastacio,Erik Müller,Maya Bechler-Speicher,Michael Bronstein,Mikhail Galkin,Holger Hoos,Mathias Niepert,Bryan Perozzi,Jan Tönshoff,Christopher Morris*

Main category: cs.LG

TL;DR: GraphBench is a comprehensive graph ML benchmarking suite addressing fragmentation by offering standardized evaluation protocols, a unified hyperparameter tuning framework, and benchmarks across node/edge/graph/generative tasks, with baselines using MPNNs and graph transformers.


<details>
  <summary>Details</summary>
Motivation: Benchmarking in graph ML is fragmented across domains and tasks, leading to poor reproducibility and slow progress. A unified, cross-domain benchmark with consistent splits, metrics, and tuning aims to enable fair comparisons and reproducibility.

Method: Introduce GraphBench, a suite spanning node-level, edge-level, graph-level, and generative tasks across diverse domains, with standardized evaluation protocols (dataset splits, OOD metrics) and a unified hyperparameter tuning framework; benchmarked using message-passing neural networks and graph transformer models to establish baselines.

Result: Provision of standardized evaluation protocols, consistent dataset splits, OOD-aware metrics, a unified hyperparameter tuning framework, and principled baselines using MPNNs and graph transformers; establishes reference performance for cross-domain graph ML benchmarking.

Conclusion: GraphBench advances graph ML benchmarking by providing a cross-domain, reproducible, and fair evaluation suite with standardized protocols and tunable baselines, enabling more robust progress across diverse graph tasks.

Abstract: Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.

</details>


### [10] [RNNs perform task computations by dynamically warping neural representations](https://arxiv.org/abs/2512.04310)
*Arthur Pellegrino,Angus Chadwick*

Main category: cs.LG

TL;DR: 提出将RNN的计算视为对输入流的动态重塑，并建立一个罗氏流形框架，从输入数据的流形出发推导系统的流形拓扑与几何，结果表明动态重塑是RNN计算的基本特征。


<details>
  <summary>Details</summary>
Motivation: 试图弥合时间变化输入下的计算过程与神经表示的几何结构之间的关系，解释为何和如何在动态过程中图片数据的表示被变形以完成任务。

Method: 构建一个Riemannian几何框架，使其能够从输入数据的流形推导出动态系统的流形拓扑与几何，并对RNN的时变几何进行表征，以揭示动态重塑在计算中的作用。

Result: 理论上证明并描述了动态重塑作为RNN计算的核心特征，提供了从输入流形推导出系统流形几何的途径。

Conclusion: 此框架将时间动态与表示几何联系起来，为理解、分析和设计RNN提供几何层面的洞见。

Abstract: Analysing how neural networks represent data features in their activations can help interpret how they perform tasks. Hence, a long line of work has focused on mathematically characterising the geometry of such "neural representations." In parallel, machine learning has seen a surge of interest in understanding how dynamical systems perform computations on time-varying input data. Yet, the link between computation-through-dynamics and representational geometry remains poorly understood. Here, we hypothesise that recurrent neural networks (RNNs) perform computations by dynamically warping their representations of task variables. To test this hypothesis, we develop a Riemannian geometric framework that enables the derivation of the manifold topology and geometry of a dynamical system from the manifold of its inputs. By characterising the time-varying geometry of RNNs, we show that dynamic warping is a fundamental feature of their computations.

</details>


### [11] [Data-regularized Reinforcement Learning for Diffusion Models at Scale](https://arxiv.org/abs/2512.04332)
*Haotian Ye,Kaiwen Zheng,Jiashu Xu,Puheng Li,Huayu Chen,Jiaqi Han,Sheng Liu,Qinsheng Zhang,Hanzi Mao,Zekun Hao,Prithvijit Chattopadhyay,Dinghao Yang,Liang Feng,Maosheng Liao,Junjie Bai,Ming-Yu Liu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: DDRL通过前向KL将策略锚定在离线数据分布，解决扩散模型强化学习中的奖励劫持问题，实现更高的人类偏好并提供稳定、可扩展的后训练范式。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在对人类偏好进行强化学习时容易被奖励劫持，且正则化的惩罚不可靠，导致质量下降、风格过强或多样性降低等问题，需要一种鲁棒、无偏的 RL 融合方法来对齐人类偏好。

Method: 提出数据正则化扩散强化学习（DDRL）：使用前向KL散度将策略锚定在离线数据分布，并将 RL 目标与扩散训练损失结合，同时提供理论分析证明其鲁棒性和无偏性。实证上给出一个简单而有效的算法框架。

Result: 在高分辨率视频生成任务上，DDRL显著提升奖励并缓解 baselines 的奖励劫持；通过超过百万小时GPU训练和一万次双盲人类评估，达到最高的人类偏好评价，验证其鲁棒性与可扩展性。

Conclusion: DDRL为扩散模型的后训练提供了一个鲁棒、可扩展且无偏的融合 RL 的范式，能在避免奖励劫持的同时提升人类偏好。

Abstract: Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.

</details>


### [12] [Distance Is All You Need: Radial Dispersion for Uncertainty Estimation in Large Language Models](https://arxiv.org/abs/2512.04351)
*Manh Nguyen,Sunil Gupta,Hung Le*

Main category: cs.LG

TL;DR: 提出了Radial Dispersion Score (RDS)，一种简单、无参数、与模型无关的未确定性度量，通过嵌入空间中采样文本的径向离散度来衡量；还提供带权变体，结合模型的逐词概率；能够对单样本评分，可用于最佳N选择和置信度筛选。在四个挑战性自由文本问答数据集及多种LLM上，RDS在幻觉检测和答案选择任务上达到最先进水平，且对采样数量和嵌入模型具有鲁棒性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法过于复杂、易碎的语义聚类或内部状态依赖问题，提出简单、无参数、全模型无关的未确定性指标。

Method: 在嵌入空间对采样生成进行径向分散度计算，得到RDS；可选加入基于模型输出的每个token概率的加权变体。支持对单样本的分数化，如Best-of-N和置信度筛选等应用。

Result: 在四个自由格式问答数据集和多种LLM上，RDS及其变体超过九个强基线，在幻觉检测和答案选择方面达到state-of-the-art，且对嵌入选择和样本数量具有鲁棒性和可扩展性。

Conclusion: RDS为未确定性评估提供了简洁、有效且可扩展的解决方案，适用于逐样本评分及多任务场景，具备良好泛化性。

Abstract: Detecting when large language models (LLMs) are uncertain is critical for building reliable systems, yet existing methods are overly complicated, relying on brittle semantic clustering or internal states. We introduce \textbf{Radial Dispersion Score (RDS)}, a simple, parameter-free, fully model-agnostic uncertainty metric that measures the radial dispersion of sampled generations in embedding space. A lightweight probability-weighted variant further incorporates the model's own token probabilities when available, outperforming different nine strong baselines. Moroever, RDS naturally extends to per-sample scoring, enabling applications such as best-of-$N$ selection and confidence-based filtering. Across four challenging free-form QA datasets and multiple LLMs, our metrics achieve state-of-the-art hallucination detection and answer selection performance, while remaining robust and scalable with respect to sample size and embedding choice.

</details>


### [13] [SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction](https://arxiv.org/abs/2512.04354)
*April S. Liang,Fatemeh Amrollahi,Yixing Jiang,Conor K. Corbin,Grace Y. E. Kim,David Mui,Trevor Crowell,Aakash Acharya,Sreedevi Mony,Soumya Punnathanam,Jack McKeown,Margaret Smith,Steven Lin,Arnold Milstein,Kevin Schulman,Jason Hom,Michael A. Pfeffer,Tho D. Pham,David Svec,Weihan Chu,Lisa Shieh,Christopher Sharp,Stephen P. Ma,Jonathan H. Chen*

Main category: cs.LG

TL;DR: ML驱动的CDS系统SmartAlert在住院环境中通过预测CBC结果的稳定性来减少重复检测；在随机对照试点中覆盖8个单位、2家医院、9270次入院，重复检测实现约15%相对降低，安全性无负面影响。


<details>
  <summary>Details</summary>
Motivation: 重复的实验室检测普遍存在，增加患者负担和医疗成本；现有教育、反馈和通用性规定效果有限，需更精准且安全地减少无效检测。

Method: 在电子健康记录(EHR)中集成基于机器学习的SmartAlert系统，以预测稳定的CBC结果并据此减少不必要的重复检测。研究在8个急性病区、2家医院、共9270例入院的随机对照试点（2024-08-15至2025-03-15）实施。

Result: 在52小时内的CBC检测次数显著下降（1.54 vs 1.82，p<0.01），实现约15%的相对减少；对二级安全结局未见不良影响。

Conclusion: 以稳健的实施与治理流程支撑的ML驱动CDS可为住院患者的实验室检测提供精准指引，安全降低不必要的重复检测。

Abstract: Repetitive laboratory testing unlikely to yield clinically useful information is a common practice that burdens patients and increases healthcare costs. Education and feedback interventions have limited success, while general test ordering restrictions and electronic alerts impede appropriate clinical care. We introduce and evaluate SmartAlert, a machine learning (ML)-driven clinical decision support (CDS) system integrated into the electronic health record that predicts stable laboratory results to reduce unnecessary repeat testing. This case study describes the implementation process, challenges, and lessons learned from deploying SmartAlert targeting complete blood count (CBC) utilization in a randomized controlled pilot across 9270 admissions in eight acute care units across two hospitals between August 15, 2024, and March 15, 2025. Results show significant decrease in number of CBC results within 52 hours of SmartAlert display (1.54 vs 1.82, p <0.01) without adverse effect on secondary safety outcomes, representing a 15% relative reduction in repetitive testing. Implementation lessons learned include interpretation of probabilistic model predictions in clinical contexts, stakeholder engagement to define acceptable model behavior, governance processes for deploying a complex model in a clinical environment, user interface design considerations, alignment with clinical operational priorities, and the value of qualitative feedback from end users. In conclusion, a machine learning-driven CDS system backed by a deliberate implementation and governance process can provide precision guidance on inpatient laboratory testing to safely reduce unnecessary repetitive testing.

</details>


### [14] [STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting](https://arxiv.org/abs/2512.04385)
*Nan Zhou,Weijie Hong,Huandong Wang,Jianfeng Zheng,Qiuhua Wang,Yali Song,Xiao-Ping Zhang,Yong Li,Xinlei Chen*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的时空物理信息预测框架STeP-Diff，通过DeepONet建模空间序列，并结合PDE约束的扩散过程，实现对来自不完整移动传感数据的空气污染场的预测。


<details>
  <summary>Details</summary>
Motivation: 移动传感设备在城市环境中的随机移动导致数据高度不完整且时序不一致，需在物理规律约束下进行高分辨率的时空污染预测。

Method: 在扩散模型的反向过程中探索训练模式；利用DeepONet建模空间测量序列；结合PDE约束的正则化，使去噪过程渐近收敛到对流-扩散动力学；在两座城市、14天、59台自研传感设备数据上进行评估。

Result: 相较于第二优算法，STeP-Diff在MAE、RMSE、MAPE上分别提升至最大89.12%、82.30%、25.00%，有效地捕捉时空相关性。

Conclusion: 将观测数据与物理规律相结合，能在不完整的移动传感数据条件下实现高精度的时空污染预测。

Abstract: Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.

</details>


### [15] [Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems](https://arxiv.org/abs/2512.04476)
*Zehao Fan,Zhenyu Liu,Yunzhen Liu,Yayue Hou,Hadjer Benmeziane,Kaoutar El Maghraoui,Liu Liu*

Main category: cs.LG

TL;DR: Proposes a context-aware MoE system that uses CXL-NDP offload and per-expert mixed-precision quantization to keep hot experts in GPU while offloading others, achieving significant throughput gains with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: MoE models grow in parameter count so large that expert weights exceed GPU memory, causing costly parameter movement when offloading to external memory. Reducing parameter movement by using near-data processing and by directing work based on activation context can improve throughput.

Method: 1) Use prefill-stage activation statistics to guide decoding-stage expert placement. 2) Dynamically pin hot experts in GPU HBM and map the remainder to CXL-NDP. 3) Apply context-aware mixed-precision quantization with per-expert bitwidths (1-4 bits) based on prefill data. 4) Overlap GPU and NDP execution to hide data movement costs.

Result: Achieves up to 8.7× decoding throughput improvement over state-of-the-art with only ~0.13% average accuracy drop on a GPU-NDP system.

Conclusion: Context-aware MoE with CXL-NDP offload and per-expert quantization effectively reduces parameter movement and overlapping compute and movement yields substantial throughput gains with minimal accuracy loss.

Abstract: Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.

</details>


### [16] [Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval](https://arxiv.org/abs/2512.04524)
*Tianle Hu,Weijun Lv,Na Han,Xiaozhao Fang,Jie Wen,Jiaxing Li,Guoxu Zhou*

Main category: cs.LG

TL;DR: 提出一种两阶段原型驱动的域自适应检索框架PSCA，通过学习正交原型实现类级语义对齐并利用几何距离评估伪标签可靠性，基于重建特征进行量化以提升哈希码质量，随后在第二阶段通过域特定量化函数生成跨域统一二值哈希码。


<details>
  <summary>Details</summary>
Motivation: 现有域自适应检索在类级语义对齐不足、过度追求样本两两对齐、缺乏伪标签可靠性与几何指导、直接对原始特征量化导致哈希码质量下降等问题。需要通过类级对齐、伪标签可靠性评估与重建特征的量化来提升跨域检索性能。

Method: 两阶段框架：第一阶段学习一组正交原型以建立类级语义连接，最大化类间可分性、压缩类内样本，并通过几何邻近度作为伪标签可靠性指示对伪标签进行自适应加权；得到的成员矩阵和原型用于特征重建，使量化发生在重建特征上以提升哈希码质量。第二阶段在重建特征上采用域特定的量化函数，并在互近似约束下生成跨域统一的二值哈希码。

Result: 大量实验在多个数据集上验证了PSCA的优越性能，具有显著的改进。

Conclusion: 通过原型驱动的类级语义对齐与基于重建特征的量化，PSCA有效缓解域偏移并提升跨域检索的哈希码质量，实现两阶段的无缝连接。

Abstract: Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.

</details>


### [17] [Explainable Graph Representation Learning via Graph Pattern Analysis](https://arxiv.org/abs/2512.04530)
*Xudong Wang,Ziheng Sun,Chris Ding,Jicong Fan*

Main category: cs.LG

TL;DR: 提出 PXGL-GNN 框架，通过图模式分析实现可解释的图表示学习，以子结构模式的计分和加权组合来解释图表示的组成与贡献。


<details>
  <summary>Details</summary>
Motivation: 解释性图表示学习在图数据中的空白点：许多工作聚焦模型级或实例级解释，而缺乏对表示层面的可解释性研究。本工作试图回答：图表示捕获了哪些具体信息？以及各模式对表示的贡献如何？

Method: 从图中按不同模式抽取子结构，学习这些模式的向量表示，并通过加权求和将它们组合成全局图表示，权重揭示各模式的重要性；方法受图核思想启发，考虑模式计数及特征信息的结合；提供鲁棒性和泛化性等理论分析；在真实数据上进行监督与无监督任务的对比实验。

Result: 理论部分给出鲁棒性与泛化性的分析结果；实验证明通过模式分析可以有效学习并解释图表示，且在多项任务上优于若干 baselines。

Conclusion: 通过基于子结构模式的表示级解释框架 PXGL-GNN，提升对图表示组成信息的理解与解释能力，兼具性能与可解释性；未来工作可扩展更多模式、进一步提升鲁棒性与规模化能力。

Abstract: Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.

</details>


### [18] [On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference](https://arxiv.org/abs/2512.04558)
*Yue Yu,Qiwei Di,Quanquan Gu,Dongruo Zhou*

Main category: cs.LG

TL;DR: BoN在理论与实验上存在固有局限，提出基于奖励筛选的序贯推理以聚焦高质量候选，理论上提供更强保证，实证上实现持续改进。


<details>
  <summary>Details</summary>
Motivation: 揭示测试时推理（TTC）在现有方法上的潜在极限，推动从标准BoN/序贯修订走向更接近最优前沿的策略。

Method: 建立混合参考策略模型来分析BoN的局限；提出奖励筛选的序贯推理（reward-filtered sequential inference），仅将高奖励的生成纳入上下文，以将计算聚焦在更优策略候选上；从理论上比较新方法与标准TTC的保证；在多样基准上进行实验验证。

Result: 理论上证明奖励筛选的序贯推理提供严格优于标准TTC的保证；实验证明在多项基准上相对于广泛使用的方法具有一致的改进，显示了框架的实用有效性。

Conclusion: 通过将计算聚焦于高质量候选并抑制低质量输出，奖励筛选的序贯推理接近最优边界，提升TTC性能且具备良好的实际应用潜力。

Abstract: Test-time compute (TTC) has become an increasingly prominent paradigm for enhancing large language models (LLMs). Despite the empirical success of methods such as best-of-$n$ (BoN) sampling and sequential revision, their fundamental limits remain unclear. We address this gap by analyzing a mixture-of-reference policy model and proving that standard BoN is inherently suboptimal. To move closer to the optimal frontier, we study reward-filtered sequential inference, a simple procedure that selectively incorporates only high-reward generations into the context. This mechanism concentrates computation on superior policy candidates and suppresses inferior ones. On the theoretical side, we show that reward-filtered sequential inference yields strictly stronger guarantees than standard TTC paradigms. On the empirical side, we evaluate such an inference strategy across diverse benchmarks and observe consistent improvements over widely used approaches, demonstrating the practical effectiveness of our framework.

</details>


### [19] [Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function](https://arxiv.org/abs/2512.04559)
*Hyeongyu Kang,Jaewoo Lee,Woocheol Shin,Kiyoung Om,Jinkyoo Park*

Main category: cs.LG

TL;DR: 提出 Soft Q-based Diffusion Finetuning (SQDF)，通过 KL 正则化的强化学习对齐扩散模型，同时引入折扣因子、一致性模型及离线回放以平衡奖励与多样性，在文本到图像对齐和在线黑盒优化中实现高奖励与高样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型微调易发生奖励过度优化，产生高奖励但不自然且多样性下降的问题，需要一种能兼顾自然性与多样性的对齐方法。

Method: 提出 SQDF：基于一个训练无须、可微估的软 Q 函数，采用重新参数化的策略梯度并进行 KL 正则化。为提升效果，加入三项创新：在扩散过程引入折扣因子以实现正确的信贷分配；将一致性模型用于 refined 的 Q 函数估计；使用离线回放缓冲区以提升模式覆盖和管理奖励-多样性权衡。

Result: 实验表明，SQDF 在文本到图像对齐任务中获得更高的目标奖励且保持多样性；在在线黑盒优化中实现高样本效率，同时维持自然度与多样性。

Conclusion: SQDF 提供一个高效且稳定的扩散对齐框架，有效缓解奖励过度优化问题，并在保持样本多样性的同时提升对齐质量。

Abstract: Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.

</details>


### [20] [LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models](https://arxiv.org/abs/2512.04562)
*Siddharth Betala,Samuel P. Gleason,Ali Ramlaoui,Andy Xu,Georgia Channing,Daniel Levy,Clémentine Fourrier,Nikita Kazeev,Chaitanya K. Joshi,Sékou-Oumar Kaba,Félix Therrien,Alex Hernandez-Garcia,Rocío Mercado,N. M. Anoop Krishnan,Alexandre Duval*

Main category: cs.LG

TL;DR: 提出 LeMat-GenBench，面向晶体材料生成模型的统一基准，提供评测套件和公开排行榜，基于12个模型的比较。


<details>
  <summary>Details</summary>
Motivation: 缺乏标准化的生成模型评估框架，难以公平比较和改进生成性材料设计模型，需要可复现、可扩展的基准来推动发现导向的模型发展。

Method: 提出 LeMat-GenBench 作为统一的评测基准，设计一组评估指标（包括稳定性、新颖性、多样性等），发布开源评测套件和 Hugging Face 公共排行榜，基准测试12个最近的生成模型。

Result: 结果显示模型的稳定性提升往往伴随新颖性和多样性的下降，且没有模型在所有维度上都表现优秀。

Conclusion: LeMat-GenBench 为公平、可复现实验的模型比较提供基础，旨在引导晶体材料生成模型朝着更可靠、面向发现的方向发展。

Abstract: Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.

</details>


### [21] [Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift](https://arxiv.org/abs/2512.04571)
*Aditi Naiknaware,Sanchit Singh,Hajar Homayouni,Salimeh Sekeh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.

</details>


### [22] [Exploiting \texttt{ftrace}'s \texttt{function\_graph} Tracer Features for Machine Learning: A Case Study on Encryption Detection](https://arxiv.org/abs/2512.04590)
*Kenan Begovic,Abdulaziz Al-Ali,Qutaibah Malluhi*

Main category: cs.LG

TL;DR: 使用 Linux 内核 ftrace 的函数图追踪器来提取图结构特征，以进行机器学习任务（如加密检测和程序识别）的系统级数据分析；在实际任务中取得高精度（约 99.28%），并提供了从原始追踪数据的预处理和特征提取的完整方法。


<details>
  <summary>Details</summary>
Motivation: 需要将系统跟踪数据转化为可用于机器学习的特征，以提升性能监控、安全分析和异常检测等场景的效果。函数图追踪器能提供丰富的调用图和时序信息，提升对系统行为的建模能力；通过把跟踪数据与多种学习算法结合，探索在加密活动检测和程序识别等任务中的应用潜力。

Method: 基于 Linux 内核 ftrace 框架中的函数图追踪器收集系统调用追踪，通过预处理原始追踪数据并提取基于图的特征，随后在加密检测任务中对多种学习算法进行评估，并在多标签任务中对正在运行的程序进行识别。

Result: 在加密检测任务中获得 99.28% 的高准确率，证实了函数图追踪器生成的特征的有效性；在另一个多标签分类实验中，基于追踪数据实现对正在运行程序的识别，验证了方法的广泛适用性。

Conclusion: 该工作弥合了系统追踪与机器学习之间的鸿沟，为性能监控、安全分析等领域的创新解决方案奠定基础，推动在系统行为分析、程序识别和异常检测方面的应用。

Abstract: This paper proposes using the Linux kernel ftrace framework, particularly the function graph tracer, to generate informative system level data for machine learning (ML) applications. Experiments on a real world encryption detection task demonstrate the efficacy of the proposed features across several learning algorithms. The learner faces the problem of detecting encryption activities across a large dataset of files, using function call traces and graph based features. Empirical results highlight an outstanding accuracy of 99.28 on the task at hand, underscoring the efficacy of features derived from the function graph tracer. The results were further validated in an additional experiment targeting a multilabel classification problem, in which running programs were identified from trace data. This work provides comprehensive methodologies for preprocessing raw trace data and extracting graph based features, offering significant advancements in applying ML to system behavior analysis, program identification, and anomaly detection. By bridging the gap between system tracing and ML, this paper paves the way for innovative solutions in performance monitoring and security analytics.

</details>


### [23] [Score Matching for Estimating Finite Point Processes](https://arxiv.org/abs/2512.04617)
*Haoqun Cao,Yixuan Zhang,Feng Zhou*

Main category: cs.LG

TL;DR: 提出一个基于Janossy测度的有限点过程分数匹配框架，给出自回归加权分数匹配估计器及参数分析；指出非参数模型下分数匹配的归一化不可识别性，并通过生存-分类扩增得到完整、无积分的训练目标；实验显示能准确估计强度并具备接近MLE的性能且更高效。


<details>
  <summary>Details</summary>
Motivation: 有限点过程在 bounded spaces 上具有特殊的随机配置，传统分数匹配缺乏严格的有限样本理论分析，亟需建立在Janossy测度上的完整理论框架，并提供可扩展且高效的估计方法以提升对时空点过程的建模能力。

Method: 建立基于Janossy测度的有限点过程分数匹配框架；提出一类自回归加权分数匹配估计器，并在经典参数设定下分析其统计性质；指出深度等非参数模型下分数匹配的归一化导致不可辨识性；提出生存-分类扩增，给出一个完整、无积分的训练目标，适用于任意基于强度的点过程模型；在时空与时空-时间数据上进行实验验证。

Result: 理论上分析了有参模型下的性质；揭示非参数情形的归一化问题并给出解决办法；实验表明能有效恢复强度并与MLE相近的性能，同时具有更高的计算效率。

Conclusion: 为有限点过程中的分数匹配提供了严格框架与实用解法，尤其通过生存-分类扩增解决了非参数模型的识别与积分问题，适用于时空点过程的强度建模与推断。

Abstract: Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estimators for point processes, this work highlights the limitations of these existing methods, which stem primarily from the lack of a mathematically rigorous analysis of how score matching behaves on finite point processes -- special random configurations on bounded spaces where many of the usual assumptions and properties of score matching no longer hold. To this end, we develop a formal framework for score matching on finite point processes via Janossy measures and, within this framework, introduce an (autoregressive) weighted score-matching estimator, whose statistical properties we analyze in classical parametric settings. For general nonparametric (e.g., deep) point process models, we show that score matching alone does not uniquely identify the ground-truth distribution due to subtle normalization issues, and we propose a simple survival-classification augmentation that yields a complete, integration-free training objective for any intensity-based point process model for spatio-temporal case. Experiments on synthetic and real-world temporal and spatio-temporal datasets, demonstrate that our method accurately recovers intensities and achieves performance comparable to MLE with better efficiency.

</details>


### [24] [Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective](https://arxiv.org/abs/2512.04625)
*Bowen Zheng,Ran Cheng*

Main category: cs.LG

TL;DR: GDKD 在预测分布视角下对 logits 进行更灵活的解耦，并强化非顶层 logits 的蒸馏，提升对多模态教师分布的鲁棒性与性能。


<details>
  <summary>Details</summary>
Motivation: 尽管 DKD 已有显著进展，但其机制尚未被充分理解；需要从教师预测分布对梯度的影响出发，深入分析 logits 解耦对知识蒸馏的作用。

Method: 提出 Generalized Decoupled Knowledge Distillation (GDKD) 损失，对 logits 进行更灵活的解耦；聚焦教师预测分布对梯度的影响，提出两点洞察：1) 以顶位 logit 进行分区能改善非顶 logits 的相互关系；2) 增强对非顶 logits 的蒸馏损失以加强它们之间的知识提取。基于此设计一个简化且高效的分区策略，处理教师分布的多模态性，并给出一个简化的 GDKD 算法。

Result: 在多项基准数据集（CIFAR-100、ImageNet、Tiny-ImageNet、CUB-200-2011、Cityscapes）上，GDKD 相较原 DKD 及其他主流蒸馏方法取得更优的性能。

Conclusion: 从预测分布角度出发的 GDKD 提供更强的知识蒸馏效果与对教师分布多模态性的鲁棒性，实现简洁且开源的代码。

Abstract: In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.

</details>


### [25] [Federated Learning for Anomaly Detection in Maritime Movement Data](https://arxiv.org/abs/2512.04635)
*Anita Graser,Axel Weißenfeld,Clemens Heistracher,Melitta Dragaschnig,Peter Widhalm*

Main category: cs.LG

TL;DR: M3fed 提出了一种用于移动异常检测的联邦学习框架，通过在 AIS 数据集上进行实验，比较联邦 M3fed 与集中式 M3，在通信成本和模型质量上进行评估。


<details>
  <summary>Details</summary>
Motivation: 在移动目标的异常检测中，数据往往分散且敏感，传统集中式训练会带来隐私风险和高通信成本，因此需要一种保护隐私、降低通信量的训练方案。

Method: 提出用于 M3 的新型联邦学习策略，训练 M3fed；以海事 AIS 数据为例进行实验，评估通信成本和联邦模型质量，并与经典的集中式 M3 进行对比。

Result: 通过实验对比，展示了在通信成本方面的降低以及联邦模型质量与集中式 M3 的可比性/竞争力。

Conclusion: M3fed 为移动异常检测提供了一个可行的、隐私友好且低通信成本的联邦学习解；在需要分布式数据且数据敏感的场景中具有应用潜力。

Abstract: This paper introduces M3fed, a novel solution for federated learning of movement anomaly detection models. This innovation has the potential to improve data privacy and reduce communication costs in machine learning for movement anomaly detection. We present the novel federated learning (FL) strategies employed to train M3fed, perform an example experiment with maritime AIS data, and evaluate the results with respect to communication costs and FL model quality by comparing classic centralized M3 and the new federated M3fed.

</details>


### [26] [Contract-Governed Training for Earth Observation: Observed Service Agreement Graphs and Coverage-Accuracy Trade-offs](https://arxiv.org/abs/2512.04644)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出一种基于契约治理的地球观测（EO）训练框架OSAG，通过将训练样本分组为契约并设定目标服务份额，使用契约归一化采样权重和两个调控参数alpha与lambda_C，在训练过程中实现对高优先区域的覆盖与全局准确率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统的EO模型通常在全局精度优化下对服务覆盖没有显式保证，无法确保在某些区域、类别或关键任务层面获得足够关注，需要一个可解释的治理机制来保障服务覆盖的公平与稳定。

Method: 提出Observ ed Service Agreement Graph（OSAG）作为轻量治理层：将训练样本分组为语义化的契约（如数据集、区域、稀有作物指标等），为每个契约设定目标服务份额；在优化中监控契约层曝光，利用契约归一化采样权重使经验覆盖收敛于目标份额，并通过两枚控制钮(alpha, lambda_C)暴露准确性-治理之间的权衡。理论方面给出 toy 场景的结论：OSAG采样将覆盖聚焦于目标，覆盖偏差上界等同于服务风险偏差，契约设计的粗细会影响治理成本。实验聚焦于AVIRIS高光谱数据（Indian Pines 与 Salinas）以及Sentinel-2 EuroSAT多光谱数据，证明OSAG在保持全局准确率的同时显著降低优先覆盖误差并提升高优先准确率，且粗-细契约 ablation显示更细的契约能在单位治理成本下降低准确率损失。

Result: OSAG使经验覆盖在目标契约处集中，覆盖偏差可控地转化为服务风险偏差；契约设计的层级（粗粒度对比细粒度）可以在治理成本与覆盖精度之间进行有效权衡；在 EuroSAT 的粗-细契约消融实验中，语义上更细的契约能更高效地提升治理收益，即以更低的准确率成本获得相同或更大的治理覆盖提升。

Conclusion: 通过引入契约治理的采样框架，OSAG为EO训练提供了可解释且可控的覆盖治理机制；语义层级的契约设计能够有效降低治理成本，同时在多源数据上实现对高优先区域覆盖的改进与全局性能的保持。

Abstract: Earth observation (EO) models are frequently trained under implicit sampling policies that optimize global accuracy but provide no explicit guarantees on who (which regions, classes, or mission-critical strata) is being served throughout training. This paper introduces a contract-governed training paradigm for EO in which training samples are grouped into service contracts -- semantically meaningful units such as (dataset, region, rare-crop indicator) -- and each contract is assigned a target service share. We instantiate this paradigm as an Observed Service Agreement Graph (OSAG), a lightweight governance layer that (i) monitors contract-level exposure (coverage) during optimization, (ii) drives empirical coverage toward target shares via contract-normalized sampling weights, and (iii) exposes explicit accuracy-governance trade-offs through two knobs: a sampling mixture coefficient alpha and a contract-regularization weight lambda_C. We provide a compact theory in a toy setting: OSAG sampling concentrates empirical coverage to targets; coverage deviations upper-bound service-risk deviations; and contract design (coarse vs. fine) modulates governance cost. Experiments on AVIRIS hyperspectral scenes (Indian Pines plus Salinas) and multispectral Sentinel-2 EuroSAT demonstrate that OSAG can substantially reduce priority coverage error while maintaining global accuracy and improving high-priority accuracy. A EuroSAT coarse-vs-fine contract ablation further evidences how semantically refined contracts can reduce the accuracy cost per unit of governance improvement.

</details>


### [27] [TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation](https://arxiv.org/abs/2512.04694)
*Baris Yilmaz,Bevan Deniz Cilgin,Erdem Akagündüz,Salih Tileylioglu*

Main category: cs.LG

TL;DR: 提出 TimesNet-Gen——一个基于时域的条件生成器，利用站点特定潜在瓶颈实现对地震场地效应的准确建模，在站点层面实现良好对齐，优于基于频谱的变分自编码器基线。


<details>
  <summary>Details</summary>
Motivation: 需要高精度的站点特异性地震烈度预测，以反映局地地质条件对地动特征的影响；数据驱动方法具潜力提取站点控制的签名。

Method: 在时域尺度上对加速度记录进行强震动生成，提出 TimesNet-Gen，包含一个站点特定潜在瓶颈的条件生成器；通过比较真实与生成记录的 HVSR 曲线和基本站点固有频率 f0 分布来评估，并用基于 f0 分布的混淆矩阵构建的分站点特异性分数进行汇总。与基于时频域的条件 VAE 基线相比，TimesNet-Gen 具备较强的站点对齐性。

Result: 在站点层面实现强对齐，与 spectrogram-based VAE 基线相比具有竞争力。

Conclusion: 时域条件生成框架 TimesNet-Gen 对站点特异性的强震动生成具有良好效果，提供了可与时频域方法媲美的替代方案，且代码公开。

Abstract: Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency $f_0$ distributions between real and generated records per station, and summarize station specificity with a score based on the $f_0$ distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.

</details>


### [28] [TRINITY: An Evolved LLM Coordinator](https://arxiv.org/abs/2512.04695)
*Jinglue Xu,Qi Sun,Peter Schwendeman,Stefan Nielsen,Edoardo Cetin,Yujin Tang*

Main category: cs.LG

TL;DR: Trinity是一种轻量级协调器，通过对LLMs执行三种角色（Thinker/Worker/Verifier）进行逐轮分工，来实现跨模型协作的高效推理；采用约0.6B参数的紧凑语言模型+约10K参数的头部，并通过进化策略优化，获得对编码、数学、推理和领域知识任务的显著提升与良好OOD泛化，达到大量基准的SOTA表现（LiveCodeBench 86.2%）。


<details>
  <summary>Details</summary>
Motivation: 解决不同基础模型在架构不匹配、API封闭等条件下的权重混合困难；通过一个轻量协调器实现多模型协作推理，避免对大模型进行微调或权重对齐的成本。

Method: 设计一个包含隐藏态表示的协调器，核心由约0.6B参数的紧凑LM和约10K参数的头部组成，通过进化策略（可分解的协方差矩阵自适应进化策略 CMA-ES）进行优化，以提升在高维与严格预算条件下的协作质量。查询以多轮方式处理，每轮协调器为选定的LLM分配一个角色（Thinker/Worker/Verifier），实现任务技能的外包与分布式学习。对编码、数学、推理、领域知识等任务进行评估，并在标准基准上比较与分析，给出理论与经验层面的原因解释。

Result: Trinity在多项任务上持续优于单一模型与现有方法，展现出对OOD任务的鲁棒泛化能力；达到SOTA水平，在LiveCodeBench等基准上取得86.2%的成绩；理论与实证分析指出：1）协调器的隐藏状态表示为输入提供丰富的上下文语境，2）在高维度和预算约束下，分离的 CMA-ES 相较于强化学习、模仿学习和随机搜索，能更好地利用潜在的块-ε可分性来优化协同策略。

Conclusion: 通过一个极简规模的协调器实现高效且自适应的跨模型协作，证明了在高维受限环境下，分离的进化策略配合隐藏态语境可以显著提升多模型组合的推理能力与泛化性。

Abstract: Combining diverse foundation models is promising, but weight-merging is limited by mismatched architectures and closed APIs. Trinity addresses this with a lightweight coordinator that orchestrates collaboration among large language models (LLMs). The coordinator, comprising a compact language model (approximately $0.6$B parameters) and a lightweight head (approximately $10$K parameters), is optimized with an evolutionary strategy for efficient and adaptive delegation. Trinity processes queries over multiple turns, where at each turn the coordinator assigns one of three roles (Thinker, Worker, or Verifier) to a selected LLM, effectively offloading complex skill acquisition from the coordinator itself. Experiments show that Trinity consistently outperforms individual models and existing methods across coding, math, reasoning, and domain knowledge tasks, and generalizes robustly to out-of-distribution tasks. On standard benchmarks, Trinity achieves state-of-the-art results, including a score of 86.2% on LiveCodeBench. Theoretical and empirical analyses identify two main factors behind this performance: (1) the coordinator's hidden-state representations provide rich contextualization of inputs, and (2) under high dimensionality and strict budget constraints, the separable Covariance Matrix Adaptation Evolution Strategy offers advantages over reinforcement learning, imitation learning, and random search by exploiting potential block-epsilon-separability.

</details>


### [29] [Towards Continuous-Time Approximations for Stochastic Gradient Descent without Replacement](https://arxiv.org/abs/2512.04703)
*Stefan Perko*

Main category: cs.LG

TL;DR: 提出基于 Young 微分方程和 epoched Brownian motion 的 SGDo 连续时间近似，给出强凸目标的几乎必然收敛性和渐近收敛速率界。


<details>
  <summary>Details</summary>
Motivation: 填补对基于 epoch 的随机梯度下降（SGDo）及相关算法的理论空白，提供一个可解析的连续时间模型，便于比较 with-replacement 与 one-pass 的理论结果。

Method: 构造以 Young 微分方程驱动的随机微分方程近似，驱动过程为 epoched Brownian motion；分析在学习率 u_t = (1+t)^(-β)、β ∈ (0,1) 情形下的收敛性。

Result: 证明对于强凸目标，连续时间近似具有几乎必然收敛性，并给出上界，表明渐近收敛速率与现有 SGDo 结果相当甚至更好。

Conclusion: 该近似为 SGDo 的理论分析提供新工具，能在给定学习率下实现与以往结果相近的收敛性与速率界，为 SGDo 的理论研究提供可分析框架。

Abstract: Gradient optimization algorithms using epochs, that is those based on stochastic gradient descent without replacement (SGDo), are predominantly used to train machine learning models in practice. However, the mathematical theory of SGDo and related algorithms remain underexplored compared to their "with replacement" and "one-pass" counterparts. In this article, we propose a stochastic, continuous-time approximation to SGDo with additive noise based on a Young differential equation driven by a stochastic process we call an "epoched Brownian motion". We show its usefulness by proving the almost sure convergence of the continuous-time approximation for strongly convex objectives and learning rate schedules of the form $u_t = \frac{1}{(1+t)^β}, β\in (0,1)$. Moreover, we compute an upper bound on the asymptotic rate of almost sure convergence, which is as good or better than previous results for SGDo.

</details>


### [30] [A Tutorial on Regression Analysis: From Linear Models to Deep Learning -- Lecture Notes on Artificial Intelligence](https://arxiv.org/abs/2512.04747)
*Jingyuan Wang,Jiahao Ji*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This article serves as the regression analysis lecture notes in the Intelligent Computing course cluster (including the courses of Artificial Intelligence, Data Mining, Machine Learning, and Pattern Recognition). It aims to provide students -- who are assumed to possess only basic university-level mathematics (i.e., with prerequisite courses in calculus, linear algebra, and probability theory) -- with a comprehensive and self-contained understanding of regression analysis without requiring any additional references. The lecture notes systematically introduce the fundamental concepts, modeling components, and theoretical foundations of regression analysis, covering linear regression, logistic regression, multinomial logistic regression, polynomial regression, basis-function models, kernel-based methods, and neural-network-based nonlinear regression. Core methodological topics include loss-function design, parameter-estimation principles, ordinary least squares, gradient-based optimization algorithms and their variants, as well as regularization techniques such as Ridge and LASSO regression. Through detailed mathematical derivations, illustrative examples, and intuitive visual explanations, the materials help students understand not only how regression models are constructed and optimized, but also how they reveal the underlying relationships between features and response variables. By bridging classical statistical modeling and modern machine-learning practice, these lecture notes aim to equip students with a solid conceptual and technical foundation for further study in advanced artificial intelligence models.

</details>


### [31] [RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting](https://arxiv.org/abs/2512.04752)
*Siqi Wang,Hailong Yang,Junjie Zhu,Xuezhu Wang,Yufan Xu,Depei Qian*

Main category: cs.LG

TL;DR: RLHFSpec提出一种在RLHF生成阶段引入推测解码的加速系统，通过自适应推测解码和作业负载感知的 drafting 策略，以及样本重新分配，在GPU资源利用上实现高吞吐，显著提升生成阶段及整个RLHF流程的速度。


<details>
  <summary>Details</summary>
Motivation: RLHF的生成阶段是整个执行流程的瓶颈，亟需优化以提升整体训练效率和响应速度。

Method: 将推测解码引入RLHF的生成阶段，提出基于负载感知的 drafting 策略选择机制，权衡验证成本与接受的token数量以选择近似最优策略；并提出样本重新分配和高效的样本迁移机制来充分利用GPU资源。

Result: 在生成阶段的吞吐量相较于现有方法有显著提升；由于有效缓解生成瓶颈，整个RLHF执行流程的性能也获得显著加速。

Conclusion: RLHFSpec通过在生成阶段引入自适应推测解码和智能的工作负载管理，成功提升了RLHF流水线的整体效率，证明其在大规模语言模型的强化学习从人类反馈中的应用潜力。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is an important fine-tuning technique for large language models (LLMs) and comprises three stages: generation, inference, and training. The generation stage generates samples that are then used to infer learnable experiences for training. We observe that the generation stage is the bottleneck of the entire execution process and consider it a key point for optimization. Specifically, we realize the first attempt to integrate speculative decoding into the RLHF generation stage and propose RLHFSpec, an RLHF system that accelerates generation execution with adaptive speculative decoding and sample reallocation. To fully exploit the performance potential provided by speculative decoding, especially dealing with the dynamic workload of the generation stage, RLHFSpec proposes a workload-aware drafting strategy selection mechanism, which selects the near-optimal strategy by jointly considering the verification cost and the number of accepted tokens. Moreover, RLHFSpec also proposes sample reallocation to fully utilize the GPU resources, and optimizes it with an efficient sample migration mechanism. The experimental results show that the RLHFSpec can achieve higher throughput in the generation stage compared to state-of-the-art works. Moreover, due to the effective alleviation of the generation bottleneck, RLHFSpec also shows significant performance speedup in the entire RLHF execution.

</details>


### [32] [MemLoRA: Distilling Expert Adapters for On-Device Memory Systems](https://arxiv.org/abs/2512.04763)
*Massimo Bini,Ondrej Bohdal,Umberto Michieli,Zeynep Akata,Mete Ozay,Taha Ceritli*

Main category: cs.LG

TL;DR: MemLoRA 提供一个在本地部署的记忆系统，通过专门的记忆适配器使小型语言模型具备记忆提取、记忆更新和记忆增强生成等记忆操作；MemLoRA-V 进一步结合小型视觉语言模型实现原生视觉理解，扩展到多模态场景。


<details>
  <summary>Details</summary>
Motivation: 在本地设备上保护隐私和降低成本的前提下，利用小型语言模型实现记忆驱动的个性化对话。现有基于大型语言模型的记忆系统成本高、难以在边缘设备部署，且缺乏本地的视觉能力。

Method: 采用知识蒸馏原理，在不同内存操作上训练独立的适配器，分别对应知识提取、内存更新和记忆增强生成；MemLoRA 通过记忆适配器实现本地记忆操作，MemLoRA-V 集成小型视觉语言模型以实现视觉推理能力。

Result: 文本任务上，MemLoRA 的表现超过10倍规模的基线模型（如 Gemma2-27B），并可达到60倍规模模型（如 GPT-OSS-120B）在 LoCoMo 基准上的性能水平；在视觉理解任务上，VLM 集成的 MemLoRA-V 相较于基于图像描述的做法在准确性上显著提高（81.3 vs 23.7），同时在文本任务上保持强劲表现。

Conclusion: 通过记忆适配器使小型模型具备高质量本地记忆操作并实现去云依赖，同时通过视觉扩展实现多模态记忆能力，证明了在隐私保护前提下的存储-检索-生成式对话的有效性和可扩展性。

Abstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.

</details>


### [33] [Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows](https://arxiv.org/abs/2512.04954)
*Rajneil Baruah*

Main category: cs.LG

TL;DR: 提出一种使用带似然加权重要性采样训练的正态化流（NF）来进行高维反问题的抹平后验估计的 amortized 推断方法；通过以高斯混合模型（GMM）初始化基分布来捕捉多模态后验，避免单峰基分布导致的错误连接；在二维/三维多模态基准任务上验证。


<details>
  <summary>Details</summary>
Motivation: 在高维反问题中高效地推断复杂、可能多模态的后验分布，无需大量后验训练样本，也无需为每个任务重新训练模型。

Method: 将正态化流与基于似然加权的重要性采样结合，进行抹平后验估计；将基分布设置为不同拓扑结构，比较单峰与高斯混合模型的效果；在2D/3D多模态基准任务上评估推理与重建 fidelity。

Result: 单峰基分布无法捕获断裂支撑，导致模式之间出现虚假的桥接；用与目标模式数量相匹配的高斯混合模型初始化流显著提升重建保真度，并在距离和散度等度量上表现更好。

Conclusion: 基分布的拓扑结构对多模态后验的建模至关重要；用与目标模式数量匹配的GMM初始化显著提升从高维反问题中得到的后验估计的保真度，方法为高效的amortized后验估计提供了一条可行路径。

Abstract: We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.

</details>


### [34] [Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning](https://arxiv.org/abs/2512.04958)
*Roberto Cipollone,Luca Iocchi,Matteo Leonetti*

Main category: cs.LG

TL;DR: 引入 Realizable Abstractions 架构的层级强化学习（HRL），并给出 RARL 算法，实现低层策略的组合化、近似最优性与鲁棒性保障。


<details>
  <summary>Details</summary>
Motivation: 当前 HRL 中的MDP 抽象要么表达能力有限、要么缺乏形式化的效率保证，难以在大规模任务上实现近似最优解。需要一个避免非马氏性、且具备近似最优性保证的抽象框架。

Method: 提出 Realizable Abstractions，将低层 MDP 与其高层决策过程建立一种可实现的关系；证明抽象策略可以通过组合选项转化为低层近似最优策略，选项等价于受约束的 MDP 的解；提出 RARL 算法，基于该抽象提供组合化、近似最优的低层策略，并给出其收敛性与鲁棒性分析。

Result: 理论上证明：抽象策略可转化为低层近似最优策略，选项由受约束的 MDP 求解来实现；RARL 在多项式样本复杂度内收敛，具有 Probably Approximately Correct (PAC) 性质，且对抽象不准确性具有鲁棒性。

Conclusion: Realizable Abstractions 为 HRL 提供了一个具有表达力和理论保障的框架，允许通过组合化的选项构建低层策略。RARL 作为实现该框架的算法，具备收敛性、近似最优性与鲁棒性等理论保障。

Abstract: The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.

</details>


### [35] [Efficient Generative Transformer Operators For Million-Point PDEs](https://arxiv.org/abs/2512.04974)
*Armand Kassaï Koupaï,Lise Le Boudec,Patrick Gallinari*

Main category: cs.LG

TL;DR: 提出 ECHO：一个基于变换器与操作符的分层编码-解码框架，能够以高效稀疏输入生成百万点PDE轨迹并在长序列上保持低误差。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在密网格上可扩展性差、动态展开时误差累积、以及任务定制性不足，限制了大规模PDE求解的实际应用。

Method: 采用分层卷积编码-解码架构实现100×时空压缩，同时保持网格点保真；通过训练与适应策略从稀疏网格生成高分辨率解；引入生成式建模以学习完整轨迹片段，减缓长 horizon 的误差漂移；训练策略解耦表示学习与下游任务监督，支持多任务与条件/无条件生成。

Result: 在多种具有复杂几何、高频动力学和长时间跨度的PDE系统上达到前列性能，能够在百万点规模的仿真中实现高效和高保真。

Conclusion: ECHO 提供一种可扩展、适应性强且多任务友好的框架，适合从稀疏输入推断高分辨率解、生成完整轨迹，并缓解长期误差积累。

Abstract: We introduce ECHO, a transformer-operator framework for generating million-point PDE trajectories. While existing neural operators (NOs) have shown promise for solving partial differential equations, they remain limited in practice due to poor scalability on dense grids, error accumulation during dynamic unrolling, and task-specific design. ECHO addresses these challenges through three key innovations. (i) It employs a hierarchical convolutional encode-decode architecture that achieves a 100 $\times$ spatio-temporal compression while preserving fidelity on mesh points. (ii) It incorporates a training and adaptation strategy that enables high-resolution PDE solution generation from sparse input grids. (iii) It adopts a generative modeling paradigm that learns complete trajectory segments, mitigating long-horizon error drift. The training strategy decouples representation learning from downstream task supervision, allowing the model to tackle multiple tasks such as trajectory generation, forward and inverse problems, and interpolation. The generative model further supports both conditional and unconditional generation. We demonstrate state-of-the-art performance on million-point simulations across diverse PDE systems featuring complex geometries, high-frequency dynamics, and long-term horizons.

</details>


### [36] [Dual-Path Region-Guided Attention Network for Ground Reaction Force and Moment Regression](https://arxiv.org/abs/2512.05030)
*Xuan Li,Samuel Bello*

Main category: cs.LG

TL;DR: Dual-Path Region-Guided Attention Network to estimate 3D GRFs/GRMs from insole data, achieving state-of-the-art accuracy on two datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate GRF/GRM estimation is vital for biomechanics and rehabilitation; insole-based estimation with attention mechanisms leveraging anatomical spatial priors and temporal priors can outperform conventional CNN-based models.

Method: A Dual-Path architecture: (1) region-guided attention with anatomy-inspired spatial and temporal priors; (2) a complementary path capturing context from the full sensor field; paths are trained jointly and fused for final GRF/GRM predictions.

Result: Outperforms CNN and CNN-LSTM baselines on two datasets; six-component average NRMSE 5.78% on the insole dataset; 1.42% NRMSE for vertical GRF on the public walking dataset.

Conclusion: The proposed framework provides robust GRF/GRM estimation from insole data, highlighting the benefit of combining region-guided attention with full-sensor context.

Abstract: Accurate estimation of three-dimensional ground reaction forces and moments (GRFs/GRMs) is crucial for both biomechanics research and clinical rehabilitation evaluation. In this study, we focus on insole-based GRF/GRM estimation and further validate our approach on a public walking dataset. We propose a Dual-Path Region-Guided Attention Network that integrates anatomy-inspired spatial priors and temporal priors into a region-level attention mechanism, while a complementary path captures context from the full sensor field. The two paths are trained jointly and their outputs are combined to produce the final GRF/GRM predictions. Conclusions: Our model outperforms strong baseline models, including CNN and CNN-LSTM architectures on two datasets, achieving the lowest six-component average NRMSE of 5.78% on the insole dataset and 1.42% for the vertical ground reaction force on the public dataset. This demonstrates robust performance for ground reaction force and moment estimation.

</details>


### [37] [SuperActivators: Only the Tail of the Distribution Contains Reliable Concept Signals](https://arxiv.org/abs/2512.05038)
*Cassandra Goldberg,Chaehyeon Kim,Adam Stein,Eric Wong*

Main category: cs.LG

TL;DR: 在概念向量的噪声中发现极端高尾部的信号（SuperActivator），该信号比传统向量或提示方法更稳健地指示概念存在，并能提升特征归因质量，跨多模态与模型架构表现一致，最高提升约14% 的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有概念向量的激活常被噪声污染，导致可解释性有限；需要发现更鲁棒的信号把内部表征与人类语义对接，并提升概念检测和特征归因的可靠性。

Method: 系统分析不同分布情况的激活信号，发现in-concept 的极高尾部激活与概念存在显著相关；在图像和文本模态、多种模型/层级/概念提取技术上验证；提出使用SuperActivator tokens进行概念检测与特征归因。

Result: SuperActivator tokens在概念检测任务中显著优于基线的向量法与提示法，F1最高提升约14%；在不同模态、模型架构、层次与概念提取方法上呈现普适性；同时提升概念的特征归因质量。

Conclusion: 极端高尾部激活提供更稳健的概念存在信号，SuperActivator机制具有较强的一般性与迁移性，推动解释性研究向更可靠的信号源和归因方法发展。

Abstract: Concept vectors aim to enhance model interpretability by linking internal representations with human-understandable semantics, but their utility is often limited by noisy and inconsistent activations. In this work, we uncover a clear pattern within the noise, which we term the SuperActivator Mechanism: while in-concept and out-of-concept activations overlap considerably, the token activations in the extreme high tail of the in-concept distribution provide a reliable signal of concept presence. We demonstrate the generality of this mechanism by showing that SuperActivator tokens consistently outperform standard vector-based and prompting concept detection approaches, achieving up to a 14% higher F1 score across image and text modalities, model architectures, model layers, and concept extraction techniques. Finally, we leverage SuperActivator tokens to improve feature attributions for concepts.

</details>


### [38] [Multi-LLM Collaboration for Medication Recommendation](https://arxiv.org/abs/2512.05066)
*Huascar Sanchez,Briland Hitaj,Jules Bergmann,Linda Briesemeister*

Main category: cs.LG

TL;DR: 在临床场景中，将基于化学素养的多模型协作应用于药物推荐的可靠性提升。通过 Chemistry 风格的交互建模来实现有效、稳定、校准的多模型集合，以提高从简短临床病历中给出的个体化药物推荐的可信度。


<details>
  <summary>Details</summary>
Motivation: 解决单一大型语言模型易产生幻觉与不一致性，以及简单模型集合在稳定性与可信度方面的不足。通过引入模型间的协作与干预控制，提升临床决策支持的可靠性、可解释性与个体化质量。

Method: 在先前的 LLM Chemistry 框架基础上，将其应用到药物推荐任务。通过 Chemistry 启发的交互建模对多模型协作进行引导，从而构建更能发挥互补优势、产出更稳定且校准的集合，提升对简短临床病历的个体化药物推荐的可信度。对真实世界临床情景进行评估，验证交互感知的集合是否能产生可信、面向患者的药物推荐。

Result: 初步结果令人鼓舞，表明以 Chemistry 引导的多模型协作可能为临床实践中的可靠与可信的 AI 助手提供一条有前景的路径；在真实场景下的实验表明该方法有潜力提供更可信和稳定的药物推荐。

Conclusion: Chemistry 基于的多模型协作为临床决策支持中的可靠性和可信度问题提供了一种有希望的解决思路，值得进一步在更大规模与更多场景中进行验证与扩展。

Abstract: As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.

</details>


### [39] [OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design](https://arxiv.org/abs/2512.05080)
*Ian Dunn,Liv Toft,Tyler Katz,Juhi Gupta,Riya Shah,Ramith Hettiarachchi,David R. Koes*

Main category: cs.LG

TL;DR: 提出 OMTRA，一种多模态流匹配模型，统一结构基药物设计任务的生成框架，覆盖从 de novo 设计到 docking 等多任务，并构建了规模化的 3D 架构数据集；在口袋条件任务上达到最新水平，但大规模预训练的增益有限。


<details>
  <summary>Details</summary>
Motivation: 结构基础药物设计（SBDD）中的不同任务（如 de novo 设计、虚拟筛选、对接）具有共性，可被同一生成框架统一表示与解决，因此需要一个多任务、可扩展的生成模型来提升整体效率与多样性。

Method: 提出多模态流匹配模型 OMTRA，作为统一的生成框架来处理多种 SBDD 任务；并构建并采用一个规模约 5×10^8 个三维分子构象的数据集，补充蛋白-配体数据，扩展训练数据的化学多样性。实现口袋条件下的 de novo 设计和 docking 的 state-of-the-art 表现，且支持无 analogue 的新任务。

Result: 在口袋条件下的 de novo 设计与 docking 任务上达到最先进性能（state-of-the-art），但对大规模预训练和多任务训练的收益相对温和，提示数据规模与任务难度并非线性放大收益的关键。

Conclusion: 统一的多模态生成框架如 OMTRA 展现出在 SBDD 中整合多任务的潜力与可行性；大规模数据与多任务学习虽带来性能提升，但收益边际递减，未来工作可聚焦于提升训练效率、任务设计的协同性以及对特定任务的细粒度优化。

Abstract: Structure-based drug design (SBDD) focuses on designing small-molecule ligands that bind to specific protein pockets. Computational methods are integral in modern SBDD workflows and often make use of virtual screening methods via docking or pharmacophore search. Modern generative modeling approaches have focused on improving novel ligand discovery by enabling de novo design. In this work, we recognize that these tasks share a common structure and can therefore be represented as different instantiations of a consistent generative modeling framework. We propose a unified approach in OMTRA, a multi-modal flow matching model that flexibly performs many tasks relevant to SBDD, including some with no analogue in conventional workflows. Additionally, we curate a dataset of 500M 3D molecular conformers, complementing protein-ligand data and expanding the chemical diversity available for training. OMTRA obtains state of the art performance on pocket-conditioned de novo design and docking; however, the effects of large-scale pretraining and multi-task training are modest. All code, trained models, and dataset for reproducing this work are available at https://github.com/gnina/OMTRA

</details>


### [40] [Gradient Descent with Provably Tuned Learning-rate Schedules](https://arxiv.org/abs/2512.05084)
*Dravyansh Sharma*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Gradient-based iterative optimization methods are the workhorse of modern machine learning. They crucially rely on careful tuning of parameters like learning rate and momentum. However, one typically sets them using heuristic approaches without formal near-optimality guarantees. Recent work by Gupta and Roughgarden studies how to learn a good step-size in gradient descent. However, like most of the literature with theoretical guarantees for gradient-based optimization, their results rely on strong assumptions on the function class including convexity and smoothness which do not hold in typical applications. In this work, we develop novel analytical tools for provably tuning hyperparameters in gradient-based algorithms that apply to non-convex and non-smooth functions. We obtain matching sample complexity bounds for learning the step-size in gradient descent shown for smooth, convex functions in prior work (up to logarithmic factors) but for a much broader class of functions. Our analysis applies to gradient descent on neural networks with commonly used activation functions (including ReLU, sigmoid and tanh). We extend our framework to tuning multiple hyperparameters, including tuning the learning rate schedule, simultaneously tuning momentum and step-size, and pre-training the initialization vector. Our approach can be used to bound the sample complexity for minimizing both the validation loss as well as the number of gradient descent iterations.

</details>


### [41] [The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception](https://arxiv.org/abs/2512.05089)
*Eduardo Di Santi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.
  This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.
  We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.
  Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.

</details>


### [42] [TV2TV: A Unified Framework for Interleaved Language and Video Generation](https://arxiv.org/abs/2512.05103)
*Xiaochuang Han,Youssef Emad,Melissa Hall,John Nguyen,Karthik Padthe,Liam Robbins,Amir Bar,Delong Chen,Michal Drozdzal,Maha Elbayad,Yushi Hu,Shang-Wen Li,Sreya Dutta Roy,Jakob Verbeek,XuDong Wang,Marjan Ghazvininejad,Luke Zettlemoyer,Emily Dinan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.

</details>


### [43] [Deep infant brain segmentation from multi-contrast MRI](https://arxiv.org/abs/2512.05114)
*Malte Hoffmann,Lilla Zöllei,Adrian V. Dalca*

Main category: cs.LG

TL;DR: BabySeg 是一个面向婴幼儿脑MRI分割的深度学习框架，利用领域随机化和多输入扫描特征聚合，在单一模型下实现对多种协议和年龄段的状态-of-the-art性能，且运行速度快于许多现有工具。


<details>
  <summary>Details</summary>
Motivation: 婴幼儿脑MRI分割因发育差异、成像约束、模态不一致、视野内非头部组织、以及运动伪影等因素而困难，现有方法常限于特定图像类型或狭窄年龄段，且对临床上更为多变的图像易脆弱。需要一个能够跨数据集、跨协议并对重复扫描和未见输入类型具备鲁棒性的统一模型。

Method: 提出 BabySeg 框架，建立在领域随机化的技术基础之上，通过合成超出现实界限的训练图像来提升数据集偏移不变性；并设计一种机制，使模型能灵活地从任意数量的输入扫描中聚合并交互特征，以应对多模态/重复扫描场景。

Result: 在多年龄段和多输入配置下，单模型实现了与或超过现有方法的状态-of-the-art 精度，且运行时间显著低于许多现有工具。

Conclusion: 该工作表明通过领域随机化和可扩展的多扫描特征聚合，可以消除婴幼儿脑MRI分割的方法碎片化，提供对异构数据的稳健、高效的一体化解决方案。

Abstract: Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.

</details>


### [44] [Value Gradient Guidance for Flow Matching Alignment](https://arxiv.org/abs/2512.05116)
*Zhen Liu,Tim Z. Xiao,Carles Domingo-Enrich,Weiyang Liu,Dinghuai Zhang*

Main category: cs.LG

TL;DR: 提出 VGG-Flow，通过梯度匹配的最优控制框架对预训练流式匹配模型进行高效微调，兼顾快速自适应和先验保留。


<details>
  <summary>Details</summary>
Motivation: 现有将流式匹配模型与人类偏好对齐的方法难以在适应效率与先验保留之间取得两全其美，需要在理论上更具可控性的微调方案。

Method: 基于最优控制，假设微调后的速度场与预训练速度场之差应与值函数梯度场对齐，以此实现梯度匹配的微调。方法还融合来自奖励模型的一阶信息，并对值函数进行启发式初始化以促进快速收敛。

Result: 在流行的文本到图像流式匹配模型 Stable Diffusion 3 上实证，表明该方法在有限计算预算下可实现有效且先验保留的对齐。

Conclusion: 提出了一种在预算受限条件下实现高效、且概率上有良好先验保留的对齐微调框架，具有潜在的实用性与扩展性。

Abstract: While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.

</details>


### [45] [The Universal Weight Subspace Hypothesis](https://arxiv.org/abs/2512.05117)
*Prakhar Kaushik,Shravan Chaudhari,Ankit Vaidya,Rama Chellappa,Alan Yuille*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [46] [Informative missingness and its implications in semi-supervised learning](https://arxiv.org/abs/2512.04392)
*Jinran Wu,You-Gan Wang,Geoffrey J. McLachlan*

Main category: stat.ML

TL;DR: 将半监督学习与有信息缺失机制结合在有限混合模型与EM框架中的统计分析，表明在缺失标签受信息约束时，建模缺失标签机制可能提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 在标记数据成本高昂的场景，利用未标记数据提高预测性能，同时考虑当缺失标签的产生与特征或类别相关时，缺失指示变量本身包含信息，因此需要一个统一的似然框架来分析。

Method: 将问题建模为有限混合模型的缺失数据问题，利用EM算法进行参数估计；将缺失标签的产生机制作为潜在变量一并建模，评估在不同缺失性（如 MAR/MNAR）下信息量对分类的影响；理论探讨在类别重叠 moderate、标记数据稀疏、缺失信息性强时，建模缺失机制的净收益。

Result: 理论上证明在某些条件下，利用缺失标签机制的信息可以带来比完全标注样本分析更低的期望错误（即更优的分类性能）。

Conclusion: 提出一个一致的统计框架，将 likelihood 推断与半监督学习行为统一起来，强调在 SSL 中对缺失机制进行建模的合理性和潜在收益。

Abstract: Semi-supervised learning (SSL) constructs classifiers using both labelled and unlabelled data. It leverages information from labelled samples, whose acquisition is often costly or labour-intensive, together with unlabelled data to enhance prediction performance. This defines an incomplete-data problem, which statistically can be formulated within the likelihood framework for finite mixture models that can be fitted using the expectation-maximisation (EM) algorithm. Ideally, one would prefer a completely labelled sample, as one would anticipate that a labelled observation provides more information than an unlabelled one. However, when the mechanism governing label absence depends on the observed features or the class labels or both, the missingness indicators themselves contain useful information. In certain situations, the information gained from modelling the missing-label mechanism can even outweigh the loss due to missing labels, yielding a classifier with a smaller expected error than one based on a completely labelled sample analysed. This improvement arises particularly when class overlap is moderate, labelled data are sparse, and the missingness is informative. Modelling such informative missingness thus offers a coherent statistical framework that unifies likelihood-based inference with the behaviour of empirical SSL methods.

</details>


### [47] [Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting](https://arxiv.org/abs/2512.04690)
*Souhir Ben Amor,Florian Ziel*

Main category: stat.ML

TL;DR: Hybrid linear-nonlinear RNN for day-ahead electricity price forecasting, integrating expert models and Kalman-filter-like linear structures within a recurrent network to improve accuracy and interpretability; tested on European hourly data (2018-2025) with ~12% accuracy gain over benchmarks.


<details>
  <summary>Details</summary>
Motivation: Accurate, interpretable short-term electricity price forecasts are crucial for decision-making and operational planning in energy systems. Pure linear or pure nonlinear models struggle to capture both stylized price features and exogenous drivers. A hybrid approach aims to combine the strengths of linear and nonlinear modeling.

Method: A recurrent neural network architecture that embeds linear components (expert models, Kalman-filter-like updates) into the network, leveraging both linear and nonlinear structures to capture calendar effects, autoregressive terms, and influences from load, renewable generation, fuel, and carbon markets. Empirical evaluation on hourly European electricity market data from 2018–2025, comparing against high-dimensional linear and neural network benchmarks.

Result: The proposed model achieves approximately 12% higher accuracy than leading benchmarks. The study also analyzes the contributions of interpretable components, demonstrating the value of integrating linear and nonlinear structures.

Conclusion: Combining linear and nonlinear model components in a unified RNN improves forecast accuracy and interpretability for day-ahead electricity prices, with evidence from a large-scale European dataset; future work could extend interpretability analysis and generalizability to other markets.

Abstract: We present a novel recurrent neural network architecture designed explicitly for day-ahead electricity price forecasting, aimed at improving short-term decision-making and operational management in energy systems. Our combined forecasting model embeds linear structures, such as expert models and Kalman filters, into recurrent networks, enabling efficient computation and enhanced interpretability. The design leverages the strengths of both linear and non-linear model structures, allowing it to capture all relevant stylised price characteristics in power markets, including calendar and autoregressive effects, as well as influences from load, renewable energy, and related fuel and carbon markets. For empirical testing, we use hourly data from the largest European electricity market spanning 2018 to 2025 in a comprehensive forecasting study, comparing our model against state-of-the-art approaches, particularly high-dimensional linear and neural network models. The proposed model achieves approximately 12% higher accuracy than leading benchmarks. We evaluate the contributions of the interpretable model components and conclude on the impact of combining linear and non-linear structures.

</details>


### [48] [Learning Causality for Longitudinal Data](https://arxiv.org/abs/2512.04980)
*Mouad EL Bouchattaoui*

Main category: stat.ML

TL;DR: 本论文/论文集提出三大贡献：1) Causal Dynamic Variational Autoencoder (CDVAE) 用于估计个体治疗效应（ITE），通过潜在风险因素捕捉治疗反应的未观测异质性，并给出潜在变量调整的理论保证与ITE误差的泛化界限；在合成与真实数据上优于基线，结合其潜在替代变量的增强接近oracle表现。2) 提出高效的长时记忆反事实回归框架，基于带对比预测编码（CPC）和InfoMax 的RNN，能在存在时间变化共因的情况下捕捉长期依赖，且避免Transformer的高计算成本，达到state-of-the-art并将CPC引入因果推断。3) 通过解码器雅可比矩阵的几何结构提出一个模型无关的可解释性层，采用稀疏自表达先验，能形成与潜在影响源一致的、可重叠的观测特征分组；给出在不需要锚特征或单亲假设下的潜观结构恢复保证，并开发可扩展的雅可比正则化方法。


<details>
  <summary>Details</summary>
Motivation: 在高维、时变数据中进行因果推断与因果表示学习面临未观测混杂、长期依赖以及对潜在因果结构的可解释性需求。本研究旨在提供具理论保证、具实证优势且具可解释性的端到端方法。

Method: 1) CDVAE：引入潜在变量捕捉仅影响结果的隐性风险因素，给出有效潜在调整的理论保证与ITE误差的泛化界限，并在合成与真实数据集上进行实验。2) 基于RNN + CPC/InfoMax 的长时反事实回归框架，通过CPC实现对长期依赖的有效建模，降低计算成本（避免Transformer），并达到最新性能。3) 基于解码器雅可比几何的可解释性层，采用稀疏自表达先验实现观测变量在潜在影响下的模块化/重叠分组，给出在离散/重叠情形下的恢复保证，并提供可扩展的雅可比正则化。

Result: CDVAE 在去偏与ITE误差方面呈现理论与实验上的优势，且与其潜在替代变量结合后，能显著提升其他模型的性能，接近在真实调整变量不可得时的oracle水平。基于CPC的RNN框架在长期反事实回归任务上实现了state-of-the-art，并将CPC成功引入因果推断领域。解码器雅可比几何的可解释性层能够在不依赖锚特征或单亲假设的情况下，恢复潜在与观测之间的结构，并给出分组的恢复保证，同时提供可扩展的正则化方法。

Conclusion: 该工作体系性推进了高维、时变数据中的因果推断与因果表示学习，兼具理论保障、实证性能提升与可解释性，展示了将潜在因素、长期依赖与解耦结构纳入统一框架的可行路径。

Abstract: This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data.
  The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error. Experiments on synthetic and real datasets show that CDVAE outperforms baselines, and that state-of-the-art models greatly improve when augmented with its latent substitutes, approaching oracle performance without access to true adjustment variables.
  The second contribution proposes an efficient framework for long-term counterfactual regression based on RNNs enhanced with Contrastive Predictive Coding (CPC) and InfoMax. It captures long-range dependencies under time-varying confounding while avoiding the computational cost of transformers, achieving state-of-the-art results and introducing CPC into causal inference.
  The third contribution advances CRL by addressing how latent causes manifest in observed variables. We introduce a model-agnostic interpretability layer based on the geometry of the decoder Jacobian. A sparse self-expression prior induces modular, possibly overlapping groups of observed features aligned with shared latent influences. We provide recovery guarantees in both disjoint and overlapping settings and show that meaningful latent-to-observed structure can be recovered without anchor features or single-parent assumptions. Scalable Jacobian-based regularization techniques are also developed.

</details>


### [49] [Towards a unified framework for guided diffusion models](https://arxiv.org/abs/2512.04985)
*Yuchen Jiao,Yuxin Chen,Gen Li*

Main category: stat.ML

TL;DR: 提出一个统一的框架来理解和提升带引导的扩散模型：融合扩散引导与奖励引导，给出 CFG 的理论分析，并提出易训练的奖励引导扩散采样器，实验支持理论。


<details>
  <summary>Details</summary>
Motivation: 理论上对带引导的扩散采样缺乏统一、可量化的框架；需要将扩散引导和奖励引导结合起来，以便更加系统地评估对目标奖励的提升及相关性。

Method: 在后向扩散过程加入一个由原始分数与奖励重加权后的分数差构成的奖励引导项；对 CFG 提供理论分析，证明其降低目标分布下分类器概率的倒数，给出 CFG 改善的具体度量解释；针对奖励引导提出一种易于训练、无需完整扩散轨迹的采样器。

Result: 建立了一个统一的理论与算法框架，量化奖励对差分引导的改进；给出 CFG 在一般目标分布上的具体性能指标改进的理论描述；提出一种简化且易于训练的奖励引导扩散采样器并通过数值实验验证理论预测。

Conclusion: 该框架将扩散引导与奖励引导统一起来，提供对 CFG 与奖励引导的深刻理论洞察，并给出实际可训练的采样方法，提升对目标奖励的控制与实现潜力。

Abstract: Guided or controlled data generation with diffusion models\blfootnote{Partial preliminary results of this work appeared in International Conference on Machine Learning 2025 \citep{li2025provable}.} has become a cornerstone of modern generative modeling. Despite substantial advances in diffusion model theory, the theoretical understanding of guided diffusion samplers remains severely limited. We make progress by developing a unified algorithmic and theoretical framework that accommodates both diffusion guidance and reward-guided diffusion. Aimed at fine-tuning diffusion models to improve certain rewards, we propose injecting a reward guidance term -- constructed from the difference between the original and reward-reweighted scores -- into the backward diffusion process, and rigorously quantify the resulting reward improvement over the unguided counterpart. As a key application, our framework shows that classifier-free guidance (CFG) decreases the expected reciprocal of the classifier probability, providing the first theoretical characterization of the specific performance metric that CFG improves for general target distributions. When applied to reward-guided diffusion, our framework yields a new sampler that is easy-to-train and requires no full diffusion trajectories during training. Numerical experiments further corroborate our theoretical findings.

</details>


### [50] [Control Consistency Losses for Diffusion Bridges](https://arxiv.org/abs/2512.05070)
*Samuel Howard,Nikolas Nüsken,Jakiw Pidstrigach*

Main category: stat.ML

TL;DR: 提出一种在线自洽学习的扩散桥方法，用以在给定起点与终点的条件下采样扩散过程路径，特别在罕见事件场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 在给定起点和终点条件下对扩散过程的路径进行采样是一个困难问题，尤其是罕见事件导致未经过终点的路径极度稀少。需要高效、可扩展的条件路径生成方法。

Method: 基于条件扩散动力学的自洽性属性，提出一个在线迭代学习框架来近似扩散桥。通过循环地对条件路径进行采样并更新桥模型，使其在在线数据流中逐步收敛到目标条件分布。

Result: 在多种实验设置中，所提方法展现出对条件路径分布的更高保真度和更好的采样效率，尤其在罕见事件情形下相比基线方法具有显著优势。

Conclusion: 在线自洽学习的扩散桥方法为条件扩散问题提供了一种高效、可扩展的解决思路，具有在实时系统和复杂场景中的应用潜力，并可进一步扩展到更高维或不同类型的随机过程。

Abstract: Simulating the conditioned dynamics of diffusion processes, given their initial and terminal states, is an important but challenging problem in the sciences. The difficulty is particularly pronounced for rare events, for which the unconditioned dynamics rarely reach the terminal state. In this work, we leverage a self-consistency property of the conditioned dynamics to learn the diffusion bridge in an iterative online manner, and demonstrate promising empirical results in a range of settings.

</details>


### [51] [Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction](https://arxiv.org/abs/2512.05092)
*Vincent Pauline,Tobias Höppe,Kirill Neklyudov,Alexander Tong,Stefan Bauer,Andrea Dittadi*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Although diffusion models now occupy a central place in generative modeling, introductory treatments commonly assume Euclidean data and seldom clarify their connection to discrete-state analogues. This article is a self-contained primer on diffusion over general state spaces, unifying continuous domains and discrete/categorical structures under one lens. We develop the discrete-time view (forward noising via Markov kernels and learned reverse dynamics) alongside its continuous-time limits -- stochastic differential equations (SDEs) in $\mathbb{R}^d$ and continuous-time Markov chains (CTMCs) on finite alphabets -- and derive the associated Fokker--Planck and master equations. A common variational treatment yields the ELBO that underpins standard training losses. We make explicit how forward corruption choices -- Gaussian processes in continuous spaces and structured categorical transition kernels (uniform, masking/absorbing and more) in discrete spaces -- shape reverse dynamics and the ELBO. The presentation is layered for three audiences: newcomers seeking a self-contained intuitive introduction; diffusion practitioners wanting a global theoretical synthesis; and continuous-diffusion experts looking for an analogy-first path into discrete diffusion. The result is a unified roadmap to modern diffusion methodology across continuous domains and discrete sequences, highlighting a compact set of reusable proofs, identities, and core theoretical principles.

</details>
