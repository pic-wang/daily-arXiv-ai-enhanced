<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 60]
- [stat.ML](#stat.ML) [Total: 8]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [KAN-GCN: Combining Kolmogorov-Arnold Network with Graph Convolution Network for an Accurate Ice Sheet Emulator](https://arxiv.org/abs/2510.24926)
*Zesheng Liu,YoungHyun Koo,Maryam Rahnemoonfar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce KAN-GCN, a fast and accurate emulator for ice sheet modeling
that places a Kolmogorov-Arnold Network (KAN) as a feature-wise calibrator
before graph convolution networks (GCNs). The KAN front end applies learnable
one-dimensional warps and a linear mixing step, improving feature conditioning
and nonlinear encoding without increasing message-passing depth. We employ this
architecture to improve the performance of emulators for numerical ice sheet
models. Our emulator is trained and tested using 36 melting-rate simulations
with 3 mesh-size settings for Pine Island Glacier, Antarctica. Across 2- to
5-layer architectures, KAN-GCN matches or exceeds the accuracy of pure GCN and
MLP-GCN baselines. Despite a small parameter overhead, KAN-GCN improves
inference throughput on coarser meshes by replacing one edge-wise
message-passing layer with a node-wise transform; only the finest mesh shows a
modest cost. Overall, KAN-first designs offer a favorable accuracy vs.
efficiency trade-off for large transient scenario sweeps.

</details>


### [2] [Perturbation Bounds for Low-Rank Inverse Approximations under Noise](https://arxiv.org/abs/2510.25571)
*Phuc Tran,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: 在观测噪声下，给出对称矩阵 A 的最佳秩-p 逆的谱范数扰动的非渐进界，利用对非整函数 1/z 的轮廓积分，界比朴素全逆界更紧，提升可达 sqrt(n)，并有实证验证。


<details>
  <summary>Details</summary>
Motivation: 现实矩阵常带噪声，低秩逆近似在可扩展的机器学习、优化和科学计算中广泛使用，但对其谱鲁棒性理解不足，需要结合谱特征给出稳健的非渐进扰动界。

Method: 研究 A_p^{-1} 在观测噪声 E 造成的扰动 	ilde A = A+E 下的误差 ||( 	ilde A^{-1})_p - A_p^{-1}||，给出非渐进扰动界；在对称 A 下，使用轮廓积分处理非整函数 f(z)=1/z，结合特征分解中的 eigengap、谱衰减与噪声在低曲率方向的对齐等因素，得到对 p 相关的界。与经典的全逆界做对比，给出可提升最多 sqrt(n) 的改进。

Result: 给出依赖 eigengap、谱衰减与噪声对齐的非渐进谱范数扰动界；理论上相对于简单的全逆界更紧，提升可达 sqrt(n)；实验上界值贴近真实误差，优于基于传统结果的估计。

Conclusion: 为带噪声的低秩逆近似提供谱感知的实用保证，适用于机器学习、优化与科学计算中的 scalable 计算场景。

Abstract: Low-rank pseudoinverses are widely used to approximate matrix inverses in
scalable machine learning, optimization, and scientific computing. However,
real-world matrices are often observed with noise, arising from sampling,
sketching, and quantization. The spectral-norm robustness of low-rank inverse
approximations remains poorly understood. We systematically study the
spectral-norm error $\| (\tilde{A}^{-1})_p - A_p^{-1} \|$ for an $n\times n$
symmetric matrix $A$, where $A_p^{-1}$ denotes the best rank-\(p\)
approximation of $A^{-1}$, and $\tilde{A} = A + E$ is a noisy observation.
Under mild assumptions on the noise, we derive sharp non-asymptotic
perturbation bounds that reveal how the error scales with the eigengap,
spectral decay, and noise alignment with low-curvature directions of $A$. Our
analysis introduces a novel application of contour integral techniques to the
\emph{non-entire} function $f(z) = 1/z$, yielding bounds that improve over
naive adaptations of classical full-inverse bounds by up to a factor of
$\sqrt{n}$. Empirically, our bounds closely track the true perturbation error
across a variety of real-world and synthetic matrices, while estimates based on
classical results tend to significantly overpredict. These findings offer
practical, spectrum-aware guarantees for low-rank inverse approximations in
noisy computational environments.

</details>


### [3] [Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy](https://arxiv.org/abs/2510.25670)
*Phuc Tran,Nisheeth K. Vishnoi,Van H. Vu*

Main category: cs.LG

TL;DR: 提出了高概率的对称矩阵谱范数扰动界，细化了 Eckart–Young–Mirsky，给出 (A+E)_p 与 A_p 的误差界，揭示 A 与对称扰动 E 的相互作用，某些情形可实现 sqrt(n) 量级的改进；并应用于差分隐私PCA 的改进效用界，另外提出一种基于复分析的轮廓自举方法，扩展到多种谱函数，实验支持理论。


<details>
  <summary>Details</summary>
Motivation: 理解噪声或测量误差对低秩近似在谱范数下的影响，尤其在差分隐私低秩近似中，需要保留数据导出矩阵的前 p 结构同时保证隐私。谱范数能捕捉最坏方向的误差，提供最强的效用保证。

Method: 建立对称矩阵在对称扰动 E 下的高概率谱范数扰动界，细化经典的 Eckart–Young–Mirsky 结果，明确刻画矩阵 A 与任意对称扰动 E 的相互作用。在弱的本征间隙和范数条件下，推导出 ||(A+E)_p − A_p|| 的尖锐界，并引入一种从复分析出发的新颖轮廓自举法，扩展至多类谱函数（包括多项式、矩阵指数等）。

Result: 给出对称情形下的误差界，能达到相较传统界的 sqrt(n) 量级提升；对差分隐私PCA 提供改进的效用保证，解决相关公开问题；理论与实证均表明界与实际谱误差紧密贴合。

Conclusion: 该方法提供更紧的谱范数扰动界，增强低秩近似在随机扰动与隐私保护情境下的鲁棒性，且方法具有广泛适用性，可应用于各类谱函数性分析的框架。

Abstract: A central challenge in machine learning is to understand how noise or
measurement errors affect low-rank approximations, particularly in the spectral
norm. This question is especially important in differentially private low-rank
approximation, where one aims to preserve the top-$p$ structure of a
data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius
norm error or changes in reconstruction quality, but these metrics can over- or
under-estimate true subspace distortion. The spectral norm, by contrast,
captures worst-case directional error and provides the strongest utility
guarantees. We establish new high-probability spectral-norm perturbation bounds
for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem
and explicitly capture interactions between a matrix $A \in \mathbb{R}^{n
\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and
norm conditions, our bounds yield sharp estimates for $\|(A + E)_p - A_p\|$,
where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up
to a factor of $\sqrt{n}$. As an application, we derive improved utility
guarantees for differentially private PCA, resolving an open problem in the
literature. Our analysis relies on a novel contour bootstrapping method from
complex analysis and extends it to a broad class of spectral functionals,
including polynomials and matrix exponentials. Empirical results on real-world
datasets confirm that our bounds closely track the actual spectral error under
diverse perturbation regimes.

</details>


### [4] [Sequences of Logits Reveal the Low Rank Structure of Language Models](https://arxiv.org/abs/2510.24966)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 将语言模型作为序列概率模型研究其低秩结构，发现不同提示和回答集合下的 logits 矩阵近似具有低秩；可通过对不相关提示输出的线性组合来生成目标提示的回答；并给出一个普适性抽象及可证明的学习保证。


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型的固有低维结构，并以模型无关的方式分析其作为序列概率模型的性质。

Method: 对广泛的现代语言模型进行实证分析：构造包含不同提示和回应集合的 logits 矩阵，检验其近似秩；展示通过对不相关或无意义提示的输出进行线性组合来生成目标提示的回答；提出将近似秩作为普遍抽象的理论框架，并给出可证明的学习保证。

Result: 在多种现代语言模型中观测到近似低秩结构；能够使用对不相关提示输出的线性组合来生成目标提示的回答；理论框架的预测与实验结果一致，并给出表示能力分析与学习保证。

Conclusion: 低秩结构为理解 LLMs 提供了一个鲁棒的抽象，既可用于生成任务也有理论支撑，并具有普适性。

Abstract: A major problem in the study of large language models is to understand their
inherent low-dimensional structure. We introduce an approach to study the
low-dimensional structure of language models at a model-agnostic level: as
sequential probabilistic models. We first empirically demonstrate that a wide
range of modern language models exhibit low-rank structure: in particular,
matrices built from the model's logits for varying sets of prompts and
responses have low approximate rank. We then show that this low-rank structure
can be leveraged for generation -- in particular, we can generate a response to
a target prompt using a linear combination of the model's outputs on unrelated,
or even nonsensical prompts.
  On the theoretical front, we observe that studying the approximate rank of
language models in the sense discussed above yields a simple universal
abstraction whose theoretical predictions parallel our experiments. We then
analyze the representation power of the abstraction and give provable learning
guarantees.

</details>


### [5] [Shift is Good: Mismatched Data Mixing Improves Test Performance](https://arxiv.org/abs/2510.25108)
*Marko Medvedev,Kaifeng Lyu,Zhiyuan Li,Nathan Srebro*

Main category: cs.LG

TL;DR: 本研究探讨训练与测试在混合分布下的比例错配问题，发现在多种情形中分布漂移可提升测试性能，即使分量无关且无跨分量迁移；给出最优训练比例及其收益范围，并将分析推广到具有不同技能分布的组合设定。


<details>
  <summary>Details</summary>
Motivation: 为应对现实中训练数据与测试数据分布不一致的问题，特别关注训练/测试比例错配对模型性能的影响，以及在组件/技能分布不同的情形下的可迁移性与潜在收益。

Method: 通过理论性分析和概率建模，研究混合分布下训练与测试比例对性能的影响，推导在何种条件下分布漂移有利，给出最优或近似最优训练比例，并扩展至组件技能分布差异的组合情形。

Result: 在多种场景中，分布漂移可带来测试性能提升，存在可识别的最优训练比例；该好处不依赖于组件之间的直接迁移，即使组件彼此独立亦可获得收益。

Conclusion: 理论分析揭示了训练数据比例对测试性能的系统性影响，提供选取最优训练比例的指引，并证明该分析同样适用于技能分布不同的组合场景，具有广泛适用性。

Abstract: We consider training and testing on mixture distributions with different
training and test proportions. We show that in many settings, and in some sense
generically, distribution shift can be beneficial, and test performance can
improve due to mismatched training proportions, even if the components are
unrelated and with no transfer between components. In a variety of scenarios,
we identify the optimal training proportions and the extent to which such
distribution shift can be beneficial. We show how the same analysis applies
also to a compositional setting with differing distribution of component
"skills'' at training and test.

</details>


### [6] [An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation](https://arxiv.org/abs/2510.25128)
*Uzair Akbar,Niki Kilbertus,Hao Shen,Krikamol Muandet,Bo Dai*

Main category: cs.LG

TL;DR: 将数据增强（DA）从传统的i.i.d.正则化扩展到跨干预的泛化，提出IV-like回归（IVL）以减小隐藏混杂的偏差，并将参数化DA视为IVL问题，与现有IV的性质松耦合时也能提升性能


<details>
  <summary>Details</summary>
Motivation: 在现实任务中，数据增强不仅要提升在i.i.d.设定下的泛化，还需在干预切换下保持鲁棒性。隐藏混杂导致的偏差难以直接用IV解决，且IV信源并不总是易得，因此探索将DA与因果推断结合的可能性，利用正则化的IVL回归来获得跨干预的稳健性与预测性能

Method: 提出一个统一框架，将DA视为对治疗生成机制的干预；引入IV-like（IVL）回归，对IV估计量进行正则化以减小偏差；将参数化DA视为IVL回归问题，研究其组合后可近似最坏情况DA的应用，提升因果估计与跨干预的泛化性能；理论分析（总体情况）+ 基于简单线性模型的有限样本仿真实验；以及真实数据实验

Result: 在总体层面给出理论支撑，表明IVL回归可以在某些放松IV性质的情形下仍有效减少混杂偏差并提升跨干预的预测与因果估计性能；有限样本通过线性示例的仿真实验验证了该方法在提升鲁棒性和泛化能力方面的潜力；真实数据实验也提供经验支持，显示与简单DA相比的改进。

Conclusion: 将DA与IVL回归有效结合，拓展DE在跨干预和隐藏混杂情景下的应用边界；参数化DA在与IVL组合时可近似最坏情形的DA应用，从而增强因果估计和跨干预泛化能力；若正则化适当，IV性质的松耦合仍能带来稳健性和预测提升。

Abstract: The technique of data augmentation (DA) is often used in machine learning for
regularization purposes to better generalize under i.i.d. settings. In this
work, we present a unifying framework with topics in causal inference to make a
case for the use of DA beyond just the i.i.d. setting, but for generalization
across interventions as well. Specifically, we argue that when the outcome
generating mechanism is invariant to our choice of DA, then such augmentations
can effectively be thought of as interventions on the treatment generating
mechanism itself. This can potentially help to reduce bias in causal effect
estimation arising from hidden confounders. In the presence of such unobserved
confounding we typically make use of instrumental variables (IVs) -- sources of
treatment randomization that are conditionally independent of the outcome.
However, IVs may not be as readily available as DA for many applications, which
is the main motivation behind this work. By appropriately regularizing IV based
estimators, we introduce the concept of IV-like (IVL) regression for mitigating
confounding bias and improving predictive performance across interventions even
when certain IV properties are relaxed. Finally, we cast parameterized DA as an
IVL regression problem and show that when used in composition can simulate a
worst-case application of such DA, further improving performance on causal
estimation and generalization tasks beyond what simple DA may offer. This is
shown both theoretically for the population case and via simulation experiments
for the finite sample case using a simple linear example. We also present real
data experiments to support our case.

</details>


### [7] [Scalable Utility-Aware Multiclass Calibration](https://arxiv.org/abs/2510.25458)
*Mahmoud Hegazy,Michael I. Jordan,Aymeric Dieuleveut*

Main category: cs.LG

TL;DR: 提出一个可扩展的多类校准评估框架：效用校准，将校准误差与具体下游效用函数相关联，统一并扩展现有校准指标，支持对更丰富的下游决策目标的评估。


<details>
  <summary>Details</summary>
Motivation: 现有多类校准评估多聚焦于顶类置信度、逐类校准等方面，或依赖成本高、复杂的变分推理方法，缺乏对实际决策目标的直接对齐。需要一种可扩展、与决策目标相符的评估框架。

Method: 提出效用校准框架，将校准误差定义为与特定下游效用函数相关的量，能够重现并统一现有指标（如顶类/逐类校准），并能扩展到更丰富的下游效用，以实现更鲁棒的评估。

Result: 给出一种通用的评估框架，理论上可把多类校准与下游效用对齐，能够对现有校准指标进行统一解释，并提供对更复杂决策目标的鲁棒评估工具。

Conclusion: 效用校准为多类校准评估提供统一、可扩展的框架，便于在具体应用中根据不同决策目标定制校准评估。

Abstract: Ensuring that classifiers are well-calibrated, i.e., their predictions align
with observed frequencies, is a minimal and fundamental requirement for
classifiers to be viewed as trustworthy. Existing methods for assessing
multiclass calibration often focus on specific aspects associated with
prediction (e.g., top-class confidence, class-wise calibration) or utilize
computationally challenging variational formulations. In this work, we study
scalable \emph{evaluation} of multiclass calibration. To this end, we propose
utility calibration, a general framework that measures the calibration error
relative to a specific utility function that encapsulates the goals or decision
criteria relevant to the end user. We demonstrate how this framework can unify
and re-interpret several existing calibration metrics, particularly allowing
for more robust versions of the top-class and class-wise calibration metrics,
and, going beyond such binarized approaches, toward assessing calibration for
richer classes of downstream utilities.

</details>


### [8] [Fortytwo: Swarm Inference with Peer-Ranked Consensus](https://arxiv.org/abs/2510.24801)
*Vladyslav Larin,Ihor Naumenko,Aleksei Ivashov,Ivan Nikitin,Alexander Firsov*

Main category: cs.LG

TL;DR: Fortytwo 提出一种基于群体智能的去中心化推理层，通过分布式两两排序共识实现AI推理的水平扩展，使用声誉权重与对比排序采纳Bradley-Terry模型，显著优于多数投票，并通过链上声誉与能力证明抵御Sybil攻击。六项基准显示在准确度与对抗干扰方面具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着中心化AI遇到算力上限与边际收益递减，需构建一个在容量和能力上都能水平扩展的推理层，以服务日益增长的推理需求。

Method: 采用群体推理(chat swarm inference)的设计：通过同类模型的对比排序（对对排序）结合Bradley–Terry风格的聚合模型进行共识；在链上记录声誉、并引入能力证明以进入排序轮；节点需完成标定/测试请求并质押声誉以抵御多身份攻击；在六个基准（包括GPQA Diamond、LiveCodeBench、AIME等）进行评估。

Result: 在GPQA Diamond等基准上， swarm推理相较多数投票表现出显著提升：GPQA Diamond为85.90%（对比68.69%，提升17.21pp，约相对提升25.1%）。对抗噪声提示的鲁棒性也较强，提示注入等攻击造成的准确度降幅仅0.12%，显著优于单一大模型基线的6.20%降幅。六项基准显示较高的准确度与对对抗性/噪声的鲁棒性，同时保持可部署性。

Conclusion: 为去中心化AI系统奠定基础，通过集体智能实现高质量推理的民主化，同时兼顾可靠性与安全性，推动可扩展、去中心化的AI推理生态。

Abstract: As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.

</details>


### [9] [TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting](https://arxiv.org/abs/2510.25502)
*Vladyslav Moroshan,Julien Siems,Arber Zela,Timur Carstensen,Frank Hutter*

Main category: cs.LG

TL;DR: TempoPFN：基于线性RNN的单变量时序基金模型，在纯合成数据上预训练，支持跨序列长度的完全并行训练，使用GatedDeltaProduct与state-weaving；在Gift-Eval零-shot评估中表现出色，优于所有合成数据方法并超多数用真实数据训练的模型；并开源数据生成和训练代码，提升可重复性和研究基础。


<details>
  <summary>Details</summary>
Motivation: 现有基金模型在零-shot长 horizon预测中存在效率低、可重复性不足的问题，且以纯合成数据训练的方法在挑战性基准上表现不佳。本研究提出一个仅用合成数据训练的单变量时序基金模型，旨在实现高效且可重复的长期预测。

Method: 提出TempoPFN，一种基于线性RNN的单变量时间序列基金模型，采用GatedDeltaProduct结构和state-weaving实现对整个序列长度的完全并行训练，避免窗口化或摘要化并保持良好的时间状态跟踪。数据层面实现统一的合成数据流水线，整合随机微分方程、高斯过程和音频合成等生成器，并引入新颖的数据增强。

Result: 在Gift-Eval的零-shot评估中，TempoPFN达到顶尖竞争水平，优于所有现有合成数据方法，并超越大量以真实数据训练的模型，同时通过完全并行的训练和推理提高了效率，相较基线更具优势。

Conclusion: 提供可重复的研究基础：开源完整的数据生成流水线与训练代码，为未来研究提供可复现性和扩展性。

Abstract: Foundation models for zero-shot time series forecasting face challenges in
efficient long-horizon prediction and reproducibility, with existing
synthetic-only approaches underperforming on challenging benchmarks. This paper
presents TempoPFN, a univariate time series foundation model based on linear
Recurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The
model uses a GatedDeltaProduct architecture with state-weaving for fully
parallelizable training across sequence lengths, eliminating the need for
windowing or summarization techniques while maintaining robust temporal
state-tracking. Our comprehensive synthetic data pipeline unifies diverse
generators, including stochastic differential equations, Gaussian processes,
and audio synthesis, with novel augmentations. In zero-shot evaluations on the
Gift-Eval benchmark, TempoPFN achieves top-tier competitive performance,
outperforming all existing synthetic-only approaches and surpassing the vast
majority of models trained on real-world data, while being more efficient than
existing baselines by leveraging fully parallelizable training and inference.
We open-source our complete data generation pipeline and training code,
providing a reproducible foundation for future research.

</details>


### [10] [Generalized Sobolev IPM for Graph-Based Measures](https://arxiv.org/abs/2510.25591)
*Tam Le,Truyen Nguyen,Hideitsu Hino,Kenji Fukumizu*

Main category: cs.LG

TL;DR: Generalizes Sobolev IPM on graphs via Orlicz geometry, introducing Orlicz-Sobolev framework with Musielak regularization; reduces to univariate optimization (GSI-M) and is much faster than Wasserstein-based methods, with applications in document classification and topological data analysis.


<details>
  <summary>Details</summary>
Motivation: Sobolev IPM tied to L^p geometry limits incorporation of richer structural priors. Need a flexible geometric framework to capture nuanced relationships and leverage recent Orlicz-Wasserstein advances, while maintaining computational efficiency on graph-structured data.

Method: Introduce Orlicz geometric structure to Sobolev IPM, yielding a generalized Sobolev IPM (GSI). Establish a theoretical link between Orlicz-Sobolev norm and Musielak norm to derive Musielak regularization for GSI (GSI-M). Exploit graph structure to reduce optimization to a univariate problem, enabling efficient computation.

Result: GSI-M achieves orders-of-magnitude speedups over the Orlicz-Wasserstein framework and standard OW methods on graphs. Demonstrates practical effectiveness in comparing probability measures on graphs for document classification and several tasks in topological data analysis.

Conclusion: The Orlicz-Sobolev generalization broadens the admissible geometric priors beyond conventional L^p structures, while maintaining computational tractability through Musielak regularization and a univariate reduction. This yields efficient, scalable tools for graph-based measure comparison with potential wide ML applicability.

Abstract: We study the Sobolev IPM problem for measures supported on a graph metric
space, where critic function is constrained to lie within the unit ball defined
by Sobolev norm. While Le et al. (2025) achieved scalable computation by
relating Sobolev norm to weighted $L^p$-norm, the resulting framework remains
intrinsically bound to $L^p$ geometric structure, limiting its ability to
incorporate alternative structural priors beyond the $L^p$ geometry paradigm.
To overcome this limitation, we propose to generalize Sobolev IPM through the
lens of \emph{Orlicz geometric structure}, which employs convex functions to
capture nuanced geometric relationships, building upon recent advances in
optimal transport theory -- particularly Orlicz-Wasserstein (OW) and
generalized Sobolev transport -- that have proven instrumental in advancing
machine learning methodologies. This generalization encompasses classical
Sobolev IPM as a special case while accommodating diverse geometric priors
beyond traditional $L^p$ structure. It however brings up significant
computational hurdles that compound those already inherent in Sobolev IPM. To
address these challenges, we establish a novel theoretical connection between
Orlicz-Sobolev norm and Musielak norm which facilitates a novel regularization
for the generalized Sobolev IPM (GSI). By further exploiting the underlying
graph structure, we show that GSI with Musielak regularization (GSI-M) reduces
to a simple \emph{univariate optimization} problem, achieving remarkably
computational efficiency. Empirically, GSI-M is several-order faster than the
popular OW in computation, and demonstrates its practical advantages in
comparing probability measures on a given graph for document classification and
several tasks in topological data analysis.

</details>


### [11] [Augmenting Biological Fitness Prediction Benchmarks with Landscapes Features from GraphFLA](https://arxiv.org/abs/2510.24826)
*Mingyu Huang,Shasha Zhou,Ke Li*

Main category: cs.LG

TL;DR: GraphFLA 通过从多模态致变数据构建并分析适应性景观，提供 20 个与景观地形相关的特征，便于解释和比较现有的预测模型。


<details>
  <summary>Details</summary>
Motivation: 现有基准规模虽大，但缺乏景观地形信息，限制对模型在不同地形下的表现进行深入解读与横向比较。

Method: 构建致变数据的景观并计算 20 个生物学相关特征以描述景观地形，覆盖 DNA/RNA/蛋白等模态，支持处理数百万级突变体。对超过 5,300 个景观应用 GraphFLA，发布 155 个组合完全景观与约 220 万序列的数据集，同时提供开源代码与数据。

Result: 在 ProteinGym、RNAGym、CIS-BP 等数据集上，GraphFLA 能解释并比较数十种模型的性能，揭示影响精度的因素，并凸显不同模型的相对优势。

Conclusion: GraphFLA 提供了景观地形层面的分析框架，提升对模型在实际景观中的表现的理解与跨数据集、跨模型的可比性。

Abstract: Machine learning models increasingly map biological sequence-fitness
landscapes to predict mutational effects. Effective evaluation of these models
requires benchmarks curated from empirical data. Despite their impressive
scales, existing benchmarks lack topographical information regarding the
underlying fitness landscapes, which hampers interpretation and comparison of
model performance beyond averaged scores. Here, we introduce GraphFLA, a Python
framework that constructs and analyzes fitness landscapes from mutagensis data
in diverse modalities (e.g., DNA, RNA, protein, and beyond) with up to millions
of mutants. GraphFLA calculates 20 biologically relevant features that
characterize 4 fundamental aspects of landscape topography. By applying
GraphFLA to over 5,300 landscapes from ProteinGym, RNAGym, and CIS-BP, we
demonstrate its utility in interpreting and comparing the performance of dozens
of fitness prediction models, highlighting factors influencing model accuracy
and respective advantages of different models. In addition, we release 155
combinatorially complete empirical fitness landscapes, encompassing over 2.2
million sequences across various modalities. All the codes and datasets are
available at https://github.com/COLA-Laboratory/GraphFLA.

</details>


### [12] [Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions](https://arxiv.org/abs/2510.25769)
*Naoki Kiyohara,Edward Johns,Yingzhen Li*

Main category: cs.LG

TL;DR: NSFs学习(潜在)SDE的转移规律，利用条件正则化流实现对任意时间点的一次性采样，兼顾分布准确性与显著加速（对于大时间间隔，速度可达约100x）。


<details>
  <summary>Details</summary>
Motivation: 传统SDE数值求解在跨越任意时间点采样时成本很高；现实世界的时间序列通常嘈杂且采样不规则，因此需要能够在任意时间间隔内直接高效采样的模型，同时保持分布的统计性质。

Method: 通过条件正则化流学习(潜在)SDE的转移分布，并加入保持随机流性质的结构约束；提出NSFs及其潜在变体，以直接建模转移法则以实现一次性采样。

Result: 在合成SDE与真实跟踪/视频数据上，NSFs在分布一致性上接近传统数值方法，同时在大时间间隔下实现显著的计算加速（最高可达约100x）。

Conclusion: NSFs为高效的任意时点采样提供了一个通用框架，既保持统计正确性又显著提升计算效率；适用于需要跨越任意时间点的SDE建模场景。

Abstract: Stochastic differential equations (SDEs) are well suited to modelling noisy
and irregularly sampled time series found in finance, physics, and machine
learning. Traditional approaches require costly numerical solvers to sample
between arbitrary time points. We introduce Neural Stochastic Flows (NSFs) and
their latent variants, which directly learn (latent) SDE transition laws using
conditional normalising flows with architectural constraints that preserve
properties inherited from stochastic flows. This enables one-shot sampling
between arbitrary states and yields up to two orders of magnitude speed-ups at
large time gaps. Experiments on synthetic SDE simulations and on real-world
tracking and video data show that NSFs maintain distributional accuracy
comparable to numerical approaches while dramatically reducing computation for
arbitrary time-point sampling.

</details>


### [13] [Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations](https://arxiv.org/abs/2510.24884)
*Olawale Salaudeen,Haoran Zhang,Kumail Alhamoud,Sara Beery,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: Accuracy-on-the-line (ID vs OOD) is often an artifact of mixing heterogeneous OOD data; a gradient-based method OODSelect reveals semantically coherent OOD subsets where higher ID accuracy can hurt OOD accuracy, across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that spurious correlations are rare and that aggregate ID-OOD metrics adequately reflect OOD robustness; understand how heterogeneity within OOD data affects observed correlations and failures.

Method: Propose OODSelect, a gradient-based method to identify semantically coherent subsets of OOD data. Evaluate across standard distribution shift benchmarks to compare ID vs subset OOD performance.

Result: Across benchmarks, many identified OOD subsets show inverse or reduced correlation between ID accuracy and OOD accuracy; sometimes over half of the OOD set; aggregated metrics obscure these failure modes.

Conclusion: Aggregate metrics can mask important OOD robustness failures. Subset-aware evaluation with tools like OODSelect is needed, and the authors release code/subsets to facilitate research.

Abstract: Benchmarks for out-of-distribution (OOD) generalization frequently show a
strong positive correlation between in-distribution (ID) and OOD accuracy
across models, termed "accuracy-on-the-line." This pattern is often taken to
imply that spurious correlations - correlations that improve ID but reduce OOD
performance - are rare in practice. We find that this positive correlation is
often an artifact of aggregating heterogeneous OOD examples. Using a simple
gradient-based method, OODSelect, we identify semantically coherent OOD subsets
where accuracy on the line does not hold. Across widely used distribution shift
benchmarks, the OODSelect uncovers subsets, sometimes over half of the standard
OOD set, where higher ID accuracy predicts lower OOD accuracy. Our findings
indicate that aggregate metrics can obscure important failure modes of OOD
robustness. We release code and the identified subsets to facilitate further
research.

</details>


### [14] [Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding](https://arxiv.org/abs/2510.24889)
*Shakeel Abdulkareem,Bora Yimenicioglu,Andrea Yang,Khartik Uppalapati,Aneesh Gudipati,Zhaoyang Fan*

Main category: cs.LG

TL;DR: Adaptive multitask EEG classifier using 32-channel signals converted to PSD features (Welch), with a GRU-TCN backbone for stroke-type, hemispheric lateralization, and severity, plus a DQN to tune decision thresholds in real time. Demonstrates high accuracy on a small patient-wise split dataset (UCLH EIT/EEG) with validation on an independent cohort (ZJU4H); threshold adaptation shifts operating point; interpretability via scalp maps and spectral visuals.


<details>
  <summary>Details</summary>
Motivation: Stroke triage at first contact requires rapid, accurate bedside tools. EEG is promising but underutilized in acute settings. This work proposes an adaptive, end-to-end EEG-based classifier that can provide multi-dimensional stroke assessment and dynamically adjust decision thresholds to balance sensitivity and specificity.

Method: Preprocess 32-channel EEG into power spectral density (Welch) features. Input to a GRU-TCN multi-task classifier predicting: (1) stroke type (healthy, ischemic, hemorrhagic), (2) hemispheric lateralization, (3) severity. A Deep Q-Network (DQN) tunes decision thresholds in real time to optimize diagnostic trade-offs. Evaluation uses a patient-wise split on the UCLH Stroke EIT/EEG dataset (44 recordings; ~26 acute stroke, 10 controls); primary outcome is stroke-type performance, with secondary outcomes for severity and lateralization. Robustness tested on an independent, low-density EEG cohort (ZJU4H). Analysis aligned with STARD 2015 guidelines.

Result: Baseline GRU-TCN achieved stroke-type accuracy 89.3% (F1 92.8%), severity ~96.9% (F1 95.9%), lateralization ~96.7% (F1 97.4%). Adding DQN threshold adaptation raised stroke-type accuracy to ~98.0% (F1 97.7%). ZJU4H cohort validated with paired patient-level statistics. Adaptive thresholding shifts operating points; integrated scalp-map and spectral visualizations aid interpretability.

Conclusion: Adaptive thresholding enables clinicians to tailor sensitivity-specificity trade-offs in real time, while the model provides multi-dimensional stroke assessment with interpretable visualizations; shows promise but is tested on small datasets and needs broader validation.

Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools;
EEG is promising but underused at first contact. We present an adaptive
multitask EEG classifier that converts 32-channel signals to power spectral
density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to
predict stroke type (healthy, ischemic, hemorrhagic), hemispheric
lateralization, and severity, and applies a deep Q-network (DQN) to tune
decision thresholds in real time. Using a patient-wise split of the UCLH Stroke
EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the
primary outcome was stroke-type performance; secondary outcomes were severity
and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for
stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%)
for lateralization. With DQN threshold adaptation, stroke-type accuracy
increased to about 98.0% (F1 97.7%). We also tested robustness on an
independent, low-density EEG cohort (ZJU4H) and report paired patient-level
statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies
(index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis;
patient-wise evaluation). Adaptive thresholding shifts the operating point to
clinically preferred sensitivity-specificity trade-offs, while integrated
scalp-map and spectral visualizations support interpretability.

</details>


### [15] [Finding Culture-Sensitive Neurons in Vision-Language Models](https://arxiv.org/abs/2510.24942)
*Xiutian Zhao,Rochelle Choenni,Rohit Saxena,Ivan Titov*

Main category: cs.LG

TL;DR: 发现VLMs中存在对特定文化敏感的神经元，并提出对比激活选择（CAS）等方法识别和评估其对文化相关问答的影响，显示这类神经元在解码层具有聚簇分布。


<details>
  <summary>Details</summary>
Motivation: 理解VLMs如何处理文化背景信息，以及文化敏感神经元在跨文化视觉问答中的作用与定位。

Method: 在CVQA基准上识别文化选择性神经元，采用多种识别方法并进行因果干预（关闭/抑制标记神经元），在3个VLMs和25个文化群体上测试；提出并验证CAS（对比激活选择）作为新的神经元筛选器；进行层级分析以定位神经元的分布。

Result: 经去活性测试，针对对应文化的问题的性能下降显著高于对其他文化的影响，表明存在文化敏感的神经元；CAS在识别这类神经元方面优于基于概率或熵的现有方法；神经元在某些解码层呈现聚簇分布。

Conclusion: 揭示了多模态表示的内部组织结构及文化背景信息的编码方式，为提升跨文化鲁棒性提供方向，如对关键文化敏感神经元的保护、校准或定向调控。

Abstract: Despite their impressive performance, vision-language models (VLMs) still
struggle on culturally situated inputs. To understand how VLMs process
culturally grounded information, we study the presence of culture-sensitive
neurons, i.e. neurons whose activations show preferential sensitivity to inputs
associated with particular cultural contexts. We examine whether such neurons
are important for culturally diverse visual question answering and where they
are located. Using the CVQA benchmark, we identify neurons of culture
selectivity and perform causal tests by deactivating the neurons flagged by
different identification methods. Experiments on three VLMs across 25 cultural
groups demonstrate the existence of neurons whose ablation disproportionately
harms performance on questions about the corresponding cultures, while having
minimal effects on others. Moreover, we propose a new margin-based selector -
Contrastive Activation Selection (CAS), and show that it outperforms existing
probability- and entropy-based methods in identifying culture-sensitive
neurons. Finally, our layer-wise analyses reveals that such neurons tend to
cluster in certain decoder layers. Overall, our findings shed new light on the
internal organization of multimodal representations.

</details>


### [16] [Strategic inputs: feature selection from game-theoretic perspective](https://arxiv.org/abs/2510.24982)
*Chi Zhao,Jing Liu,Elena Parilina*

Main category: cs.LG

TL;DR: 基于博弈论的端到端特征选择框架，针对表格数据，通过把特征视为玩家进行合作博弈，结合样本选择、特征重要性评估、冗余特征消除与模型训练优化，在提升计算效率的同时尽量保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 随着数据规模指数增长，许多特征对模型贡献有限却消耗大量计算资源，需高效的特征选择来降低成本并保持性能。

Method: 将特征建模为玩家，进行合作博弈以评估特征间的协同影响和边际贡献；框架包含样本选择、博弈论特征重要性评估、冗余特征消除、优化模型训练四个组成部分。

Result: 实验表明该方法在降低计算成本的同时能保持预测性能显著，提供了大型机器学习计算挑战的高效解决方案。

Conclusion: 对大规模表格数据的计算开销具有潜在显著收益，给出可复用的端到端特征选择框架，源代码可获取。

Abstract: The exponential growth of data volumes has led to escalating computational
costs in machine learning model training. However, many features fail to
contribute positively to model performance while consuming substantial
computational resources. This paper presents an end-to-end feature selection
framework for tabular data based on game theory. We formulate feature selection
procedure based on a cooperative game where features are modeled as players,
and their importance is determined through the evaluation of synergistic
interactions and marginal contributions. The proposed framework comprises four
core components: sample selection, game-theoretic feature importance
evaluation, redundant feature elimination, and optimized model training.
Experimental results demonstrate that the proposed method achieves substantial
computation reduction while preserving predictive performance, thereby offering
an efficient solution of the computational challenges of large-scale machine
learning. The source code is available at
https://github.com/vectorsss/strategy_inputs.

</details>


### [17] [LRT-Diffusion: Calibrated Risk-Aware Guidance for Diffusion Policies](https://arxiv.org/abs/2510.24983)
*Ximan Sun,Xiang Cheng*

Main category: cs.LG

TL;DR: LRT-Diffusion 提供一种用于离线强化学习的风险感知采样规则，对每个去噪步骤将等效的无条件先验与状态条件策略头之间的对数似然比作为证据进行判断，通过一个 logistc 控制器对条件均值进行门控，阈值 tau 在 H0 下按用户设定的 Type-I 级别 alpha 标定。这将引导从固定的推动转变为基于证据的风险预算调整。训练保持不变，推理时混合 Q-梯度更新，标准化状态/动作，并在 D4RL MuJoCo 任务上改善返回-OOD 权衡。理论层面给出 alpha 级标定、稳定性界限和在离线 RL 中何时超越 Q-引导的比较。结论：LRT-Diffusion 是一个可直接替换的推理时方法，在离线 diffusion 策略上加入了可解释的、经标定的风险控制。


<details>
  <summary>Details</summary>
Motivation: 传统扩散策略在离线 RL 中常通过启发式引导，但缺乏统计意义的风险管理；需要在推理阶段引入可解释的风险控制，同时保持训练结构不变。

Method: 在每个去噪步骤中，将无条件先验与状态条件策略头之间的对数似然比作为证据进行序贯假设检验；累积对数似然比并用 Logistic 控制器对条件均值进行门控，门控阈值 tau 在 H0 条件下按 alpha 标定。训练阶段仍使用标准的两头 epsilon 预测 DDPM 结构不变。推理时，LRT 指导可与 Q-梯度相结合（在无条件均值、LRT 门控均值或二者混合处进行 critic-梯度更新），实现从开发者可控的保守性到开发者可控的利用性的一条连续线。状态/动作在训练和测试阶段统一标准化。

Result: 在 D4RL MuJoCo 任务上，LRT-Diffusion 相比强基线的 Q 指引，在给定 alpha 的情况下，提升了返回与 OOD 的权衡。理论部分给出 level-alpha 的标定、简洁的稳定性界限，以及在 Off-support 误差占主导时 LRT 如何超越 Q-引导的情形。

Conclusion: LRT-Diffusion 是一个推理时即可使用的 drop-in 方法，为离线 RL 的扩散策略引入可标定的风险控制，与现有的 Q-引导方法互补，能在遵循用户给定风险预算的前提下改善返回与 OOD 的权衡。

Abstract: Diffusion policies are competitive for offline reinforcement learning (RL)
but are typically guided at sampling time by heuristics that lack a statistical
notion of risk. We introduce LRT-Diffusion, a risk-aware sampling rule that
treats each denoising step as a sequential hypothesis test between the
unconditional prior and the state-conditional policy head. Concretely, we
accumulate a log-likelihood ratio and gate the conditional mean with a logistic
controller whose threshold tau is calibrated once under H0 to meet a
user-specified Type-I level alpha. This turns guidance from a fixed push into
an evidence-driven adjustment with a user-interpretable risk budget.
Importantly, we deliberately leave training vanilla (two heads with standard
epsilon-prediction) under the structure of DDPM. LRT guidance composes
naturally with Q-gradients: critic-gradient updates can be taken at the
unconditional mean, at the LRT-gated mean, or a blend, exposing a continuum
from exploitation to conservatism. We standardize states and actions
consistently at train and test time and report a state-conditional
out-of-distribution (OOD) metric alongside return. On D4RL MuJoCo tasks,
LRT-Diffusion improves the return-OOD trade-off over strong Q-guided baselines
in our implementation while honoring the desired alpha. Theoretically, we
establish level-alpha calibration, concise stability bounds, and a return
comparison showing when LRT surpasses Q-guidance-especially when off-support
errors dominate. Overall, LRT-Diffusion is a drop-in, inference-time method
that adds principled, calibrated risk control to diffusion policies for offline
RL.

</details>


### [18] [Enhancing Hierarchical Reinforcement Learning through Change Point Detection in Time Series](https://arxiv.org/abs/2510.24988)
*Hemanath Arumugam,Falong Fan,Bo Liu*

Main category: cs.LG

TL;DR: 通过将自监督的Transformer式变点检测（CPD）整合到Option-Critic框架，本文提出的结构化HRL方法能够自适应分割轨迹、并基于CPD定义的区间学习和分化，从而促进选项的自动发现、更快收敛和更高的性能。


<details>
  <summary>Details</summary>
Motivation: HRL在长时程任务中通过选项实现时间抽象，但难以自主发现有语义意义的子目标和合适的终止边界，导致学习效率低下。引入结构化先验（变点）以指导分段和选项学习，提升可解释性、鲁棒性和样本效率。

Method: 在Option-Critic框架中加入自监督、基于Transformer的变点检测模块CPD；用来自潜信号的伪标签对CPD进行监督，推断环境动力学的潜在变化点。变点用于：(i) 稳定终止梯度的监督信号，(ii) 通过分段行为克隆对内部策略进行预训练，(iii) 通过跨选项的发散惩罚在CPD定义的状态分区上实现功能专门化。总体目标是在标准actor-critic损失上叠加结构感知的辅助损失以提升学习。

Result: 在 Four-Rooms 与 Pinball 任务上，CPD 指导的智能体实现了更快的收敛、更高的累计回报以及显著的选项专门化提升，显示结合变点分段的结构先验可以提高HRL的可解释性、样本效率和鲁棒性。

Conclusion: 将结构化因果信息（通过CPD定义的状态分区）融入HRL可促进自监督的选项发现与分段学习，提升复杂任务中的学习效率与可解释性。

Abstract: Hierarchical Reinforcement Learning (HRL) enhances the scalability of
decision-making in long-horizon tasks by introducing temporal abstraction
through options-policies that span multiple timesteps. Despite its theoretical
appeal, the practical implementation of HRL suffers from the challenge of
autonomously discovering semantically meaningful subgoals and learning optimal
option termination boundaries. This paper introduces a novel architecture that
integrates a self-supervised, Transformer-based Change Point Detection (CPD)
module into the Option-Critic framework, enabling adaptive segmentation of
state trajectories and the discovery of options. The CPD module is trained
using heuristic pseudo-labels derived from intrinsic signals to infer latent
shifts in environment dynamics without external supervision. These inferred
change-points are leveraged in three critical ways: (i) to serve as supervisory
signals for stabilizing termination function gradients, (ii) to pretrain
intra-option policies via segment-wise behavioral cloning, and (iii) to enforce
functional specialization through inter-option divergence penalties over
CPD-defined state partitions. The overall optimization objective enhances the
standard actor-critic loss using structure-aware auxiliary losses. In our
framework, option discovery arises naturally as CPD-defined trajectory segments
are mapped to distinct intra-option policies, enabling the agent to
autonomously partition its behavior into reusable, semantically meaningful
skills. Experiments on the Four-Rooms and Pinball tasks demonstrate that
CPD-guided agents exhibit accelerated convergence, higher cumulative returns,
and significantly improved option specialization. These findings confirm that
integrating structural priors via change-point segmentation leads to more
interpretable, sample-efficient, and robust hierarchical policies in complex
environments.

</details>


### [19] [What Really Matters in Matrix-Whitening Optimizers?](https://arxiv.org/abs/2510.25000)
*Kevin Frans,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: 矩阵 whitening 优化器通过方差自适应发挥作用，而不仅仅是谱归一化；方差自适应变体优于符号下降；低秩方差估计在不损失性能的情况下降低内存。


<details>
  <summary>Details</summary>
Motivation: 系统地解构矩阵 whitening 优化器，找出驱动性能的关键组成部分，特别是方差自适应的作用。

Method: 在超参数上对不同矩阵 whitening 变体（如 SOAP、Muon 等）与元素级优化器（如 Adam）进行对比，系统性消融方差自适应策略，探索 Lookahead 的影响，以及低秩方差估计的内存权衡；通过多组实验验证各自的贡献与局限。

Result: 所有矩阵 whitening 变体在对比中普遍优于 Adam 等元素级对比；不仅谱归一化的准确性解释不了增益，SOAP 取得最大的每步增益；方差自适应是被忽视的关键成分，方差自适应版本优于符号下降版本，包括自适应的 Muon；消融实验显示 Lookahead 不如预期，低秩方差估计能在不损失性能的前提下降低内存成本。

Conclusion: 方差自适应是矩阵 whitening 成功背后的被忽视因素；谱归一化与方差自适应共同驱动性能提升，实际应用应优先采用方差自适应的变体，并探究低秩估计带来的内存效率。

Abstract: A range of recent optimizers have emerged that approximate the same
"matrix-whitening" transformation in various ways. In this work, we
systematically deconstruct such optimizers, aiming to disentangle the key
components that explain performance. Across tuned hyperparameters across the
board, all flavors of matrix-whitening methods reliably outperform elementwise
counterparts, such as Adam. Matrix-whitening is often related to spectral
descent -- however, experiments reveal that performance gains are *not
explained solely by accurate spectral normalization* -- particularly, SOAP
displays the largest per-step gain, even though Muon more accurately descends
along the steepest spectral descent direction. Instead, we argue that
matrix-whitening serves two purposes, and the variance adaptation component of
matrix-whitening is the overlooked ingredient explaining this performance gap.
Experiments show that variance-adapted versions of optimizers consistently
outperform their sign-descent counterparts, including an adaptive version of
Muon. We further ablate variance adaptation strategies, finding that while
lookahead style approximations are not as effective, low-rank variance
estimators can effectively reduce memory costs without a performance loss.

</details>


### [20] [Disentangling Shared and Private Neural Dynamics with SPIRE: A Latent Modeling Framework for Deep Brain Stimulation](https://arxiv.org/abs/2510.25023)
*Rahil Soroushmojdehi,Sina Javadzadeh,Mehrnaz Asadi,Terence D. Sanger*

Main category: cs.LG

TL;DR: SPIRE 通过将多区域神经数据分解为共享与私有潜在子空间，提出一种深度多编码器自编码器并引入对齐与解耦损失，在仅基线数据上即可恢复跨区域结构并揭示外部扰动重组；在仿真与 DBS 记录上表现优越，共享潜在分量对刺激特征具泛化能力，提供实用、可重复的跨区域分析工具。


<details>
  <summary>Details</summary>
Motivation: 解决多区域神经数据中的共享结构与区域特异性成分的分离，同时在外部扰动（如刺激）作用下仍能稳定地捕捉跨区域动力学。

Method: 提出 SPIRE：一个深度多编码器自编码器，将记录分解为共享与私有潜在子空间；设计了新的对齐与解耦损失以强化共享结构的对齐与私有子空间的区分；仅基线数据训练；在合成数据（有地面 truth 的潜在变量）与实际 DBS 记录上评估。

Result: 在带有地面真值的合成数据中，SPIRE 相较经典概率模型在存在非线性变形和时间错位时具有更强鲁棒性并实现更好的分解；在深脑刺激记录上，共享潜在分量稳定编码与刺激相关的签名，且具有跨站点与跨频率的泛化能力。

Conclusion: SPIRE 为多区域神经动力学分析提供一种实用、可重复的工具，特别适用于刺激干预情境下的跨区域结构揭示与迁移性分析。

Abstract: Disentangling shared network-level dynamics from region-specific activity is
a central challenge in modeling multi-region neural data. We introduce SPIRE
(Shared-Private Inter-Regional Encoder), a deep multi-encoder autoencoder that
factorizes recordings into shared and private latent subspaces with novel
alignment and disentanglement losses. Trained solely on baseline data, SPIRE
robustly recovers cross-regional structure and reveals how external
perturbations reorganize it. On synthetic benchmarks with ground-truth latents,
SPIRE outperforms classical probabilistic models under nonlinear distortions
and temporal misalignments. Applied to intracranial deep brain stimulation
(DBS) recordings, SPIRE shows that shared latents reliably encode
stimulation-specific signatures that generalize across sites and frequencies.
These results establish SPIRE as a practical, reproducible tool for analyzing
multi-region neural dynamics under stimulation.

</details>


### [21] [Machine Learning based Analysis for Radiomics Features Robustness in Real-World Deployment Scenarios](https://arxiv.org/abs/2510.25026)
*Sarmad Ahmad Khan,Simon Bernatz,Zahra Moslehi,Florian Buettner*

Main category: cs.LG

TL;DR: 在多序列 MRI 与不同分割策略下，基于可重复特征的 radiomics 模型对分布移位表现出更高鲁棒性；协议相关特征易受影响，数据增强改善不确定性校准；温度缩放对校准提升有限。


<details>
  <summary>Details</summary>
Motivation: 解决影像获取协议、定位与分割差异带来的分布移位对 radiomics 机器学习模型的影响，提升临床决策支持的鲁棒性。

Method: 在 5 种 MRI 序列（T2-HASTE、T2-TSE、T2-MAP、T1-TSE、T2-FLAIR）以及 16 种水果幻象 phantom 的设置下，比较不同获取协议与分割策略对模型可靠性的影响（分割：全量、局部、旋转；评估包括观察者间变异）。使用 XGBoost 在两组特征上训练模型：一组为 8 个一致鲁棒特征，另一组为序列特异特征；在 in-domain 与 out-of-domain 条件下评估预测力与不确定性。还进行数据增强与校准方法比较。

Result: 使用协议不变特征训练的模型在分布移位下的 F1 保持 >0.85；使用全特征的模型在协议变化时性能下降约 40%；数据增强显著提升不确定性估计质量，ECE 降低约 35% 而不降低准确性；温度缩放对校准收益有限，显示 XGBoost 的固有鲁棒性。

Conclusion: 协议感知的特征选择结合受控 phantom 研究可有效预测并提升 radiomics 模型在现实世界的分布移位下的行为，提供了一套应对真实协议变异的鲁棒 radiomics 开发框架。

Abstract: Radiomics-based machine learning models show promise for clinical decision
support but are vulnerable to distribution shifts caused by variations in
imaging protocols, positioning, and segmentation. This study systematically
investigates the robustness of radiomics-based machine learning models under
distribution shifts across five MRI sequences. We evaluated how different
acquisition protocols and segmentation strategies affect model reliability in
terms of predictive power and uncertainty-awareness. Using a phantom of 16
fruits, we evaluated distribution shifts through: (1) protocol variations
across T2-HASTE, T2-TSE, T2-MAP, T1-TSE, and T2-FLAIR sequences; (2)
segmentation variations (full, partial, rotated); and (3) inter-observer
variability. We trained XGBoost classifiers on 8 consistent robust features
versus sequence-specific features, testing model performance under in-domain
and out-of-domain conditions. Results demonstrate that models trained on
protocol-invariant features maintain F1-scores >0.85 across distribution
shifts, while models using all features showed 40% performance degradation
under protocol changes. Dataset augmentation substantially improved the quality
of uncertainty estimates and reduced the expected calibration error (ECE) by
35% without sacrificing accuracy. Temperature scaling provided minimal
calibration benefits, confirming XGBoost's inherent reliability. Our findings
reveal that protocol-aware feature selection and controlled phantom studies
effectively predict model behavior under distribution shifts, providing a
framework for developing robust radiomics models resilient to real-world
protocol variations.

</details>


### [22] [Graph Distance Based on Cause-Effect Estimands with Latents](https://arxiv.org/abs/2510.25037)
*Zhufeng Li,Niki Kilbertus*

Main category: cs.LG

TL;DR: 提出一种针对有向无环混合图（ADMG）的图距离度量，基于未观测混淆下的因果效应估计任务，通过固定识别和符号验证器来量化图差异对因果估计量的扭曲程度，并在不同图扰动下分析行为，并与现有距离度量比较。


<details>
  <summary>Details</summary>
Motivation: 在因果发现中，评价发现的图对因果推断的影响困难，尤其在存在潜在混淆时。本工作希望提供一个对下游因果估计更具直观和可比较性的距离度量。

Method: 以 ADMG 为对象，基于对因果效应的识别（fixing identification）和符号验证器（symbolic verifier）来量化图变动对因果 estimand 的影响。研究扰动对距离的影响行为，并与现有距离度量进行比较。

Result: 提出了一种新的距离度量并对其性质进行分析，展示其在不同图扰动下的行为，以及与现有度量的比较结果（具体数值未在摘要中给出）。

Conclusion: 该距离度量将因果估计的下游任务纳入评价框架，更直接地反映图结构变化对因果效应推断的影响，并可作为评估图生成与结构学习进展的工具。

Abstract: Causal discovery aims to recover graphs that represent causal relations among
given variables from observations, and new methods are constantly being
proposed. Increasingly, the community raises questions about how much progress
is made, because properly evaluating discovered graphs remains notoriously
difficult, particularly under latent confounding. We propose a graph distance
measure for acyclic directed mixed graphs (ADMGs) based on the downstream task
of cause-effect estimation under unobserved confounding. Our approach uses
identification via fixing and a symbolic verifier to quantify how graph
differences distort cause-effect estimands for different treatment-outcome
pairs. We analyze the behavior of the measure under different graph
perturbations and compare it against existing distance metrics.

</details>


### [23] [Dynamically Weighted Momentum with Adaptive Step Sizes for Efficient Deep Network Training](https://arxiv.org/abs/2510.25042)
*Zhifeng Wang,Longlong Li,Chunyan Zeng*

Main category: cs.LG

TL;DR: 提出了一种名为DWMGrad的动态引导优化算法，通过历史数据动态更新动量与学习率，提升收敛速度与准确性，适配多种训练场景。


<details>
  <summary>Details</summary>
Motivation: 现有的SGD/Adam在学习效率波动、复杂模型和非凸优化面临挑战，难以自适应地选择学习率、避免局部最优并处理高维空间。需要一种能结合历史信息、动态调整策略的优化器。

Method: 在传统优化方法基础上引入动态引导机制，利用历史数据动态更新动量与学习率，允许算法灵活地调整对历史信息的依赖，以适应不同训练任务与环境。

Result: 大量实验表明DWMGrad在多种场景下实现更快的收敛速度和更高的准确性。

Conclusion: 通过动态历史信息引导，DWMGrad能够自适应调整以应对变化的训练环境和任务复杂性，从而提升训练效率与模型性能。

Abstract: Within the current sphere of deep learning research, despite the extensive
application of optimization algorithms such as Stochastic Gradient Descent
(SGD) and Adaptive Moment Estimation (Adam), there remains a pronounced
inadequacy in their capability to address fluctuations in learning efficiency,
meet the demands of complex models, and tackle non-convex optimization issues.
These challenges primarily arise from the algorithms' limitations in handling
complex data structures and models, for instance, difficulties in selecting an
appropriate learning rate, avoiding local optima, and navigating through
high-dimensional spaces. To address these issues, this paper introduces a novel
optimization algorithm named DWMGrad. This algorithm, building on the
foundations of traditional methods, incorporates a dynamic guidance mechanism
reliant on historical data to dynamically update momentum and learning rates.
This allows the optimizer to flexibly adjust its reliance on historical
information, adapting to various training scenarios. This strategy not only
enables the optimizer to better adapt to changing environments and task
complexities but also, as validated through extensive experimentation,
demonstrates DWMGrad's ability to achieve faster convergence rates and higher
accuracies under a multitude of scenarios.

</details>


### [24] [Continual Low-Rank Adapters for LLM-based Generative Recommender Systems](https://arxiv.org/abs/2510.25093)
*Hyunsik Yoo,Ting-Wei Li,SeongKu Kang,Zhining Liu,Charlie Xu,Qilin Qi,Hanghang Tong*

Main category: cs.LG

TL;DR: PESO 提出了一种在推荐场景中的连贯学习方法，通过近端正则化将当前 LoRA 适配器锚定在最近的冻结状态，从而在适应新用户行为与保持旧偏好之间实现平衡。


<details>
  <summary>Details</summary>
Motivation: 在推荐系统中，用户和物品的偏好随时间变化，传统的基于保留旧任务的持续学习方法可能过于强调旧偏好，反而在当前兴趣显著变化时降低性能。需要一个能数据感知地引导 LoRA 子空间的适配策略。

Method: PESO 在 LoRA 的适配层引入近端正则化项，约束当前适配器与最近的冻结状态的接近度，使其在快速适应与保留历史之间取得权衡。理论上给出在 LoRA 子空间的方向性、数据感知的引导。

Result: 在实验上，PESO 及其变体在基于 LoRA 的持续学习方法中实现了更好的性能，表现出对新用户行为的更强适应能力和对历史信息的有效控制。

Conclusion: 通过近端正则化，PESO 提供了一种在推荐中的持续学习的有效权衡机制，理论与实验均支持其优于现有 LoRA 基线的方法。

Abstract: While large language models (LLMs) achieve strong performance in
recommendation, they face challenges in continual learning as users, items, and
user preferences evolve over time. Existing LoRA-based continual methods
primarily focus on preserving performance on previous tasks, but this overlooks
the unique nature of recommendation: the goal is not to predict past
preferences, and outdated preferences can even harm performance when current
interests shift significantly. To address this, we propose PESO (Proximally
rEgularized Single evolving lOra, a continual adaptation method for LoRA in
recommendation. PESO introduces a proximal regularizer that anchors the current
adapter to its most recent frozen state, enabling the model to flexibly balance
adaptation and preservation, and to better capture recent user behaviors.
Theoretically, we show that this proximal design provides data-aware,
direction-wise guidance in the LoRA subspace. Empirically, PESO consistently
outperforms existing LoRA-based continual learning methods.

</details>


### [25] [Learning Fair Graph Representations with Multi-view Information Bottleneck](https://arxiv.org/abs/2510.25096)
*Chuxun Liu,Debo Cheng,Qingfeng Chen,Jiangzhang Gan,Jiuyong Li,Lin Liu*

Main category: cs.LG

TL;DR: 提出 FairMIB：一个多视角信息瓶颈框架，通过将图分解为特征、结构和扩散视图来缓解 GNN 的复杂性偏见，并利用对比学习与信息瓶颈在多视角上平衡任务性能与公平性，且在扩散视图中引入逆概率加权的邻接矫正以减缓偏见传播。


<details>
  <summary>Details</summary>
Motivation: GNN 在传播信息时会放大训练数据中的偏见，导致不公平结果；现有方法将偏见视为单一源，忽略特征与结构的多重影响，导致公平性与效用之间的权衡不佳。

Method: 将图分解成特征、结构、扩散三视图，通过对比学习最大化跨视图互信息以获得偏见消除的表征；引入多视角信息瓶颈目标以在保护敏感属性信息的同时平衡任务效用与公平性；在扩散视图中加入逆概率加权的邻接矫正以减缓偏见在消息传递中的扩散。

Result: 在五个真实数据集上实现了在效用与公平性指标上都达到或接近最优的性能（state-of-the-art）。

Conclusion: 通过多视角信息瓶颈与对比学习的组合，FairMIB 能在保持任务性能的同时显著提升公平性，有效缓解 GNN 的偏见传播问题。

Abstract: Graph neural networks (GNNs) excel on relational data by passing messages
over node features and structure, but they can amplify training data biases,
propagating discriminatory attributes and structural imbalances into unfair
outcomes. Many fairness methods treat bias as a single source, ignoring
distinct attribute and structure effects and leading to suboptimal fairness and
utility trade-offs. To overcome this challenge, we propose FairMIB, a
multi-view information bottleneck framework designed to decompose graphs into
feature, structural, and diffusion views for mitigating complexity biases in
GNNs. Especially, the proposed FairMIB employs contrastive learning to maximize
cross-view mutual information for bias-free representation learning. It further
integrates multi-perspective conditional information bottleneck objectives to
balance task utility and fairness by minimizing mutual information with
sensitive attributes. Additionally, FairMIB introduces an inverse
probability-weighted (IPW) adjacency correction in the diffusion view, which
reduces the spread of bias propagation during message passing. Experiments on
five real-world benchmark datasets demonstrate that FairMIB achieves
state-of-the-art performance across both utility and fairness metrics.

</details>


### [26] [A Unified Bilevel Model for Adversarial Learning and A Case Study](https://arxiv.org/abs/2510.25121)
*Yutong Zheng,Qingna Li*

Main category: cs.LG

TL;DR: 提出一个用于对抗学习的统一双层优化模型，并将其应用于聚类中的对抗攻击分析，借助数据扰动解释鲁棒性与攻击性，同时引入δ-测度来量化攻击效果。


<details>
  <summary>Details</summary>
Motivation: 理解对抗攻击在复杂模型中的机制，特别是在聚类等无监督任务中，现有方法难以解释攻击效果。需要一个统一框架来量化扰动对聚类结果的影响。

Method: 提出双层优化框架用于对抗学习；从数据扰动角度分析聚类模型的鲁棒性，指出小扰动时鲁棒性好，大扰动时结果改变构成攻击；定义并分析δ-测度，在所提双层框架中用于衡量攻击效果。

Result: 理论上揭示了扰动规模与聚类结果之间的关系；在框架内给出δ-测度的可定义性分析，作为评估攻击效果和鲁棒性的工具。

Conclusion: 该双层对抗学习框架为聚类等无监督任务的对抗分析提供了统一视角和可量化指标δ-测度，揭示了扰动阈值下的鲁棒性区间及攻击条件。

Abstract: Adversarial learning has been attracting more and more attention thanks to
the fast development of machine learning and artificial intelligence. However,
due to the complicated structure of most machine learning models, the mechanism
of adversarial attacks is not well interpreted. How to measure the effect of
attack is still not quite clear. In this paper, we propose a unified bilevel
model for adversarial learning. We further investigate the adversarial attack
in clustering models and interpret it from data perturbation point of view. We
reveal that when the data perturbation is relatively small, the clustering
model is robust, whereas if it is relatively large, the clustering result
changes, which leads to an attack. To measure the effect of attacks for
clustering models, we analyse the well-definedness of the so-called
$\delta$-measure, which can be used in the proposed bilevel model for
adversarial learning of clustering models.

</details>


### [27] [Lipschitz-aware Linearity Grafting for Certified Robustness](https://arxiv.org/abs/2510.25130)
*Yongjin Han,Suhyun Kim*

Main category: cs.LG

TL;DR: 通过将线性特性嵌入非线性激活中来减少近似误差，从而紧化 l_infty 局部 Lipschitz 常数并提升有证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 认证鲁棒性高度依赖于局部 Lipschitz 常数。求解最坏对抗样本是 NP-hard，现有的近似/放宽方法易产生较大误差，阻碍紧界和鲁棒性提升。

Method: 将线性 grafting 引入非线性激活，以消除主导的近似误差；从 l_infty 局部 Lipschitz 的角度理论分析其作用机制；提出 Lipschitz-aware 的线性 grafting，且在不需要认证训练的情况下提升鲁棒性。

Result: 理论分析揭示线性 grafting 如何通过减小近似误差来 tightening 局部 Lipschitz；大量实验表明该方法能显著提高 l_infty 局部 Lipschitz 的紧性，进而提升认证鲁棒性。

Conclusion: 线性 grafting 是减少近似误差、从而提升基于 Lipschitz 的认证鲁棒性的一种有效策略，可扩展到更多激活函数和模型结构，未来可进一步分析在不同数据集和攻击设置的鲁棒性。

Abstract: Lipschitz constant is a fundamental property in certified robustness, as
smaller values imply robustness to adversarial examples when a model is
confident in its prediction. However, identifying the worst-case adversarial
examples is known to be an NP-complete problem. Although over-approximation
methods have shown success in neural network verification to address this
challenge, reducing approximation errors remains a significant obstacle.
Furthermore, these approximation errors hinder the ability to obtain tight
local Lipschitz constants, which are crucial for certified robustness.
Originally, grafting linearity into non-linear activation functions was
proposed to reduce the number of unstable neurons, enabling scalable and
complete verification. However, no prior theoretical analysis has explained how
linearity grafting improves certified robustness. We instead consider linearity
grafting primarily as a means of eliminating approximation errors rather than
reducing the number of unstable neurons, since linear functions do not require
relaxation. In this paper, we provide two theoretical contributions: 1) why
linearity grafting improves certified robustness through the lens of the
$l_\infty$ local Lipschitz constant, and 2) grafting linearity into non-linear
activation functions, the dominant source of approximation errors, yields a
tighter local Lipschitz constant. Based on these theoretical contributions, we
propose a Lipschitz-aware linearity grafting method that removes dominant
approximation errors, which are crucial for tightening the local Lipschitz
constant, thereby improving certified robustness, even without certified
training. Our extensive experiments demonstrate that grafting linearity into
these influential activations tightens the $l_\infty$ local Lipschitz constant
and enhances certified robustness.

</details>


### [28] [Machine Learning Guided Optimal Transmission Switching to Mitigate Wildfire Ignition Risk](https://arxiv.org/abs/2510.25147)
*Weimin Huang,Ryan Piansky,Bistra Dilkina,Daniel K. Molzahn*

Main category: cs.LG

TL;DR: 提出一种 ML 指导的 MILP 求解框架，用于快速给出高质量的 OPS 去/留线路决策，降低火灾风险并减少负荷丢失，在大型真实测试系统上比传统优化方法更快且解质量更好。


<details>
  <summary>Details</summary>
Motivation: 在高 wildfire 风险区域需要去 energize（去激活）输电线路以降低火灾点火风险，同时 OPS 问题本质上是难以快速求解的 MILP；多实例之间共享结构但参数各异，催生对快速求解的需求。

Method: 在现有 ML 指导 MILP 的基础上扩展，结合域知识（需要通断的线路数等约束）构建 ML-guided 框架，利用 wildfire 风险、负荷、可再生能源等参数的共性结构，进行快速决策生成；在大规模的加州合成测试系统上验证性能。

Result: ML 指导方法在解质和求解速度上优于传统优化方法，能够快速产生高质量的去激活/激活线路决策，且适用于大规模系统。

Conclusion: 以 ML 指导的 OPS 求解框架具有良好可扩展性，能够在高风险情境下平衡火灾风险控制与供电可靠性，提升运营决策的效率。

Abstract: To mitigate acute wildfire ignition risks, utilities de-energize power lines
in high-risk areas. The Optimal Power Shutoff (OPS) problem optimizes line
energization statuses to manage wildfire ignition risks through
de-energizations while reducing load shedding. OPS problems are computationally
challenging Mixed-Integer Linear Programs (MILPs) that must be solved rapidly
and frequently in operational settings. For a particular power system, OPS
instances share a common structure with varying parameters related to wildfire
risks, loads, and renewable generation. This motivates the use of Machine
Learning (ML) for solving OPS problems by exploiting shared patterns across
instances. In this paper, we develop an ML-guided framework that quickly
produces high-quality de-energization decisions by extending existing ML-guided
MILP solution methods while integrating domain knowledge on the number of
energized and de-energized lines. Results on a large-scale realistic
California-based synthetic test system show that the proposed ML-guided method
produces high-quality solutions faster than traditional optimization methods.

</details>


### [29] [Selective Learning for Deep Time Series Forecasting](https://arxiv.org/abs/2510.25207)
*Yisong Fu,Zezhi Shao,Chengqing Yu,Yujie Li,Zhulin An,Qi Wang,Yongjun Xu,Fei Wang*

Main category: cs.LG

TL;DR: 提出了一种选择性学习策略用于深度时间序列预测，通过双掩码筛选时步来计算MSE损失，聚焦于可泛化的时步，排除不泛化和异常时步；在八个真实数据集上显著提升多种模型的预测性能（Informer 37.4%、TimesNet 8.4%、iTransformer 6.5% MSE 下降）。


<details>
  <summary>Details</summary>
Motivation: 深度时间序列预测易过拟合，原因是对所有时步统一优化的MSE损失对噪声和异常敏感，导致难以泛化；需要在训练中区分不同时步的重要性，避免被非泛化时步牵制。

Method: 提出双掩码机制：1) 不确定性掩码（基于残差熵筛选不确定时步），2) 异常掩码（基于残差下界估计排除异常时步），在优化中仅对选中的子集时步计算MSE，从而引导模型关注可泛化时步。

Result: 在八个真实数据集上的广泛实验表明，Selective Learning 能显著提升现有深度TSF模型的预测性能，具体包括 Informer 的 MSE 降幅达到 37.4%，TimesNet 8.4%，以及 iTransformer 6.5%。

Conclusion: Selective Learning 能提升主流深度TSF模型的鲁棒性和泛化性，提供一种有效的减轻过拟合的方法；未来可进一步探究掩码策略的鲁棒性、与其他鲁棒损失的结合、以及在更多任务场景中的适用性。

Abstract: Benefiting from high capacity for capturing complex temporal patterns, deep
learning (DL) has significantly advanced time series forecasting (TSF).
However, deep models tend to suffer from severe overfitting due to the inherent
vulnerability of time series to noise and anomalies. The prevailing DL paradigm
uniformly optimizes all timesteps through the MSE loss and learns those
uncertain and anomalous timesteps without difference, ultimately resulting in
overfitting. To address this, we propose a novel selective learning strategy
for deep TSF. Specifically, selective learning screens a subset of the whole
timesteps to calculate the MSE loss in optimization, guiding the model to focus
on generalizable timesteps while disregarding non-generalizable ones. Our
framework introduces a dual-mask mechanism to target timesteps: (1) an
uncertainty mask leveraging residual entropy to filter uncertain timesteps, and
(2) an anomaly mask employing residual lower bound estimation to exclude
anomalous timesteps. Extensive experiments across eight real-world datasets
demonstrate that selective learning can significantly improve the predictive
performance for typical state-of-the-art deep models, including 37.4% MSE
reduction for Informer, 8.4% for TimesNet, and 6.5% for iTransformer.

</details>


### [30] [Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning](https://arxiv.org/abs/2510.25226)
*Miao Zhang,Junpeng Li,Changchun Hua,Yana Yang*

Main category: cs.LG

TL;DR: 提出一种自适应损失加权的代价敏感多类PU学习方法，在经验风险最小化框架中对正样本与从未标注数据中推断出的负样本损失赋予数据自适应权重，使经验风险成为目标风险的无偏估计，并给出泛化误差界及在八个公开数据集上的实验结果。


<details>
  <summary>Details</summary>
Motivation: 多类别PU学习中往往难以实现无偏风险估计，直接将未标注数据作为负样本会导致偏差，影响性能与稳定性；缺乏统一的理论框架解释其泛化能力。

Method: 在经验风险最小化框架内引入数据自适应的损失权重，对正样本与推断出的负样本的损失分别赋予不同权重，使整体损失成为目标风险的无偏估计。系统化地建模 MPU 数据生成过程，并给出通用的泛化误差界。

Result: 在八个公开数据集上，覆盖不同的类先验和类别数，所提方法在准确性与稳定性方面相对于强基线表现出一致提升。

Conclusion: 通过无偏风险估计和自适应损失权重，所提出的 MPU 学习方法在多类 PU 场景下实现更稳健的推断，并给出理论与实证双重支撑。

Abstract: Positive--Unlabeled (PU) learning considers settings in which only positive
and unlabeled data are available, while negatives are missing or left
unlabeled. This situation is common in real applications where annotating
reliable negatives is difficult or costly. Despite substantial progress in PU
learning, the multi-class case (MPU) remains challenging: many existing
approaches do not ensure \emph{unbiased risk estimation}, which limits
performance and stability. We propose a cost-sensitive multi-class PU method
based on \emph{adaptive loss weighting}. Within the empirical risk minimization
framework, we assign distinct, data-dependent weights to the positive and
\emph{inferred-negative} (from the unlabeled mixture) loss components so that
the resulting empirical objective is an unbiased estimator of the target risk.
We formalize the MPU data-generating process and establish a generalization
error bound for the proposed estimator. Extensive experiments on \textbf{eight}
public datasets, spanning varying class priors and numbers of classes, show
consistent gains over strong baselines in both accuracy and stability.

</details>


### [31] [BSFA: Leveraging the Subspace Dichotomy to Accelerate Neural Network Training](https://arxiv.org/abs/2510.25244)
*Wenjie Zhou,Bohan Wang,Wei Chen,Xueqi Cheng*

Main category: cs.LG

TL;DR: 提出 Bulk-Space-Filtration-Accelerator（BSFA），通过对梯度更新在 Dom-space（损失 Hessian 的前导特征方向）与 Bulk-space（正交子空间）进行差异化放缩来加速训练，同时增强稳定性。利用对历史更新的 PCA 子空间估计与按参数块的策略，使 BSFA 具备可扩展性和可实用性。在多任务上显示显著加速，尤其在 LLaMA-72M/134M 的预训练任务中与 AdamW 相比获得约 2 倍训练速度提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习优化中存在一个重要现象：更新在顶特征方向贡献大但对损失下降作用小，而正交的 Bulk-space 更新虽幅度较小却驱动大部分学习进展。需要一种可插拔、可扩展的框架，差异性地处理两类子空间更新，以提升训练速度与稳定性，尤其适用于当下的大型模型。

Method: 提出 BSFA 框架：在每次迭代中将梯度或更新投影到 Dom-space 与 Bulk-space，分别对两者进行不同的缩放；利用对历史更新的主成分分析（PCA）估计子空间并在“按参数块”的粒度上应用；这是一种 plug-and-play 的实现，且设计了高效的子空间估计器以降低额外开销。最终将两部分分量重新组合后用于优化器（如 AdamW）的更新。

Result: 在多项任务上展现加速效果，特别是在预训练 LLaMA-72M（WikiText-103 数据集）与 LLaMA-134M（OpenWebText 数据集）上，与常规模型 AdamW 相比，获得大约 2 倍的训练速度提升。

Conclusion: BSFA 提供一个实用且可扩展的训练加速框架，通过对 Dom-space 与 Bulk-space 的差异化处理来提升收敛速度并保持稳定性，且通过 PCA 基于历史更新的子空间估计与块级实现确保了计算成本可控。

Abstract: Recent studies \citep{gur2018gradient,song2024does, wen2024understanding}
highlight a fundamental dichotomy in deep learning optimization: Although
parameter updates along the top eigendirections of the loss Hessian (Dom-space)
capture most of the update magnitude, they often contribute minimally to loss
reduction. In contrast, updates in the orthogonal component (Bulk-space) have
smaller magnitudes but drive most learning progress. In this work, we further
advance the understanding of this phenomenon and introduce the
\textbf{Bulk-Space-Filtration-Accelerator (BSFA)}, a novel plug-and-play
framework. BSFA accelerates training by differentially scaling update
components projected onto these distinct subspaces, simultaneously enhancing
stability by moderating updates in the dominant subspace and boosting
convergence speed by amplifying those in the bulk-space. To ensure BSFA is both
practical and scalable for contemporary large models, we introduce two key
innovations: an efficient estimator using Principal Component Analysis (PCA) on
historical updates for fast subspace estimation, and a block-wise strategy that
applies this estimation on a per-parameter-block basis. These designs make BSFA
computationally tractable and highly effective. We demonstrate BSFA's
acceleration across various tasks, notably achieving approximately 2$\times$
speedup when pre-training LLaMA-72M on WikiText-103 and LLaMA-134M on
OpenWebText compared to vanilla AdamW.

</details>


### [32] [IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning](https://arxiv.org/abs/2510.25262)
*Xiandong Zou,Pan Zhou*

Main category: cs.LG

TL;DR: IBNorm是一种受信息瓶颈启发的归一化方法，通过有限制的压缩操作，在保留预测信息的同时抑制干扰变异，从而在保持稳定性的同时提升信息表达能力，并在语言与视觉模型上优于常规归一化。


<details>
  <summary>Details</summary>
Motivation: 现有归一化方法（BatchNorm、LayerNorm、RMSNorm）以方差为中心，强制零均值和单位方差，但并不显式控制表示学习到的与任务相关的信息量。需要一种能够在稳定训练的同时提高对任务相关信息保留的归一化方案。

Method: 提出IBNorm及其家族，通过受信息瓶颈原理启发的有界压缩操作，使嵌入在抑制无关变异的同时尽量保留预测信息。给出理论分析，证明IBNorm比方差中心的归一化在信息瓶颈价值(IB value)上更高且具有更紧的泛化界限。通过在大规模语言模型（如LLaMA、GPT-2）与视觉模型（ResNet、ViT）上的实验，结合互信息分析，验证其信息瓶颈行为优于传统归一化。

Result: IBNorm在大模型和视觉模型上持续超越BatchNorm、LayerNorm、RMSNorm；互信息分析显示更强的信息瓶颈特性。将公开代码。

Conclusion: IBNorm提供更具信息性的表达，同时保留标准归一化的稳定性与兼容性；理论上具备更高的IB值和更紧的泛化界限，实验上对多种模型具备实证效用。

Abstract: Normalization is fundamental to deep learning, but existing approaches such
as BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zero
mean and unit variance, stabilizing training without controlling how
representations capture task-relevant information. We propose IB-Inspired
Normalization (IBNorm), a simple yet powerful family of methods grounded in the
Information Bottleneck principle. IBNorm introduces bounded compression
operations that encourage embeddings to preserve predictive information while
suppressing nuisance variability, yielding more informative representations
while retaining the stability and compatibility of standard normalization.
Theoretically, we prove that IBNorm achieves a higher IB value and tighter
generalization bounds than variance-centric methods. Empirically, IBNorm
consistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scale
language models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutual
information analysis confirming superior information bottleneck behavior. Code
will be released publicly.

</details>


### [33] [On the Stability of Neural Networks in Deep Learning](https://arxiv.org/abs/2510.25282)
*Blaise Delattre*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep learning has achieved remarkable success across a wide range of tasks,
but its models often suffer from instability and vulnerability: small changes
to the input may drastically affect predictions, while optimization can be
hindered by sharp loss landscapes. This thesis addresses these issues through
the unifying perspective of sensitivity analysis, which examines how neural
networks respond to perturbations at both the input and parameter levels.
  We study Lipschitz networks as a principled way to constrain sensitivity to
input perturbations, thereby improving generalization, adversarial robustness,
and training stability. To complement this architectural approach, we introduce
regularization techniques based on the curvature of the loss function,
promoting smoother optimization landscapes and reducing sensitivity to
parameter variations. Randomized smoothing is also explored as a probabilistic
method for enhancing robustness at decision boundaries.
  By combining these perspectives, we develop a unified framework where
Lipschitz continuity, randomized smoothing, and curvature regularization
interact to address fundamental challenges in stability. The thesis contributes
both theoretical analysis and practical methodologies, including efficient
spectral norm computation, novel Lipschitz-constrained layers, and improved
certification procedures.

</details>


### [34] [Hierarchical Physics-Embedded Learning for Spatiotemporal Dynamical Systems](https://arxiv.org/abs/2510.25306)
*Xizhe Wang,Xiaobin Song,Qingshan Jia,Hongbo Zhao,Benben Jiang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modeling complex spatiotemporal dynamics, particularly in
far-from-equilibrium systems, remains a grand challenge in science. The
governing partial differential equations (PDEs) for these systems are often
intractable to derive from first principles, due to their inherent complexity,
characterized by high-order derivatives and strong nonlinearities, coupled with
incomplete physical knowledge. This has spurred the development of data-driven
methods, yet these approaches face limitations: Purely data-driven models are
often physically inconsistent and data-intensive, while existing
physics-informed methods lack the structural capacity to represent complex
operators or systematically integrate partial physical knowledge. Here, we
propose a hierarchical physics-embedded learning framework that fundamentally
advances both the forward spatiotemporal prediction and inverse discovery of
physical laws from sparse and noisy data. The key innovation is a two-level
architecture that mirrors the process of scientific discovery: the first level
learns fundamental symbolic components of a PDE, while the second learns their
governing combinations. This hierarchical decomposition not only reduces
learning complexity but, more importantly, enables a structural integration of
prior knowledge. Known physical laws are directly embedded into the models
computational graph, guaranteeing physical consistency and improving data
efficiency. By building the framework upon adaptive Fourier Neural Operators,
we can effectively capture the non-local dependencies and high-order operators
characteristic of dynamical systems. Additionally, by structurally decoupling
known and unknown terms, the framework further enables interpretable discovery
of underlying governing equations through symbolic regression, without
presupposing functional forms.

</details>


### [35] [Dense and Diverse Goal Coverage in Multi Goal Reinforcement Learning](https://arxiv.org/abs/2510.25311)
*Sagalpreet Singh,Rishi Saket,Aravindan Raghuveer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning algorithms are primarily focused on learning a policy
that maximizes expected return. As a result, the learned policy can exploit one
or few reward sources. However, in many natural situations, it is desirable to
learn a policy that induces a dispersed marginal state distribution over
rewarding states, while maximizing the expected return which is typically tied
to reaching a goal state. This aspect remains relatively unexplored. Existing
techniques based on entropy regularization and intrinsic rewards use
stochasticity for encouraging exploration to find an optimal policy which may
not necessarily lead to dispersed marginal state distribution over rewarding
states. Other RL algorithms which match a target distribution assume the latter
to be available apriori. This may be infeasible in large scale systems where
enumeration of all states is not possible and a state is determined to be a
goal state only upon reaching it. We formalize the problem of maximizing the
expected return while uniformly visiting the goal states as Multi Goal RL in
which an oracle classifier over the state space determines the goal states. We
propose a novel algorithm that learns a high-return policy mixture with
marginal state distribution dispersed over the set of goal states. Our
algorithm is based on optimizing a custom RL reward which is computed - based
on the current policy mixture - at each iteration for a set of sampled
trajectories. The latter are used via an offline RL algorithm to update the
policy mixture. We prove performance guarantees for our algorithm, showing
efficient convergence bounds for optimizing a natural objective which captures
the expected return as well as the dispersion of the marginal state
distribution over the goal states. We design and perform experiments on
synthetic MDPs and standard RL environments to evaluate the effectiveness of
our algorithm.

</details>


### [36] [CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices](https://arxiv.org/abs/2510.25323)
*Xuchen Feng,Siyu Liao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Normalizing flows are deep generative models that enable efficient likelihood
estimation and sampling through invertible transformations. A key challenge is
to design linear layers that enhance expressiveness while maintaining efficient
computation of the Jacobian determinant and inverse. We introduce a novel
invertible linear layer based on the product of circulant and diagonal
matrices. This decomposition reduces parameter complexity from
$\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$ using $m$ diagonal matrices and $m-1$
circulant matrices while still approximating general linear transformations. By
leveraging the Fast Fourier Transform, our approach reduces the time complexity
of matrix inversion from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn\log n)$ and that
of computing the log-determinant from $\mathcal{O}(n^3)$ to $\mathcal{O}(mn)$,
where $n$ is the input dimension. We build upon this layer to develop
Circulant-Diagonal Flow (CDFlow), which achieves strong density estimation on
natural image datasets and effectively models data with inherent periodic
structure. Furthermore, CDFlow significantly accelerates key operations in
normalizing flows, providing practical benefits for scalable generative
modeling.

</details>


### [37] [Beyond Leakage and Complexity: Towards Realistic and Efficient Information Cascade Prediction](https://arxiv.org/abs/2510.25348)
*Jie Peng,Rui Wang,Qiang Wang,Zhewei Wei,Bin Tong,Guan Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Information cascade popularity prediction is a key problem in analyzing
content diffusion in social networks. However, current related works suffer
from three critical limitations: (1) temporal leakage in current
evaluation--random cascade-based splits allow models to access future
information, yielding unrealistic results; (2) feature-poor datasets that lack
downstream conversion signals (e.g., likes, comments, or purchases), which
limits more practical applications; (3) computational inefficiency of complex
graph-based methods that require days of training for marginal gains. We
systematically address these challenges from three perspectives: task setup,
dataset construction, and model design. First, we propose a time-ordered
splitting strategy that chronologically partitions data into consecutive
windows, ensuring models are evaluated on genuine forecasting tasks without
future information leakage. Second, we introduce Taoke, a large-scale
e-commerce cascade dataset featuring rich promoter/product attributes and
ground-truth purchase conversions--capturing the complete diffusion lifecycle
from promotion to monetization. Third, we develop CasTemp, a lightweight
framework that efficiently models cascade dynamics through temporal walks,
Jaccard-based neighbor selection for inter-cascade dependencies, and GRU-based
encoding with time-aware attention. Under leak-free evaluation, CasTemp
achieves state-of-the-art performance across four datasets with
orders-of-magnitude speedup. Notably, it excels at predicting second-stage
popularity conversions--a practical task critical for real-world applications.

</details>


### [38] [Analysis of Semi-Supervised Learning on Hypergraphs](https://arxiv.org/abs/2510.25354)
*Adrien Weihs,Andrea Bertozzi,Matthew Thorpe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hypergraphs provide a natural framework for modeling higher-order
interactions, yet their theoretical underpinnings in semi-supervised learning
remain limited. We provide an asymptotic consistency analysis of variational
learning on random geometric hypergraphs, precisely characterizing the
conditions ensuring the well-posedness of hypergraph learning as well as
showing convergence to a weighted $p$-Laplacian equation. Motivated by this, we
propose Higher-Order Hypergraph Learning (HOHL), which regularizes via powers
of Laplacians from skeleton graphs for multiscale smoothness. HOHL converges to
a higher-order Sobolev seminorm. Empirically, it performs strongly on standard
baselines.

</details>


### [39] [Parameter Averaging in Link Prediction](https://arxiv.org/abs/2510.25361)
*Rupesh Sapkota,Caglar Demir,Arnab Sharma,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ensemble methods are widely employed to improve generalization in machine
learning. This has also prompted the adoption of ensemble learning for the
knowledge graph embedding (KGE) models in performing link prediction. Typical
approaches to this end train multiple models as part of the ensemble, and the
diverse predictions are then averaged. However, this approach has some
significant drawbacks. For instance, the computational overhead of training
multiple models increases latency and memory overhead. In contrast, model
merging approaches offer a promising alternative that does not require training
multiple models. In this work, we introduce model merging, specifically
weighted averaging, in KGE models. Herein, a running average of model
parameters from a training epoch onward is maintained and used for predictions.
To address this, we additionally propose an approach that selectively updates
the running average of the ensemble model parameters only when the
generalization performance improves on a validation dataset. We evaluate these
two different weighted averaging approaches on link prediction tasks, comparing
the state-of-the-art benchmark ensemble approach. Additionally, we evaluate the
weighted averaging approach considering literal-augmented KGE models and
multi-hop query answering tasks as well. The results demonstrate that the
proposed weighted averaging approach consistently improves performance across
diverse evaluation settings.

</details>


### [40] [A Convexity-dependent Two-Phase Training Algorithm for Deep Neural Networks](https://arxiv.org/abs/2510.25366)
*Tomas Hrycej,Bernhard Bermeitinger,Massimo Pavone,Götz-Henrik Wiegand,Siegfried Handschuh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The key task of machine learning is to minimize the loss function that
measures the model fit to the training data. The numerical methods to do this
efficiently depend on the properties of the loss function. The most decisive
among these properties is the convexity or non-convexity of the loss function.
The fact that the loss function can have, and frequently has, non-convex
regions has led to a widespread commitment to non-convex methods such as Adam.
However, a local minimum implies that, in some environment around it, the
function is convex. In this environment, second-order minimizing methods such
as the Conjugate Gradient (CG) give a guaranteed superlinear convergence. We
propose a novel framework grounded in the hypothesis that loss functions in
real-world tasks swap from initial non-convexity to convexity towards the
optimum. This is a property we leverage to design an innovative two-phase
optimization algorithm. The presented algorithm detects the swap point by
observing the gradient norm dependence on the loss. In these regions,
non-convex (Adam) and convex (CG) algorithms are used, respectively. Computing
experiments confirm the hypothesis that this simple convexity structure is
frequent enough to be practically exploited to substantially improve
convergence and accuracy.

</details>


### [41] [Position: Biology is the Challenge Physics-Informed ML Needs to Evolve](https://arxiv.org/abs/2510.25368)
*Julien Martinelli*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Physics-Informed Machine Learning (PIML) has successfully integrated
mechanistic understanding into machine learning, particularly in domains
governed by well-known physical laws. This success has motivated efforts to
apply PIML to biology, a field rich in dynamical systems but shaped by
different constraints. Biological modeling, however, presents unique
challenges: multi-faceted and uncertain prior knowledge, heterogeneous and
noisy data, partial observability, and complex, high-dimensional networks. In
this position paper, we argue that these challenges should not be seen as
obstacles to PIML, but as catalysts for its evolution. We propose
Biology-Informed Machine Learning (BIML): a principled extension of PIML that
retains its structural grounding while adapting to the practical realities of
biology. Rather than replacing PIML, BIML retools its methods to operate under
softer, probabilistic forms of prior knowledge. We outline four foundational
pillars as a roadmap for this transition: uncertainty quantification,
contextualization, constrained latent structure inference, and scalability.
Foundation Models and Large Language Models will be key enablers, bridging
human expertise with computational modeling. We conclude with concrete
recommendations to build the BIML ecosystem and channel PIML-inspired
innovation toward challenges of high scientific and societal relevance.

</details>


### [42] [GPTOpt: Towards Efficient LLM-Based Black-Box Optimization](https://arxiv.org/abs/2510.25404)
*Jamison Meindl,Yunsheng Tian,Tony Cui,Veronika Thost,Zhang-Wei Hong,Jie Chen,Wojciech Matusik,Mina Konaković Luković*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Global optimization of expensive, derivative-free black-box functions demands
extreme sample efficiency. Classical methods such as Bayesian Optimization (BO)
can be effective, but they often require careful parameter tuning to each
application domain. At the same time, Large Language Models (LLMs) have shown
broad capabilities, yet state-of-the-art models remain limited in solving
continuous black-box optimization tasks. We introduce GPTOpt, an LLM-based
optimization method that equips LLMs with continuous black-box optimization
capabilities. By fine-tuning large language models on extensive synthetic
datasets derived from diverse BO parameterizations, GPTOpt leverages LLM
pre-training to generalize across optimization tasks. On a variety of black-box
optimization benchmarks, GPTOpt surpasses traditional optimizers, highlighting
the capacity of LLMs for advanced numerical reasoning and introducing a
flexible framework for global optimization without parameter tuning.

</details>


### [43] [Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks](https://arxiv.org/abs/2510.25480)
*Florian A. Hölzl,Daniel Rueckert,Georgios Kaissis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Robust validation metrics remain essential in contemporary deep learning, not
only to detect overfitting and poor generalization, but also to monitor
training dynamics. In the supervised classification setting, we investigate
whether interactions between training data and model weights can yield such a
metric that both tracks generalization during training and attributes
performance to individual training samples. We introduce Gradient-Weight
Alignment (GWA), quantifying the coherence between per-sample gradients and
model weights. We show that effective learning corresponds to coherent
alignment, while misalignment indicates deteriorating generalization. GWA is
efficiently computable during training and reflects both sample-specific
contributions and dataset-wide learning dynamics. Extensive experiments show
that GWA accurately predicts optimal early stopping, enables principled model
comparisons, and identifies influential training samples, providing a
validation-set-free approach for model analysis directly from the training
data.

</details>


### [44] [Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI](https://arxiv.org/abs/2510.25497)
*Luca Andolfi,Eleonora Giunchiglia*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Neurosymbolic AI is growing in popularity thanks to its ability to combine
neural perception and symbolic reasoning in end-to-end trainable models.
However, recent findings reveal these are prone to shortcut reasoning, i.e., to
learning unindented concepts--or neural predicates--which exploit spurious
correlations to satisfy the symbolic constraints. In this paper, we address
reasoning shortcuts at their root cause and we introduce prototypical
neurosymbolic architectures. These models are able to satisfy the symbolic
constraints (be right) because they have learnt the correct basic concepts (for
the right reasons) and not because of spurious correlations, even in extremely
low data regimes. Leveraging the theory of prototypical learning, we
demonstrate that we can effectively avoid reasoning shortcuts by training the
models to satisfy the background knowledge while taking into account the
similarity of the input with respect to the handful of labelled datapoints. We
extensively validate our approach on the recently proposed rsbench benchmark
suite in a variety of settings and tasks with very scarce supervision: we show
significant improvements in learning the right concepts both in synthetic tasks
(MNIST-EvenOdd and Kand-Logic) and real-world, high-stake ones (BDD-OIA). Our
findings pave the way to prototype grounding as an effective,
annotation-efficient strategy for safe and reliable neurosymbolic learning.

</details>


### [45] [Support Vector Machine-Based Burnout Risk Prediction with an Interactive Interface for Organizational Use](https://arxiv.org/abs/2510.25509)
*Bruno W. G. Teodosio,Mário J. O. T. Lira,Pedro H. M. Araújo,Lucas R. C. Farias*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Burnout is a psychological syndrome marked by emotional exhaustion,
depersonalization, and reduced personal accomplishment, with a significant
impact on individual well-being and organizational performance. This study
proposes a machine learning approach to predict burnout risk using the
HackerEarth Employee Burnout Challenge dataset. Three supervised algorithms
were evaluated: nearest neighbors (KNN), random forest, and support vector
machine (SVM), with model performance evaluated through 30-fold
cross-validation using the determination coefficient (R2). Among the models
tested, SVM achieved the highest predictive performance (R2 = 0.84) and was
statistically superior to KNN and Random Forest based on paired $t$-tests. To
ensure practical applicability, an interactive interface was developed using
Streamlit, allowing non-technical users to input data and receive burnout risk
predictions. The results highlight the potential of machine learning to support
early detection of burnout and promote data-driven mental health strategies in
organizational settings.

</details>


### [46] [FaCT: Faithful Concept Traces for Explaining Neural Network Decisions](https://arxiv.org/abs/2510.25512)
*Amin Parchami-Araghi,Sukrut Rao,Jonas Fischer,Bernt Schiele*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep networks have shown remarkable performance across a wide range of tasks,
yet getting a global concept-level understanding of how they function remains a
key challenge. Many post-hoc concept-based approaches have been introduced to
understand their workings, yet they are not always faithful to the model.
Further, they make restrictive assumptions on the concepts a model learns, such
as class-specificity, small spatial extent, or alignment to human expectations.
In this work, we put emphasis on the faithfulness of such concept-based
explanations and propose a new model with model-inherent mechanistic
concept-explanations. Our concepts are shared across classes and, from any
layer, their contribution to the logit and their input-visualization can be
faithfully traced. We also leverage foundation models to propose a new
concept-consistency metric, C$^2$-Score, that can be used to evaluate
concept-based methods. We show that, compared to prior work, our concepts are
quantitatively more consistent and users find our concepts to be more
interpretable, all while retaining competitive ImageNet performance.

</details>


### [47] [Transformers Provably Learn Directed Acyclic Graphs via Kernel-Guided Mutual Information](https://arxiv.org/abs/2510.25542)
*Yuan Cheng,Yu Huang,Zhe Xiong,Yingbin Liang,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Uncovering hidden graph structures underlying real-world data is a critical
challenge with broad applications across scientific domains. Recently,
transformer-based models leveraging the attention mechanism have demonstrated
strong empirical success in capturing complex dependencies within graphs.
However, the theoretical understanding of their training dynamics has been
limited to tree-like graphs, where each node depends on a single parent.
Extending provable guarantees to more general directed acyclic graphs (DAGs) --
which involve multiple parents per node -- remains challenging, primarily due
to the difficulty in designing training objectives that enable different
attention heads to separately learn multiple different parent relationships.
  In this work, we address this problem by introducing a novel
information-theoretic metric: the kernel-guided mutual information (KG-MI),
based on the $f$-divergence. Our objective combines KG-MI with a multi-head
attention framework, where each head is associated with a distinct marginal
transition kernel to model diverse parent-child dependencies effectively. We
prove that, given sequences generated by a $K$-parent DAG, training a
single-layer, multi-head transformer via gradient ascent converges to the
global optimum in polynomial time. Furthermore, we characterize the attention
score patterns at convergence. In addition, when particularizing the
$f$-divergence to the KL divergence, the learned attention scores accurately
reflect the ground-truth adjacency matrix, thereby provably recovering the
underlying graph structure. Experimental results validate our theoretical
findings.

</details>


### [48] [Hybrid Quantum-Classical Recurrent Neural Networks](https://arxiv.org/abs/2510.25557)
*Wenduan Xu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a hybrid quantum-classical recurrent neural network (QRNN)
architecture in which the entire recurrent core is realized as a parametrized
quantum circuit (PQC) controlled by a classical feedforward network. The hidden
state is the quantum state of an $n$-qubit PQC, residing in an exponentially
large Hilbert space $\mathbb{C}^{2^n}$. The PQC is unitary by construction,
making the hidden-state evolution norm-preserving without external constraints.
At each timestep, mid-circuit readouts are combined with the input embedding
and processed by the feedforward network, which provides explicit classical
nonlinearity. The outputs parametrize the PQC, which updates the hidden state
via unitary dynamics. The QRNN is compact and physically consistent, and it
unifies (i) unitary recurrence as a high-capacity memory, (ii) partial
observation via mid-circuit measurements, and (iii) nonlinear classical control
for input-conditioned parametrization. We evaluate the model in simulation with
up to 14 qubits on sentiment analysis, MNIST, permuted MNIST, copying memory,
and language modeling, adopting projective measurements as a limiting case to
obtain mid-circuit readouts while maintaining a coherent recurrent quantum
memory. We further devise a soft attention mechanism over the mid-circuit
readouts in a sequence-to-sequence model and show its effectiveness for machine
translation. To our knowledge, this is the first model (RNN or otherwise)
grounded in quantum operations to achieve competitive performance against
strong classical baselines across a broad class of sequence-learning tasks.

</details>


### [49] [Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting](https://arxiv.org/abs/2510.25563)
*Víctor Medina,Giovanny A. Cuervo-Londoño,Javier Sánchez*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The accurate prediction of oceanographic variables is crucial for
understanding climate change, managing marine resources, and optimizing
maritime activities. Traditional ocean forecasting relies on numerical models;
however, these approaches face limitations in terms of computational cost and
scalability. In this study, we adapt Aurora, a foundational deep learning model
originally designed for atmospheric forecasting, to predict sea surface
temperature (SST) in the Canary Upwelling System. By fine-tuning this model
with high-resolution oceanographic reanalysis data, we demonstrate its ability
to capture complex spatiotemporal patterns while reducing computational
demands. Our methodology involves a staged fine-tuning process, incorporating
latitude-weighted error metrics and optimizing hyperparameters for efficient
learning. The experimental results show that the model achieves a low RMSE of
0.119K, maintaining high anomaly correlation coefficients (ACC $\approx
0.997$). The model successfully reproduces large-scale SST structures but faces
challenges in capturing finer details in coastal regions. This work contributes
to the field of data-driven ocean forecasting by demonstrating the feasibility
of using deep learning models pre-trained in different domains for oceanic
applications. Future improvements include integrating additional oceanographic
variables, increasing spatial resolution, and exploring physics-informed neural
networks to enhance interpretability and understanding. These advancements can
improve climate modeling and ocean prediction accuracy, supporting
decision-making in environmental and economic sectors.

</details>


### [50] [A Framework for Bounding Deterministic Risk with PAC-Bayes: Applications to Majority Votes](https://arxiv.org/abs/2510.25569)
*Benjamin Leblanc,Pascal Germain*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: PAC-Bayes is a popular and efficient framework for obtaining generalization
guarantees in situations involving uncountable hypothesis spaces.
Unfortunately, in its classical formulation, it only provides guarantees on the
expected risk of a randomly sampled hypothesis. This requires stochastic
predictions at test time, making PAC-Bayes unusable in many practical
situations where a single deterministic hypothesis must be deployed. We propose
a unified framework to extract guarantees holding for a single hypothesis from
stochastic PAC-Bayesian guarantees. We present a general oracle bound and
derive from it a numerical bound and a specialization to majority vote. We
empirically show that our approach consistently outperforms popular baselines
(by up to a factor of 2) when it comes to generalization bounds on
deterministic classifiers.

</details>


### [51] [Feedback Alignment Meets Low-Rank Manifolds: A Structured Recipe for Local Learning](https://arxiv.org/abs/2510.25594)
*Arani Roy,Marco P. Apolinario,Shristi Das Biswas,Kaushik Roy*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Training deep neural networks (DNNs) with backpropagation (BP) achieves
state-of-the-art accuracy but requires global error propagation and full
parameterization, leading to substantial memory and computational overhead.
Direct Feedback Alignment (DFA) enables local, parallelizable updates with
lower memory requirements but is limited by unstructured feedback and poor
scalability in deeper architectures, specially convolutional neural networks.
To address these limitations, we propose a structured local learning framework
that operates directly on low-rank manifolds defined by the Singular Value
Decomposition (SVD) of weight matrices. Each layer is trained in its decomposed
form, with updates applied to the SVD components using a composite loss that
integrates cross-entropy, subspace alignment, and orthogonality regularization.
Feedback matrices are constructed to match the SVD structure, ensuring
consistent alignment between forward and feedback pathways. Our method reduces
the number of trainable parameters relative to the original DFA model, without
relying on pruning or post hoc compression. Experiments on CIFAR-10, CIFAR-100,
and ImageNet show that our method achieves accuracy comparable to that of BP.
Ablation studies confirm the importance of each loss term in the low-rank
setting. These results establish local learning on low-rank manifolds as a
principled and scalable alternative to full-rank gradient-based training.

</details>


### [52] [Uncertainty Quantification for Regression: A Unified Framework based on kernel scores](https://arxiv.org/abs/2510.25599)
*Christopher Bülte,Yusuf Sale,Gitta Kutyniok,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Regression tasks, notably in safety-critical domains, require proper
uncertainty quantification, yet the literature remains largely
classification-focused. In this light, we introduce a family of measures for
total, aleatoric, and epistemic uncertainty based on proper scoring rules, with
a particular emphasis on kernel scores. The framework unifies several
well-known measures and provides a principled recipe for designing new ones
whose behavior, such as tail sensitivity, robustness, and out-of-distribution
responsiveness, is governed by the choice of kernel. We prove explicit
correspondences between kernel-score characteristics and downstream behavior,
yielding concrete design guidelines for task-specific measures. Extensive
experiments demonstrate that these measures are effective in downstream tasks
and reveal clear trade-offs among instantiations, including robustness and
out-of-distribution detection performance.

</details>


### [53] [INT v.s. FP: A Comprehensive Study of Fine-Grained Low-bit Quantization Formats](https://arxiv.org/abs/2510.25602)
*Mengzhao Chen,Meng Wu,Hui Jin,Zhihang Yuan,Jing Liu,Chaoyi Zhang,Yunshui Li,Jie Huang,Jin Ma,Zeyue Xue,Zhiheng Liu,Xingyan Bin,Ping Luo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern AI hardware, such as Nvidia's Blackwell architecture, is increasingly
embracing low-precision floating-point (FP) formats to handle the pervasive
activation outliers in Large Language Models (LLMs). Despite this industry
trend, a unified comparison of FP and integer (INT) quantization across varying
granularities has been missing, leaving algorithm and hardware co-design
without clear guidance. This paper fills that gap by systematically
investigating the trade-offs between FP and INT formats. We reveal a critical
performance crossover: while FP excels in coarse-grained quantization, the
comparison at fine-grained (block-wise) levels is more nuanced. Our
comprehensive comparison demonstrates that for popular 8-bit fine-grained
formats (e.g., MX with block size 32), MXINT8 is superior to its FP counterpart
in both algorithmic accuracy and hardware efficiency. However, for 4-bit
formats, FP (e.g., MXFP4, NVFP4) often holds an accuracy advantage , though we
show that NVINT4 can surpass NVFP4 when outlier-mitigation techniques like
Hadamard rotation are applied. We also introduce a symmetric clipping method
that resolves gradient bias in fine-grained low-bit INT training, enabling
nearly lossless performance for MXINT8 training. These findings challenge the
current hardware trajectory, demonstrating that a one-size-fits-all FP approach
is suboptimal and advocating that fine-grained INT formats, particularly
MXINT8, offer a better balance of accuracy, power, and efficiency for future AI
accelerators.

</details>


### [54] [BOLT-GAN: Bayes-Optimal Loss for Stable GAN Training](https://arxiv.org/abs/2510.25609)
*Mohammadreza Tavasoli Naeini,Ali Bereyhi,Morteza Noshad,Ben Liang,Alfred O. Hero III*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce BOLT-GAN, a simple yet effective modification of the WGAN
framework inspired by the Bayes Optimal Learning Threshold (BOLT). We show that
with a Lipschitz continuous discriminator, BOLT-GAN implicitly minimizes a
different metric distance than the Earth Mover (Wasserstein) distance and
achieves better training stability. Empirical evaluations on four standard
image generation benchmarks (CIFAR-10, CelebA-64, LSUN Bedroom-64, and LSUN
Church-64) show that BOLT-GAN consistently outperforms WGAN, achieving 10-60%
lower Frechet Inception Distance (FID). Our results suggest that BOLT is a
broadly applicable principle for enhancing GAN training.

</details>


### [55] [Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization](https://arxiv.org/abs/2510.25616)
*Nikita Kachaev,Mikhail Kolosov,Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The growing success of Vision-Language-Action (VLA) models stems from the
promise that pretrained Vision-Language Models (VLMs) can endow agents with
transferable world knowledge and vision-language (VL) grounding, laying a
foundation for action models with broader generalization. Yet when these VLMs
are adapted to the action modality, it remains unclear to what extent their
original VL representations and knowledge are preserved. In this work, we
conduct a systematic study of representation retention during VLA fine-tuning,
showing that naive action fine-tuning leads to degradation of visual
representations. To characterize and measure these effects, we probe VLA's
hidden representations and analyze attention maps, further, we design a set of
targeted tasks and methods that contrast VLA models with their counterpart
VLMs, isolating changes in VL capabilities induced by action fine-tuning. We
further evaluate a range of strategies for aligning visual representations and
introduce a simple yet effective method that mitigates degradation and yields
improved generalization to out-of-distribution (OOD) scenarios. Taken together,
our analysis clarifies the trade-off between action fine-tuning and the
degradation of VL representations and highlights practical approaches to
recover inherited VL capabilities. Code is publicly available:
https://blind-vla-paper.github.io

</details>


### [56] [Subgraph Federated Learning via Spectral Methods](https://arxiv.org/abs/2510.25657)
*Javad Aliakbari,Johan Östman,Ashkan Panahi,Alexandre Graell i Amat*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We consider the problem of federated learning (FL) with graph-structured data
distributed across multiple clients. In particular, we address the prevalent
scenario of interconnected subgraphs, where interconnections between clients
significantly influence the learning process. Existing approaches suffer from
critical limitations, either requiring the exchange of sensitive node
embeddings, thereby posing privacy risks, or relying on
computationally-intensive steps, which hinders scalability. To tackle these
challenges, we propose FedLap, a novel framework that leverages global
structure information via Laplacian smoothing in the spectral domain to
effectively capture inter-node dependencies while ensuring privacy and
scalability. We provide a formal analysis of the privacy of FedLap,
demonstrating that it preserves privacy. Notably, FedLap is the first subgraph
FL scheme with strong privacy guarantees. Extensive experiments on benchmark
datasets demonstrate that FedLap achieves competitive or superior utility
compared to existing techniques.

</details>


### [57] [Mechanistic Interpretability of RNNs emulating Hidden Markov Models](https://arxiv.org/abs/2510.25674)
*Elia Torre,Michele Viscione,Lucas Pompe,Benjamin F Grewe,Valerio Mante*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recurrent neural networks (RNNs) provide a powerful approach in neuroscience
to infer latent dynamics in neural populations and to generate hypotheses about
the neural computations underlying behavior. However, past work has focused on
relatively simple, input-driven, and largely deterministic behaviors - little
is known about the mechanisms that would allow RNNs to generate the richer,
spontaneous, and potentially stochastic behaviors observed in natural settings.
Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of
natural behaviors into discrete latent states with stochastic transitions
between them, a type of dynamics that may appear at odds with the continuous
state spaces implemented by RNNs. Here we first show that RNNs can replicate
HMM emission statistics and then reverse-engineer the trained networks to
uncover the mechanisms they implement. In the absence of inputs, the activity
of trained RNNs collapses towards a single fixed point. When driven by
stochastic input, trajectories instead exhibit noise-sustained dynamics along
closed orbits. Rotation along these orbits modulates the emission probabilities
and is governed by transitions between regions of slow, noise-driven dynamics
connected by fast, deterministic transitions. The trained RNNs develop highly
structured connectivity, with a small set of "kick neurons" initiating
transitions between these regions. This mechanism emerges during training as
the network shifts into a regime of stochastic resonance, enabling it to
perform probabilistic computations. Analyses across multiple HMM architectures
- fully connected, cyclic, and linear-chain - reveal that this solution
generalizes through the modular reuse of the same dynamical motif, suggesting a
compositional principle by which RNNs can emulate complex discrete latent
dynamics.

</details>


### [58] [Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics](https://arxiv.org/abs/2510.25683)
*Alessandro Lucchetti,Francesco Cadini,Marco Giglio,Luca Lomazzi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Neural Networks (GNNs) have recently been explored as surrogate models
for numerical simulations. While their applications in computational fluid
dynamics have been investigated, little attention has been given to structural
problems, especially for dynamic cases. To address this gap, we introduce the
Graph Network-based Structural Simulator (GNSS), a GNN framework for surrogate
modeling of dynamic structural problems.
  GNSS follows the encode-process-decode paradigm typical of GNN-based machine
learning models, and its design makes it particularly suited for dynamic
simulations thanks to three key features: (i) expressing node kinematics in
node-fixed local frames, which avoids catastrophic cancellation in
finite-difference velocities; (ii) employing a sign-aware regression loss,
which reduces phase errors in long rollouts; and (iii) using a
wavelength-informed connectivity radius, which optimizes graph construction.
  We evaluate GNSS on a case study involving a beam excited by a 50kHz
Hanning-modulated pulse. The results show that GNSS accurately reproduces the
physics of the problem over hundreds of timesteps and generalizes to unseen
loading conditions, where existing GNNs fail to converge or deliver meaningful
predictions.
  Compared with explicit finite element baselines, GNSS achieves substantial
inference speedups while preserving spatial and temporal fidelity. These
findings demonstrate that locality-preserving GNNs with physics-consistent
update rules are a competitive alternative for dynamic, wave-dominated
structural simulations.

</details>


### [59] [Convolutional Spiking-based GRU Cell for Spatio-temporal Data](https://arxiv.org/abs/2510.25696)
*Yesmine Abdennadher,Eleonora Cicciarella,Michele Rossi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Spike-based temporal messaging enables SNNs to efficiently process both
purely temporal and spatio-temporal time-series or event-driven data. Combining
SNNs with Gated Recurrent Units (GRUs), a variant of recurrent neural networks,
gives rise to a robust framework for sequential data processing; however,
traditional RNNs often lose local details when handling long sequences.
Previous approaches, such as SpikGRU, fail to capture fine-grained local
dependencies in event-based spatio-temporal data. In this paper, we introduce
the Convolutional Spiking GRU (CS-GRU) cell, which leverages convolutional
operations to preserve local structure and dependencies while integrating the
temporal precision of spiking neurons with the efficient gating mechanisms of
GRUs. This versatile architecture excels on both temporal datasets (NTIDIGITS,
SHD) and spatio-temporal benchmarks (MNIST, DVSGesture, CIFAR10DVS). Our
experiments show that CS-GRU outperforms state-of-the-art GRU variants by an
average of 4.35%, achieving over 90% accuracy on sequential tasks and up to
99.31% on MNIST. It is worth noting that our solution achieves 69% higher
efficiency compared to SpikGRU. The code is available at:
https://github.com/YesmineAbdennadher/CS-GRU.

</details>


### [60] [Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning](https://arxiv.org/abs/2510.25759)
*Ethan Harvey,Dennis Johan Loevlie,Michael C. Hughes*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify
high-resolution 2D images by processing patches or classify 3D volumes by
processing slices. However, conventional MIL approaches treat instances
separately, ignoring contextual relationships such as the appearance of nearby
patches or slices that can be essential in real applications. We design a
synthetic classification task where accounting for adjacent instance features
is crucial for accurate prediction. We demonstrate the limitations of
off-the-shelf MIL approaches by quantifying their performance compared to the
optimal Bayes estimator for this task, which is available in closed-form. We
empirically show that newer correlated MIL methods still struggle to generalize
as well as possible when trained from scratch on tens of thousands of
instances.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [61] [Certainty in Uncertainty: Reasoning over Uncertain Knowledge Graphs with Statistical Guarantees](https://arxiv.org/abs/2510.24754)
*Yuqicheng Zhu,Jingcheng Wu,Yizhen Wang,Hongkuan Zhou,Jiaoyan Chen,Evgeny Kharlamov,Steffen Staab*

Main category: stat.ML

TL;DR: 提出 UnKGCP，在不确定知识图嵌入(UnKGE)的基础上提供可控覆盖率的预测区间，区间长度反映不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有的 UnKGE 仅给出点估计，缺乏对预测的不确定性量化，限制在高风险应用中的可信度。

Method: 结合保序预测(conformal prediction)框架，设计适用于 UnKGE 的非拟合度量和高效区间构造，提供理论覆盖概率保证。

Result: 理论上保证区间在用户设定的置信水平下包含真实分数；区间较短且经验上能有效涵盖预测的不确定性，实验结果表明区间尖锐且捕获不确定性。

Conclusion: UnKGCP 为 UnKGE 提供可验证的预测不确定性量化，能在多种 UnKGE 方法上实现稳健的区间预测。

Abstract: Uncertain knowledge graph embedding (UnKGE) methods learn vector
representations that capture both structural and uncertainty information to
predict scores of unseen triples. However, existing methods produce only point
estimates, without quantifying predictive uncertainty-limiting their
reliability in high-stakes applications where understanding confidence in
predictions is crucial. To address this limitation, we propose \textsc{UnKGCP},
a framework that generates prediction intervals guaranteed to contain the true
score with a user-specified level of confidence. The length of the intervals
reflects the model's predictive uncertainty. \textsc{UnKGCP} builds on the
conformal prediction framework but introduces a novel nonconformity measure
tailored to UnKGE methods and an efficient procedure for interval construction.
We provide theoretical guarantees for the intervals and empirically verify
these guarantees. Extensive experiments on standard benchmarks across diverse
UnKGE methods further demonstrate that the intervals are sharp and effectively
capture predictive uncertainty.

</details>


### [62] [Tree Ensemble Explainability through the Hoeffding Functional Decomposition and TreeHFD Algorithm](https://arxiv.org/abs/2510.24815)
*Clément Bénard*

Main category: stat.ML

TL;DR: 提出 TreeHFD，从数据样本估计 tree ensemble 的 Hoeffding 分解；在输入相关性存在时，通过分层正交约束实现唯一且稀疏的分解，具有可解释的主效应与交互；给出收敛性与性质；在仿真与真实数据上表现良好，并给出 Python 包；并指出 TreeSHAP 与 Hoeffding 分解之间的强联系。


<details>
  <summary>Details</summary>
Motivation: 黑箱树模型在高风险决策中的可解释性受限。Hoeffding/ANOVA 分解在输入独立时提供唯一的逐维加和解释，但现实数据往往存在相关性，因此需要通过分层正交性约束得到唯一、稀疏的分解；但如何从样本数据估计这种分解仍是未解决的问题。

Method: 提出 TreeHFD 算法，以从数据样本估计树 ensemble 的 Hoeffding 分解。通过引入分层正交性约束，确保分解的正交性、稀疏性，以及对变量的因果选择性；给出理论收敛性结果；并分析与树模型的结合。

Result: 在仿真和真实数据上表现良好，证实高性能。提供 Python 包 treehfd（GitHub 链接）。实证表明 TreeSHAP（基于 Shapley 值）与 Hoeffding 分解存在强连接。

Conclusion: 给出一种可在输入变量相关性存在时对树集成进行唯一、稀疏且可解释分解的新方法；揭示了 Shapley 基于 TreeSHAP 与 Hoeffding 分解之间的理论联系，为解释性工具的理论统一与应用提供基础。

Abstract: Tree ensembles have demonstrated state-of-the-art predictive performance
across a wide range of problems involving tabular data. Nevertheless, the
black-box nature of tree ensembles is a strong limitation, especially for
applications with critical decisions at stake. The Hoeffding or ANOVA
functional decomposition is a powerful explainability method, as it breaks down
black-box models into a unique sum of lower-dimensional functions, provided
that input variables are independent. In standard learning settings, input
variables are often dependent, and the Hoeffding decomposition is generalized
through hierarchical orthogonality constraints. Such generalization leads to
unique and sparse decompositions with well-defined main effects and
interactions. However, the practical estimation of this decomposition from a
data sample is still an open problem. Therefore, we introduce the TreeHFD
algorithm to estimate the Hoeffding decomposition of a tree ensemble from a
data sample. We show the convergence of TreeHFD, along with the main properties
of orthogonality, sparsity, and causal variable selection. The high performance
of TreeHFD is demonstrated through experiments on both simulated and real data,
using our treehfd Python package (https://github.com/ThalesGroup/treehfd).
Besides, we empirically show that the widely used TreeSHAP method, based on
Shapley values, is strongly connected to the Hoeffding decomposition.

</details>


### [63] [Convergence of off-policy TD(0) with linear function approximation for reversible Markov chains](https://arxiv.org/abs/2510.25514)
*Maik Overmars,Jasper Goseling,Richard Boucherie*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the convergence of off-policy TD(0) with linear function
approximation when used to approximate the expected discounted reward in a
Markov chain. It is well known that the combination of off-policy learning and
function approximation can lead to divergence of the algorithm. Existing
results for this setting modify the algorithm, for instance by reweighing the
updates using importance sampling. This establishes convergence at the expense
of additional complexity. In contrast, our approach is to analyse the standard
algorithm, but to restrict our attention to the class of reversible Markov
chains. We demonstrate convergence under this mild reversibility condition on
the structure of the chain, which in many applications can be assumed using
domain knowledge. In particular, we establish a convergence guarantee under an
upper bound on the discount factor in terms of the difference between the
on-policy and off-policy process. This improves upon known results in the
literature that state that convergence holds for a sufficiently small discount
factor by establishing an explicit bound. Convergence is with probability one
and achieves projected Bellman error equal to zero. To obtain these results, we
adapt the stochastic approximation framework that was used by Tsitsiklis and
Van Roy [1997 for the on-policy case, to the off-policy case. We illustrate our
results using different types of reversible Markov chains, such as
one-dimensional random walks and random walks on a weighted graph.

</details>


### [64] [Using latent representations to link disjoint longitudinal data for mixed-effects regression](https://arxiv.org/abs/2510.25531)
*Clemens Schächter,Maren Hackenberg,Michelle Pfaffenlehner,Félix B. Tambe-Ndonfack,Thorsten Schmidt,Astrid Pechmann,Janbernd Kirschner,Jan Hasenauser,Harald Binder*

Main category: stat.ML

TL;DR: 在稀有疾病的小样本数据中，将来自不同测量工具的纵向数据映射到一个共享的潜在时间轨迹，通过变分自编码器嵌入，再在潜在表示上用混合效应回归建模疾病动态与治疗切换效应，提供联合参数估计的统计检验方法；应用于脊髓肌萎缩性侧索硬化患者的数据分析。


<details>
  <summary>Details</summary>
Motivation: 解决稀有疾病试验中的小样本和随时间工具变化导致的数据不连贯问题，需在不同测量工具间对齐并评估治疗切换的影响。

Method: 使用一组变分自编码器在各时间点将测量项嵌入同一潜在空间，并跨工具对齐；在潜在表示上应用混合效应回归以建模疾病动力学与治疗切换效应；提出考虑VAE与混合效应参数联合估计的统计检验方法；将估计的效应映射回观测项以便解释。

Result: 在脊髓肌萎缩患者的数据中实现不同测量工具的对齐以支持混合效应回归；潜在表示成功捕捉治疗切换效应并可回映至观测项，便于解读；方法可用于模型选择并评估治疗切换的影响，显示在小样本情境下的潜力。

Conclusion: 联合潜在表示建模能处理不连贯的纵向数据与小样本挑战，从而实现对治疗切换等干预效果的统计推断。

Abstract: Many rare diseases offer limited established treatment options, leading
patients to switch therapies when new medications emerge. To analyze the impact
of such treatment switches within the low sample size limitations of rare
disease trials, it is important to use all available data sources. This,
however, is complicated when usage of measurement instruments change during the
observation period, for example when instruments are adapted to specific age
ranges. The resulting disjoint longitudinal data trajectories, complicate the
application of traditional modeling approaches like mixed-effects regression.
We tackle this by mapping observations of each instrument to a aligned
low-dimensional temporal trajectory, enabling longitudinal modeling across
instruments. Specifically, we employ a set of variational autoencoder
architectures to embed item values into a shared latent space for each time
point. Temporal disease dynamics and treatment switch effects are then captured
through a mixed-effects regression model applied to latent representations. To
enable statistical inference, we present a novel statistical testing approach
that accounts for the joint parameter estimation of mixed-effects regression
and variational autoencoders. The methodology is applied to quantify the impact
of treatment switches for patients with spinal muscular atrophy. Here, our
approach aligns motor performance items from different measurement instruments
for mixed-effects regression and maps estimated effects back to the observed
item level to quantify the treatment switch effect. Our approach allows for
model selection as well as for assessing effects of treatment switching. The
results highlight the potential of modeling in joint latent representations for
addressing small data challenges.

</details>


### [65] [Error Bounds and Optimal Schedules for Masked Diffusions with Factorized Approximations](https://arxiv.org/abs/2510.25544)
*Hugo Lavenant,Giacomo Zanella*

Main category: stat.ML

TL;DR: 给出对离散数据生成模型的误差界、调度策略以及求解方法的理论分析与发现。


<details>
  <summary>Details</summary>
Motivation: 解决离散数据自回归模型的高计算成本，同时希望在偏差可控的前提下，给出普遍的误差界并优化生成过程的调度策略。

Method: 给出相对熵误差界，独立于数据维度，只与每次迭代生成的令牌数相关；研究非恒定调度（不同步生成令牌数）对性能的影响，给出与数据分布的信息概况（information profile）相关的最优调度；将方法定义为直接的采样算法，而非时间反演扩散过程的推导，给出简洁透明的证明。

Result: 证明了在一定条件下的计算-精度折衷的误差界，并揭示非恒定调度的潜在优势，给出基于信息分布的最优调度策略；通过直接采样算法的框架，获得更易理解的证明。

Conclusion: MDMs在计算成本与采样偏差之间可控的折衷得到理论支撑；调度策略的优化可通过数据的信息分布特征进行 principled 选择，方法简单且理论证明直观。

Abstract: Recently proposed generative models for discrete data, such as Masked
Diffusion Models (MDMs), exploit conditional independence approximations to
reduce the computational cost of popular Auto-Regressive Models (ARMs), at the
price of some bias in the sampling distribution. We study the resulting
computation-vs-accuracy trade-off, providing general error bounds (in relative
entropy) that depend only on the average number of tokens generated per
iteration and are independent of the data dimensionality (i.e. sequence
length), thus supporting the empirical success of MDMs. We then investigate the
gain obtained by using non-constant schedule sizes (i.e. varying the number of
unmasked tokens during the generation process) and identify the optimal
schedule as a function of a so-called information profile of the data
distribution, thus allowing for a principled optimization of schedule sizes. We
define methods directly as sampling algorithms and do not use classical
derivations as time-reversed diffusion processes, leading us to simple and
transparent proofs.

</details>


### [66] [Monitoring the calibration of probability forecasts with an application to concept drift detection involving image classification](https://arxiv.org/abs/2510.25573)
*Christopher T. Franck,Anne R. Driscoll,Zoe Szajnfarber,William H. Woodall*

Main category: stat.ML

TL;DR: 提出一种基于累计和（CUSUM）并带有动态边界的监控图，用以持续监测图像分类模型的预测校准随时间的变化，能够在不访问模型内部的前提下实现早期失校准告警。


<details>
  <summary>Details</summary>
Motivation: 随着时间和环境的变化，图像分类模型的预测概率与真实事件之间的校准可能下降。现有方法多关注静态评估，缺乏对连续运行中的校准监控。需要一种能在运行过程中基于预测概率与事件结果，持续检测校准偏离的通用方法，尤其适用于概念漂移和上下文变化场景。

Method: 提出一个带动态限制的CUSUM监控図，直接在概率预测和事件结果上运行，无需访问模型内部参数或结构。该方法可应用于传统过程监控和概念漂移情景，目标是在预测校准发生偏离时尽早发出警报。

Result: 该方法具备对校准失效的早期检测能力，且具有广泛适用性，能够在部署阶段对任何需要监控概率预测的场景进行非侵入式检测，不依赖于对模型内部的访问。

Conclusion: 为持续、非侵入式的预测校准监控提供了一种通用工具，能够在运行环境中实时检测因环境变化或概念漂移导致的校准偏离，提升系统的可靠性与可维护性。

Abstract: Machine learning approaches for image classification have led to impressive
advances in that field. For example, convolutional neural networks are able to
achieve remarkable image classification accuracy across a wide range of
applications in industry, defense, and other areas. While these machine
learning models boast impressive accuracy, a related concern is how to assess
and maintain calibration in the predictions these models make. A classification
model is said to be well calibrated if its predicted probabilities correspond
with the rates events actually occur. While there are many available methods to
assess machine learning calibration and recalibrate faulty predictions, less
effort has been spent on developing approaches that continually monitor
predictive models for potential loss of calibration as time passes. We propose
a cumulative sum-based approach with dynamic limits that enable detection of
miscalibration in both traditional process monitoring and concept drift
applications. This enables early detection of operational context changes that
impact image classification performance in the field. The proposed chart can be
used broadly in any situation where the user needs to monitor probability
predictions over time for potential lapses in calibration. Importantly, our
method operates on probability predictions and event outcomes and does not
require under-the-hood access to the machine learning model.

</details>


### [67] [How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs](https://arxiv.org/abs/2510.25753)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Pretrained Transformers demonstrate remarkable in-context learning (ICL)
capabilities, enabling them to adapt to new tasks from demonstrations without
parameter updates. However, theoretical studies often rely on simplified
architectures (e.g., omitting MLPs), data models (e.g., linear regression with
isotropic inputs), and single-source training, limiting their relevance to
realistic settings. In this work, we study ICL in pretrained Transformers with
nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with
heterogeneous input, task, and noise distributions. We analyze a model where
the MLP comprises two layers, with the first layer trained via a single
gradient step and the second layer fully optimized. Under high-dimensional
asymptotics, we prove that such models are equivalent in ICL error to
structured polynomial predictors, leveraging results from the theory of
Gaussian universality and orthogonal polynomials. This equivalence reveals that
nonlinear MLPs meaningfully enhance ICL performance, particularly on nonlinear
tasks, compared to linear baselines. It also enables a precise analysis of data
mixing effects: we identify key properties of high-quality data sources (low
noise, structured covariances) and show that feature learning emerges only when
the task covariance exhibits sufficient structure. These results are validated
empirically across various activation functions, model sizes, and data
distributions. Finally, we experiment with a real-world scenario involving
multilingual sentiment analysis where each language is treated as a different
source. Our experimental results for this case exemplify how our findings
extend to real-world cases. Overall, our work advances the theoretical
foundations of ICL in Transformers and provides actionable insight into the
role of architecture and data in ICL.

</details>


### [68] [E-Scores for (In)Correctness Assessment of Generative Model Outputs](https://arxiv.org/abs/2510.25770)
*Guneet S. Dhillon,Javier González,Teodora Pandeva,Alicia Curth*

Main category: stat.ML

TL;DR: 用e-value替代p-value的 conformal prediction 来评估LLM输出的正确性，提供后验自适应容忍度并保持等效的统计保证；在数学事实性与属性约束满足等任务上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于p值的 conformal prediction 易受p-hacking影响，缺乏灵活的事后容忍度调整。需在不降低统计保证的前提下，为LLM输出的正确性提供更灵活的后验控制。

Method: 将e-value/e-score引入到面向LLM输出的正确性评估框架中，推导等价的统计保证，并通过上界大小失真(size distortion)的方式实现事后容忍度的自适应选择。与基于p值的做法进行对比，实验覆盖不同正确性类型（如数学事实性、属性约束满足）。

Result: 与p值等价的统计保证得到保留，同时e-score允许在观察到e-score后自适应选择容忍度；通过实验验证在不同正确性类型上的有效性。

Conclusion: e-values/e-scores为生成模型输出的正确性评估提供灵活的事后控制手段，保持统计保证的同时引入大小失真上界，从而更适应实际应用中的容忍度设定。

Abstract: While generative models, especially large language models (LLMs), are
ubiquitous in today's world, principled mechanisms to assess their
(in)correctness are limited. Using the conformal prediction framework, previous
works construct sets of LLM responses where the probability of including an
incorrect response, or error, is capped at a desired user-defined tolerance
level. However, since these methods are based on p-values, they are susceptible
to p-hacking, i.e., choosing the tolerance level post-hoc can invalidate the
guarantees. We therefore leverage e-values to complement generative model
outputs with e-scores as a measure of incorrectness. In addition to achieving
the same statistical guarantees as before, e-scores provide users flexibility
in adaptively choosing tolerance levels after observing the e-scores
themselves, by upper bounding a post-hoc notion of error called size
distortion. We experimentally demonstrate their efficacy in assessing LLM
outputs for different correctness types: mathematical factuality and property
constraints satisfaction.

</details>
