<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [TabSieve: Explicit In-Table Evidence Selection for Tabular Prediction](https://arxiv.org/abs/2602.11700)
*Yongyao Wang,Ziqi Miao,Lu Yang,Haonan Jia,Wenting Yan,Chen Qian,Lijun Li*

Main category: cs.LG

TL;DR: TabSieve提出一个select-then-predict框架，通过选择少量信息行作为证据再进行预测，显著提升表格预测在小样本下的性能，并实现证据的可审计性。


<details>
  <summary>Details</summary>
Motivation: 现有表格模型多依赖逐实例推断，基于LLM的提示对相关行的利用往往不稳定，嘈杂上下文会降低性能，因此需要一种显式、可审计的证据选择机制来提升鲁棒性。

Method: 提出 TabSieve：先选择一组信息量较高的证据行，再在选定证据的条件下进行目标预测。为训练提供数据支持，构建 TabSieve-SFT-40K：从331张真实表格中用强教师模型合成高质量推理轨迹并进行严格筛选。提出 TAB-GRPO：一种强化学习方法，联合优化证据选择与预测正确性，且通过动态任务优势平衡来稳定混合回归与分类训练。

Result: 在包含75个分类表和52个回归表的 held-out 基准上，TabSieve在各-shot预算下均有提升，相对第二优基线的平均增益为分类2.92%、回归4.45%。进一步分析显示模型对所选证据的关注度更高，对嘈杂上下文更具鲁棒性。

Conclusion: 证据选择的显式使用提升了表格预测的鲁棒性和可审计性，TabSieve及其数据合成与强化学习训练策略为小样本场景下的表格推断提供了有效路径。

Abstract: Tabular prediction can benefit from in-table rows as few-shot evidence, yet existing tabular models typically perform instance-wise inference and LLM-based prompting is often brittle. Models do not consistently leverage relevant rows, and noisy context can degrade performance. To address this challenge, we propose TabSieve, a select-then-predict framework that makes evidence usage explicit and auditable. Given a table and a query row, TabSieve first selects a small set of informative rows as evidence and then predicts the missing target conditioned on the selected evidence. To enable this capability, we construct TabSieve-SFT-40K by synthesizing high-quality reasoning trajectories from 331 real tables using a strong teacher model with strict filtering. Furthermore, we introduce TAB-GRPO, a reinforcement learning recipe that jointly optimizes evidence selection and prediction correctness with separate rewards, and stabilizes mixed regression and classification training via dynamic task-advantage balancing. Experiments on a held-out benchmark of 75 classification and 52 regression tables show that TabSieve consistently improves performance across shot budgets, with average gains of 2.92% on classification and 4.45% on regression over the second-best baseline. Further analysis indicates that TabSieve concentrates more attention on the selected evidence, which improves robustness to noisy context.

</details>


### [2] [U-Former ODE: Fast Probabilistic Forecasting of Irregular Time Series](https://arxiv.org/abs/2602.11738)
*Ilya Kuleshov,Alexander Marusov,Alexey Zaytsev*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Probabilistic forecasting of irregularly sampled time series is crucial in domains such as healthcare and finance, yet it remains a formidable challenge. Existing Neural Controlled Differential Equation (Neural CDE) approaches, while effective at modelling continuous dynamics, suffer from slow, inherently sequential computation, which restricts scalability and limits access to global context. We introduce UFO (U-Former ODE), a novel architecture that seamlessly integrates the parallelizable, multiscale feature extraction of U-Nets, the powerful global modelling of Transformers, and the continuous-time dynamics of Neural CDEs. By constructing a fully causal, parallelizable model, UFO achieves a global receptive field while retaining strong sensitivity to local temporal dynamics. Extensive experiments on five standard benchmarks -- covering both regularly and irregularly sampled time series -- demonstrate that UFO consistently outperforms ten state-of-the-art neural baselines in predictive accuracy. Moreover, UFO delivers up to 15$\times$ faster inference compared to conventional Neural CDEs, with consistently strong performance on long and highly multivariate sequences.

</details>
