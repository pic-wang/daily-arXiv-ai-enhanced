{"id": "2511.10619", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.10619", "abs": "https://arxiv.org/abs/2511.10619", "authors": ["Avrim Blum", "Marten Garicano", "Kavya Ravichandran", "Dravyansh Sharma"], "title": "Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem", "comment": "25 pages", "summary": "The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $\u03a9(k)$ and $\u03a9(\\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u7c7b\u53c2\u6570\u5316\u7684\u6539\u8fdb\u578b bandit \u7b97\u6cd5\u65cf\uff0c\u5e76\u57fa\u4e8e\u79bb\u7ebf\u6570\u636e\u5b66\u4e60\u8fd1\u4f3c\u6700\u4f18\u7b97\u6cd5\u4ee5\u83b7\u5f97\u6570\u636e\u76f8\u5173\u7684\u6837\u672c\u590d\u6742\u5ea6\u4fdd\u8bc1\uff1b\u5728\u989d\u5916\u7684\u51f9\u6027\u5047\u8bbe\u4e0b\uff0c\u7b2c\u4e00\u65cf\u7b97\u6cd5\u5b9e\u73b0\u5bf9 k \u7684\u6700\u4f18\u4f9d\u8d56\uff1b\u7b2c\u4e8c\u65cf\u5728\u826f\u6027\u5b9e\u4f8b\u4e0b\u53ef\u5b9e\u73b0\u6700\u4f73\u81c2\u8bc6\u522b\uff0c\u5728\u56f0\u96be\u5b9e\u4f8b\u4e0b\u9000\u5316\u5230\u6700\u574f\u60c5\u5f62\uff0c\u5e76\u7ed9\u51fa\u4ece\u7edf\u8ba1\u5b66\u4e60\u89c6\u89d2\u7684\u5f3a\u6570\u636e\u76f8\u5173\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u5206\u914d\u7814\u7a76\u4e0e\u8d44\u6e90\u7684\u95ee\u9898\uff08\u5982\u65b0\u6280\u672f\u7814\u7a76\u6295\u5165\u3001\u4e34\u5e8a\u8bd5\u9a8c\u3001\u8d85\u53c2\u6570\u8c03\u4f18\u7b49\uff09\uff0c\u73b0\u6709\u786e\u5b9a\u6027/\u968f\u673a\u7b97\u6cd5\u5b58\u5728 \u03a9(k) \u4e0e \u03a9(\u221ak) \u7684\u4e0b\u754c\uff0c\u4f7f\u5f97\u6700\u574f\u60c5\u5f62\u4e0b\u7684\u8fd1\u4f3c\u5f3a\u5ea6\u53d7\u9650\u3002\u901a\u8fc7\u53c2\u6570\u5316\u7b97\u6cd5\u65cf\u5e76\u5229\u7528\u79bb\u7ebf\u6570\u636e\u8fdb\u884c\u5b66\u4e60\uff0c\u53ef\u5728\u6570\u636e\u6761\u4ef6\u5145\u5206\u65f6\u83b7\u5f97\u66f4\u5f3a\u7684\u4f9d\u8d56\u4e8e\u95ee\u9898\u53c2\u6570\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4e24\u7c7b\u53c2\u6570\u5316\u7684 bandit \u7b97\u6cd5\u65cf\uff0c\u5e76\u5bf9\u4ece\u79bb\u7ebf\u6570\u636e\u4e2d\u5b66\u4e60\u9760\u8fd1\u6700\u4f18\u7b97\u6cd5\u7684\u6837\u672c\u590d\u6742\u5ea6\u8fdb\u884c\u754c\u5b9a\uff1a\u7b2c\u4e00\u65cf\u5305\u542b\u6b64\u524d\u5de5\u4f5c\u4e2d\u7684\u6700\u4f18\u968f\u673a\u7b97\u6cd5\uff0c\u7ed9\u51fa\u5728\u5bf9\u6536\u76ca\u66f2\u7ebf\u5177\u5907\u989d\u5916\u51f9\u6027\u5f3a\u5ea6\u5047\u8bbe\u65f6\u53ef\u83b7\u5f97\u5bf9 k \u7684\u66f4\u597d\u4f9d\u8d56\u7684\u7b97\u6cd5\uff1b\u7b2c\u4e8c\u65cf\u5728\u826f\u597d\u5b9e\u4f8b\u4e0b\u53ef\u4fdd\u8bc1\u6700\u4f73\u81c2\u8bc6\u522b\uff0c\u5728\u5dee\u52b2\u5b9e\u4f8b\u4e0b\u9000\u56de\u5230\u6700\u574f\u60c5\u5f62\u4fdd\u8bc1\u3002\u4ee5\u7edf\u8ba1\u5b66\u4e60\u7684\u89d2\u5ea6\u5206\u6790 bandit \u5956\u52b1\u4f18\u5316\u95ee\u9898\uff0c\u5728\u4e0d\u9700\u8981\u5b9e\u9645\u9a8c\u8bc1\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u66f4\u5f3a\u7684\u6570\u636e\u76f8\u5173\u4fdd\u8bc1\u3002", "result": "\u5728\u79bb\u7ebf\u6570\u636e\u9a71\u52a8\u4e0b\uff0c\u80fd\u591f\u9009\u62e9\u51fa\u5728\u4e0d\u540c\u5b9e\u4f8b\u6761\u4ef6\u4e0b\u5177\u6709\u66f4\u5f3a\u6570\u636e\u4f9d\u8d56\u6027\u7684\u8fd1\u4f3c\u6700\u4f18\u7b97\u6cd5\uff1b\u7b2c\u4e00\u65cf\u5728\u5bf9\u6536\u76ca\u66f2\u7ebf\u7684\u51f9\u6027\u5c5e\u6027\u6ee1\u8db3\u65f6\uff0c\u5bf9 k \u7684\u4f9d\u8d56\u8fbe\u5230\u6700\u4f18\uff1b\u7b2c\u4e8c\u65cf\u5b9e\u73b0\u5bf9\u826f\u6027\u4e0e\u6076\u6027\u5b9e\u4f8b\u7684\u81ea\u9002\u5e94\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7684\u5b66\u4e60\u53ef\u83b7\u5f97\u66f4\u597d\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u9650\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u3001\u6570\u636e\u9a71\u52a8\u7684\u6539\u8fdb\u578b bandit \u6846\u67b6\uff0c\u80fd\u591f\u5728\u79bb\u7ebf\u6570\u636e\u57fa\u7840\u4e0a\u9009\u62e9\u5177\u6709\u66f4\u597d\u6570\u636e\u76f8\u5173\u4fdd\u8bc1\u7684\u7b97\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u5b9e\u4f8b\u6761\u4ef6\u4e0b\u81ea\u9002\u5e94\u5730\u5728\u6700\u4f73\u81c2\u8bc6\u522b\u4e0e\u6700\u574f\u60c5\u5f62\u4fdd\u8bc1\u4e4b\u95f4\u5207\u6362\uff0c\u63d0\u5347\u4e86\u5bf9 improving bandits \u7684\u7406\u8bba\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u9002\u7528\u6027\u4e0e\u9c81\u68d2\u6027\u3002"}}
