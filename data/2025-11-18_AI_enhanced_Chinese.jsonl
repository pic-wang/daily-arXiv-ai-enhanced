{"id": "2511.10706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10706", "abs": "https://arxiv.org/abs/2511.10706", "authors": ["Zitong Zhang", "Hao Sun"], "title": "Differentiable Sparse Identification of Lagrangian Dynamics", "comment": null, "summary": "Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.10707", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10707", "abs": "https://arxiv.org/abs/2511.10707", "authors": ["Sirui Liang", "Pengfei Cao", "Jian Zhao", "Cong Huang", "Jun Zhao", "Kang Liu"], "title": "Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning", "comment": "accepted by aaai2026", "summary": "Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.", "AI": {"tldr": "BREP ReFT\u901a\u8fc7\u504f\u7f6e-\u53d7\u9650\u524d\u7f00\u8868\u793a\u5fae\u8c03\u6765\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff1a\u901a\u8fc7\u88c1\u526a\u8bad\u7ec3\u6570\u636e\u4ee5\u4f18\u5316\u521d\u59cb\u63a8\u7406\u524d\u7f00\u3001\u5728\u65e9\u671f\u63a8\u7406\u9636\u6bb5\u5e72\u9884\u4ee5\u9632\u6b62\u9519\u8bef\u7d2f\u79ef\u3001\u4ee5\u53ca\u7ea6\u675f\u5e72\u9884\u5411\u91cf\u7684\u5e45\u5ea6\u6765\u907f\u514d\u5e72\u6270\u6570\u503c\u7f16\u7801\uff1b\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6ReFT\u548c\u57fa\u4e8e\u6743\u91cd\u7684PEFT\u3002", "motivation": "\u73b0\u6709\u7684\u8868\u793a\u5fae\u8c03\uff08ReFT\uff09\u5c3d\u7ba1\u5728\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edfPEFT\uff0c\u4f46\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u539f\u56e0\u5728\u4e8e\u96be\u4ee5\u5728\u63a8\u7406\u65e9\u671f\u9636\u6bb5\u4ea7\u751f\u6709\u6548\u7684\u524d\u7f00\u5e76\u4e14\u53ef\u80fd\u6270\u52a8\u6570\u503c\u7f16\u7801\u548c\u9010\u6b65\u9519\u8bef\u7d2f\u79ef\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5b9a\u4f4d\u5e76\u4f18\u5316\u63a8\u7406\u521d\u671f\u7684\u5173\u952e\u9636\u6bb5\uff0c\u540c\u65f6\u4fdd\u62a4\u6570\u503c\u7f16\u7801\u3002", "method": "\u63d0\u51faBias-REstrained Prefix Representation FineTuning\uff08BREP ReFT\uff09\uff0c\u901a\u8fc7\uff081\uff09\u88c1\u526a\u8bad\u7ec3\u6570\u636e\u4ee5\u4f18\u5316\u521d\u59cb\u63a8\u7406\u524d\u7f00\u7684\u751f\u6210\uff0c\uff082\uff09\u5728\u65e9\u671f\u63a8\u7406\u9636\u6bb5\u8fdb\u884c\u5e72\u9884\u4ee5\u963b\u65ad\u9519\u8bef\u7684\u7d2f\u79ef\uff0c\uff083\uff09\u7ea6\u675f\u5e72\u9884\u5411\u91cf\u7684\u5e45\u5ea6\u4ee5\u907f\u514d\u5e72\u6270\u6570\u503c\u7f16\u7801\uff0c\u4ece\u800c\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0cBREP ReFT\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6ReFT\u548c\u57fa\u4e8e\u6743\u91cd\u7684PEFT\uff0c\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BREP ReFT\u901a\u8fc7\u805a\u7126\u63a8\u7406\u521d\u671f\u9636\u6bb5\u5e76\u5bf9\u5e72\u9884\u8fc7\u7a0b\u8fdb\u884c\u53d7\u63a7\u7ea6\u675f\uff0c\u6210\u529f\u63d0\u5347\u4e86ReFT\u5728\u6570\u5b66\u63a8\u7406\u4e0a\u7684\u8868\u73b0\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5bf9\u6570\u503c\u7f16\u7801\u654f\u611f\u7684\u63a8\u7406\u4efb\u52a1\u53cb\u597d\u7684\u5fae\u8c03\u8303\u5f0f\u3002\u6e90\u7801\u516c\u5f00\u4e8eGithub\u3002"}}
{"id": "2511.10710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.10710", "abs": "https://arxiv.org/abs/2511.10710", "authors": ["Giorgio Morales", "Frederic Jurie", "Jalal Fadili"], "title": "Towards Uncertainty Quantification in Generative Model Learning", "comment": "Accepted at EurIPS 2025 Workshop: Epistemic Intelligence in Machine Learning (EIML@EurIPS 2025)", "summary": "While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.", "AI": {"tldr": "\u63d0\u51fa\u7528\u805a\u5408\u7cbe\u786e\u7387-\u53ec\u56de\u66f2\u7ebf\u6765\u91cf\u5316\u751f\u6210\u6a21\u578b\u5bf9\u5206\u5e03\u8fd1\u4f3c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u6bd4\u8f83\u4e0d\u540c\u67b6\u6784\u5728\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u7279\u5f81\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u591a\u6570\u5173\u6ce8\u5b66\u4e60\u5230\u7684\u5206\u5e03\u4e0e\u76ee\u6807\u5206\u5e03\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u5ffd\u7565\u5bf9\u8fd9\u79cd\u8fd1\u4f3c\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0e\u6bd4\u8f83\uff1b\u9700\u8981\u5efa\u7acb\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u6846\u67b6\u6765\u63d0\u5347\u5bf9\u751f\u6210\u6a21\u578b\u53ef\u9760\u6027\u7684\u7406\u89e3\u3002", "method": " formalize \u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5728\u751f\u6210\u6a21\u578b\u5b66\u4e60\u4e2d\u7684\u95ee\u9898\uff0c\u63a2\u8ba8\u57fa\u4e8e\u96c6\u6210\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u66f2\u7ebf\u6765\u63cf\u8ff0\u5206\u5e03\u8fd1\u4f3c\u7684\u4e0d\u786e\u5b9a\u6027\uff1b\u5728\u5408\u6210\u6570\u636e\u96c6\u8fdb\u884c\u521d\u6b65\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u5728\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u7684\u521d\u6b65\u5b9e\u9a8c\u4e2d\uff0c\u805a\u5408\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u66f2\u7ebf\u80fd\u591f\u6355\u6349\u6a21\u578b\u8fd1\u4f3c\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4fbf\u4e8e\u5bf9\u4e0d\u540c\u67b6\u6784\u5728\u4e0d\u786e\u5b9a\u6027\u7279\u5f81\u4e0a\u7684\u7cfb\u7edf\u6bd4\u8f83\u3002", "conclusion": "\u63d0\u51fa\u4e00\u4e2a\u53ef\u884c\u7684\u8bc4\u4f30\u65b9\u5411\uff0c\u672a\u6765\u5de5\u4f5c\u5e94\u6269\u5927\u5230\u771f\u5b9e\u6570\u636e\u3001\u7406\u8bba\u5206\u6790\u548c\u66f4\u4e30\u5bcc\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u63a2\u7d22\u66f4\u590d\u6742\u7684\u4e0d\u786e\u5b9a\u6027\u6e90\u3002"}}
{"id": "2511.10713", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.10713", "abs": "https://arxiv.org/abs/2511.10713", "authors": ["Jun Masaki", "Ariaki Higashi", "Naoko Shinagawa", "Kazuhiko Hirata", "Yuichi Kurita", "Akira Furui"], "title": "Movement-Specific Analysis for FIM Score Classification Using Spatio-Temporal Deep Learning", "comment": "10 pages, 5 figures, 3tables, Accepted for the 2026 IEEE/SICE International Symposium on System Integration (SII 2026), January 11-14, 2026, Cancun, Mexico", "summary": "The functional independence measure (FIM) is widely used to evaluate patients' physical independence in activities of daily living. However, traditional FIM assessment imposes a significant burden on both patients and healthcare professionals. To address this challenge, we propose an automated FIM score estimation method that utilizes simple exercises different from the designated FIM assessment actions. Our approach employs a deep neural network architecture integrating a spatial-temporal graph convolutional network (ST-GCN), bidirectional long short-term memory (BiLSTM), and an attention mechanism to estimate FIM motor item scores. The model effectively captures long-term temporal dependencies and identifies key body-joint contributions through learned attention weights. We evaluated our method in a study of 277 rehabilitation patients, focusing on FIM transfer and locomotion items. Our approach successfully distinguishes between completely independent patients and those requiring assistance, achieving balanced accuracies of 70.09-78.79 % across different FIM items. Additionally, our analysis reveals specific movement patterns that serve as reliable predictors for particular FIM evaluation items.", "AI": {"tldr": "Summary generation failed", "motivation": "Motivation analysis unavailable", "method": "Method extraction failed", "result": "Result analysis unavailable", "conclusion": "Conclusion extraction failed"}}
{"id": "2511.11539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2511.11539", "abs": "https://arxiv.org/abs/2511.11539", "authors": ["Diptarka Chakraborty", "Kushagra Chatterjee", "Debarati Das", "Tien-Long Nguyen"], "title": "Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications", "comment": "Accepted in AAAI 2026 for Oral Representation", "summary": "Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \\emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.\n  In this work, we generalize the study of the \\emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].\n  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \\emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \\emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].", "AI": {"tldr": "\u628a\u805a\u7c7b\u95ee\u9898\u6269\u5c55\u5230\u591a\u7ec4\u4fdd\u62a4\u5c5e\u6027\u7684\u516c\u5e73\u805a\u7c7b\uff0c\u7ed9\u51faNP-hard\u6027\u8bc1\u660e\u548c\u8fd1\u7ebf\u65f6\u95f4\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u63d0\u5347\u4e86\u516c\u5e73\u76f8\u5173\u805a\u7c7b\u548c\u516c\u5e73\u5171\u8bc6\u805a\u7c7b\u7684\u8fd1\u4f3c\u7ed3\u679c\uff0c\u5e76\u9996\u6b21\u7ed9\u51fa\u591a\u7ec4\u60c5\u666f\u4e0b\u7684\u8fd1\u4f3c\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e2d\u805a\u7c7b\u5bf9\u5305\u542b\u5e74\u9f84\u3001 ethnicity\u3001\u6027\u522b\u7b49\u591a\u91cd\u4fdd\u62a4\u5c5e\u6027\u7684\u7fa4\u4f53\u7684\u516c\u5e73 Representation\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f25\u8865\u4ec5\u5728\u4e24\u7ec4\u60c5\u5f62\u4e0b\u5df2\u6709\u89e3\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u8bc1\u660e\u5728\u4efb\u610f\u6570\u91cf\u7684\u7ec4\u4e14\u7ec4\u5927\u5c0f\u76f8\u7b49\u65f6\uff0cclosest fair clustering NP-hard\uff1b\u63d0\u51fa\u8fd1\u7ebf\u65f6\u95f4\u7684\u591a\u7ec4\u60c5\u5f62\u8fd1\u4f3c\u7b97\u6cd5\uff1b\u4ee5closest fair clustering\u4e3a\u57fa\uff0c\u6539\u8fdb\u516c\u5e73\u76f8\u5173\u805a\u7c7b\uff08fair correlation clustering\uff09\u7684\u8fd1\u4f3c\uff1b\u9996\u6b21\u7ed9\u51fa\u591a\u7ec4\u60c5\u5f62\u4e0b\u7684\u516c\u5e73\u5171\u8bc6\u805a\u7c7b\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u83b7\u5f97\u8fd1\u7ebf\u65f6\u95f4\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u4efb\u610f\u5927\u5c0f\u7684\u591a\u7ec4\u6570\u636e\uff1b\u63d0\u9ad8\u4e86\u76f8\u5173\u4e0e\u5171\u8bc6\u805a\u7c7b\u7684\u8fd1\u4f3c\u4fdd\u8bc1\uff1b\u89e3\u51b3\u4e86COLT'25\u548cAISTATS'20\u7b49\u5de5\u4f5c\u4e2d\u7684\u5f00\u653e\u95ee\u9898\u3002", "conclusion": "\u5c06 closest fair clustering \u4ece\u4e24\u7ec4\u6269\u5c55\u5230\u591a\u7ec4\uff0c\u63a8\u52a8\u516c\u5e73\u805a\u7c7b\u7814\u7a76\u7684\u7406\u8bba\u4e0e\u5b9e\u8df5\u8fdb\u5c55\uff0c\u4e3a\u540e\u5904\u7406\u516c\u5e73\u6027\u63d0\u4f9b\u6709\u6548\u5de5\u5177\uff0c\u5e76\u4e3a\u591a\u5c5e\u6027\u4fdd\u62a4\u4e0b\u7684\u805a\u7c7b\u516c\u5e73\u6027\u5960\u5b9a\u57fa\u7840\u3002"}}
