<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 7]
- [cs.LG](#cs.LG) [Total: 58]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Disentangled representations via score-based variational autoencoders](https://arxiv.org/abs/2512.17127)
*Benjamin S. H. Lyo,Eero P. Simoncelli,Cristina Savin*

Main category: stat.ML

TL;DR: SAMI 将扩散模型与变分自编码器（VAE）结合，通过统一证据下界来学习分层、可解释的潜在表征，能够从静态图像中学习到语义因子、在自然图像上实现因子化潜在维度，并将视频序列编码为更直的潜在轨迹，同时可从预训练扩散模型中提取表征，给出一个严格的概率性框架以发现无监督的语义轴。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型的隐式结构中挖掘可解释、可操作的潜在表征，并通过将扩散模型的证据下界与VAE结合，获得一个统一而严格的目标函数，从而在无监督条件下发现数据的语义因素。

Method: 将扩散模型的分数驱动过程与变分自编码器结合，统一两者的ELBO，提出一个从噪声到数据的分层生成与编码框架；通过分数指导的扩散过程实现对潜在变量的正则化和解释性约束；可对静态图像进行训练并在保持静态图片训练的前提下推导出视频潜在轨迹，以及从预训练扩散模型中提取表征。

Result: 在合成数据中复现真实生成因子，在复杂自然图像中学习到因子化的语义潜在维度，能够把视频序列编码为比其他编码器更直的潜在轨迹；能够用最少额外训练从预训练扩散模型中提取有用表征；给出无监督条件下识别语义轴的新途径，以及严格的概率化表述以便对学习的表征进行形式化分析。

Conclusion: 隐式扩散模型中的结构信息可以通过与VAE的协同组合变得显式且可解释，从而实现对无监督表征的更深理解与控制，具有潜在的研究与应用价值。

Abstract: We present the Score-based Autoencoder for Multiscale Inference (SAMI), a method for unsupervised representation learning that combines the theoretical frameworks of diffusion models and VAEs. By unifying their respective evidence lower bounds, SAMI formulates a principled objective that learns representations through score-based guidance of the underlying diffusion process. The resulting representations automatically capture meaningful structure in the data: it recovers ground truth generative factors in our synthetic dataset, learns factorized, semantic latent dimensions from complex natural images, and encodes video sequences into latent trajectories that are straighter than those of alternative encoders, despite training exclusively on static images. Furthermore, SAMI can extract useful representations from pre-trained diffusion models with minimal additional training. Finally, the explicitly probabilistic formulation provides new ways to identify semantically meaningful axes in the absence of supervised labels, and its mathematical exactness allows us to make formal statements about the nature of the learned representation. Overall, these results indicate that implicit structural information in diffusion models can be made explicit and interpretable through synergistic combination with a variational autoencoder.

</details>


### [2] [Sharp Structure-Agnostic Lower Bounds for General Functional Estimation](https://arxiv.org/abs/2512.17341)
*Jikai Jin,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \citet{jin2024structure} by the same authors.

</details>


### [3] [Generative modeling of conditional probability distributions on the level-sets of collective variables](https://arxiv.org/abs/2512.17374)
*Fatima-Zahrae Akhyar,Wei Zhang,Gabriel Stoltz,Christof Schütte*

Main category: stat.ML

TL;DR: 提出一种统一的条件生成模型，用于同级别集ξ的多组分条件分布的学习，并结合增强采样的数据增强，在数值示例中验证，潜在可用于分子系统生成建模。


<details>
  <summary>Details</summary>
Motivation: 在高维分布中，对给定聚集变量ξ的等高线/等值面上的条件分布进行建模具有重要意义，尤其是在低概率区域。现有方法往往只能对单一层级集进行建模，且在稀缺区域性能不佳。本工作提出一个可同时学习多个ξ层级集的通用高效框架，并通过增强采样提升数据质量。

Method: 提出一种通用高效的学习框架，能够在不同ξ层级集上同时学习条件生成模型；通过跨层级的参数共享或结构设计实现对多层级的联合建模；并引入利用增强采样数据进行数据丰富化的策略，以改善低概率区域的学习效果。

Result: 通过具体数值例子验证所提方法的有效性，展示在多个ξ层级集上的一致性和稳定性，以及对低概率区域的改进。所提出的方法对包括分子系统在内的高维生成建模具有潜在应用。

Conclusion: 该方法为多层级条件生成建模提供一个通用框架，并通过数据增强提升稀有区域的建模质量，具有在生物物理等领域的潜在应用前景。

Abstract: Given a probability distribution $μ$ in $\mathbb{R}^d$ represented by data, we study in this paper the generative modeling of its conditional probability distributions on the level-sets of a collective variable $ξ: \mathbb{R}^d \rightarrow \mathbb{R}^k$, where $1 \le k<d$. We propose a general and effcient learning approach that is able to learn generative models on different level-sets of $ξ$ simultaneously. To improve the learning quality on level-sets in low-probability regions, we also propose a strategy for data enrichment by utilizing data from enhanced sampling techniques. We demonstrate the effectiveness of our proposed learning approach through concrete numerical examples. The proposed approach is potentially useful for the generative modeling of molecular systems in biophysics, for instance.

</details>


### [4] [Perfect reconstruction of sparse signals using nonconvexity control and one-step RSB message passing](https://arxiv.org/abs/2512.17426)
*Xiaosi Gu,Ayaka Sakata,Tomoyuki Obuchi*

Main category: stat.ML

TL;DR: 提出1RSB-AMP用于SCAD稀疏信号重建，给出1RSB-SE/更新规则，并比较与RS-AMP的表现，提出新的边界与NCC协议，提升完美重建的算法性极限。


<details>
  <summary>Details</summary>
Motivation: 在非凸惩罚下的稀疏信号重建中，探索1RSB修正的AMP可否改善算法极限，并理解热力学性质（重叠、自由熵、复杂度等）在1RSB中的作用。

Method: 从1RSB的BP推导出1RSB-AMP更新规则，给出1RSB-SE方程；进行RS与1RSB两者的对比分析；固定点分析得到相图；提出以最小化发散区域大小作为Parisi参数的选择准则，并结合NCC协议提升算法极限；数值解和实验验证。

Result: 1RSB-AMP与1RSB-SE在宏观层面高度一致；在RS-AMP发散区域，1RSB描述仍然可用；相图包含成功、失败、发散三区，发散边界受Parisi参数影响；采用最小化发散区大小的准则结合NCC可提升完美重建的算法性极限；实验和数值解证实提升虽有，但仍略低于贝叶斯最优阈值；给出1RSB热力学量的行为。

Conclusion: 1RSB修正可以在非凸惩罚下改善算法极限，提供对热力学量的洞察，但实际改进有限，且边界受Parisi参数控制，需要额外准则/协议以接近Bayes阈值。

Abstract: We consider sparse signal reconstruction via minimization of the smoothly clipped absolute deviation (SCAD) penalty, and develop one-step replica-symmetry-breaking (1RSB) extensions of approximate message passing (AMP), termed 1RSB-AMP. Starting from the 1RSB formulation of belief propagation, we derive explicit update rules of 1RSB-AMP together with the corresponding state evolution (1RSB-SE) equations. A detailed comparison shows that 1RSB-AMP and 1RSB-SE agree remarkably well at the macroscopic level, even in parameter regions where replica-symmetric (RS) AMP, termed RS-AMP, diverges and where the 1RSB description itself is not expected to be thermodynamically exact. Fixed-point analysis of 1RSB-SE reveals a phase diagram consisting of success, failure, and diverging phases, as in the RS case. However, the diverging-region boundary now depends on the Parisi parameter due to the 1RSB ansatz, and we propose a new criterion -- minimizing the size of the diverging region -- rather than the conventional zero-complexity condition, to determine its value. Combining this criterion with the nonconvexity-control (NCC) protocol proposed in a previous RS study improves the algorithmic limit of perfect reconstruction compared with RS-AMP. Numerical solutions of 1RSB-SE and experiments with 1RSB-AMP confirm that this improved limit is achieved in practice, though the gain is modest and remains slightly inferior to the Bayes-optimal threshold. We also report the behavior of thermodynamic quantities -- overlaps, free entropy, complexity, and the non-self-averaging susceptibility -- that characterize the 1RSB phase in this problem.

</details>


### [5] [Fast and Robust: Computationally Efficient Covariance Estimation for Sub-Weibull Vectors](https://arxiv.org/abs/2512.17632)
*Even He*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High-dimensional covariance estimation is notoriously sensitive to outliers. While statistically optimal estimators exist for general heavy-tailed distributions, they often rely on computationally expensive techniques like semidefinite programming or iterative M-estimation ($O(d^3)$). In this work, we target the specific regime of \textbf{Sub-Weibull distributions} (characterized by stretched exponential tails $\exp(-t^α)$). We investigate a computationally efficient alternative: the \textbf{Cross-Fitted Norm-Truncated Estimator}. Unlike element-wise truncation, our approach preserves the spectral geometry while requiring $O(Nd^2)$ operations, which represents the theoretical lower bound for constructing a full covariance matrix. Although spherical truncation is geometrically suboptimal for anisotropic data, we prove that within the Sub-Weibull class, the exponential tail decay compensates for this mismatch. Leveraging weighted Hanson-Wright inequalities, we derive non-asymptotic error bounds showing that our estimator recovers the optimal sub-Gaussian rate $\tilde{O}(\sqrt{r(Σ)/N})$ with high probability. This provides a scalable solution for high-dimensional data that exhibits tails heavier than Gaussian but lighter than polynomial decay.

</details>


### [6] [Generative Multi-Objective Bayesian Optimization with Scalable Batch Evaluations for Sample-Efficient De Novo Molecular Design](https://arxiv.org/abs/2512.17659)
*Madhav R. Muthyala,Farshud Sorourifar,Tianhong Tan,You Peng,Joel A. Paulson*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Designing molecules that must satisfy multiple, often conflicting objectives is a central challenge in molecular discovery. The enormous size of chemical space and the cost of high-fidelity simulations have driven the development of machine learning-guided strategies for accelerating design with limited data. Among these, Bayesian optimization (BO) offers a principled framework for sample-efficient search, while generative models provide a mechanism to propose novel, diverse candidates beyond fixed libraries. However, existing methods that couple the two often rely on continuous latent spaces, which introduces both architectural entanglement and scalability challenges. This work introduces an alternative, modular "generate-then-optimize" framework for de novo multi-objective molecular design/discovery. At each iteration, a generative model is used to construct a large, diverse pool of candidate molecules, after which a novel acquisition function, qPMHI (multi-point Probability of Maximum Hypervolume Improvement), is used to optimally select a batch of candidates most likely to induce the largest Pareto front expansion. The key insight is that qPMHI decomposes additively, enabling exact, scalable batch selection via only simple ranking of probabilities that can be easily estimated with Monte Carlo sampling. We benchmark the framework against state-of-the-art latent-space and discrete molecular optimization methods, demonstrating significant improvements across synthetic benchmarks and application-driven tasks. Specifically, in a case study related to sustainable energy storage, we show that our approach quickly uncovers novel, diverse, and high-performing organic (quinone-based) cathode materials for aqueous redox flow battery applications.

</details>


### [7] [Imputation Uncertainty in Interpretable Machine Learning Methods](https://arxiv.org/abs/2512.17689)
*Pegah Golchian,Marvin N. Wright*

Main category: stat.ML

TL;DR: 本研究比较不同缺失数据处理（单重插补 vs 多重插补）对可解释性机器学习（IML）方法的置信区间覆盖概率的影响，并发现单重插补常低估方差，而多数情况下只有多重插补的覆盖率接近名义水平。


<details>
  <summary>Details</summary>
Motivation: 现实数据中的缺失值普遍存在，且缺失处理不充分会影响对模型解释的置信性与稳定性。需要评估不同插补方法及其不确定性对解释性量量（如置换特征重要性、部分依赖图、Shapley值）置信区间的覆盖情况。

Method: 比较不同插补方法对 IML 方法的置信区间覆盖的影响，具体评估置换特征重要性、部分依赖图和 Shapley 值的置信区间覆盖率；在单重插补与多重插补条件下，分析方差估计及覆盖率的差异，可能结合真实数据与模拟数据进行评估。

Result: 单重插补普遍低估方差，导致置信区间过于狭窄；在大多数情形下，只有多重插补的覆盖率接近名义水平（如 95%）。

Conclusion: 在含缺失数据的情境下进行 IML 结论推断时，应采用多重插补来充分考虑插补的不确定性，以获得更可靠的置信区间；单重插补可能导致对解释性量量的过度自信。

Abstract: In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [8] [Dion2: A Simple Method to Shrink Matrix in Muon](https://arxiv.org/abs/2512.16928)
*Kwangjun Ahn,Noah Amsel,John Langford*

Main category: cs.LG

TL;DR: Dion2 提供一个简单且高效的稀疏更新策略，在 Muon 的正交化步骤中仅对选定的行/列进行正交化，显著降低成本并提升可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决 Muon 的正交化阶段的超线性成本问题，尤其在大规模数据/模型下的计算与通信开销。

Method: 在每次迭代中随机或策略性地选择矩阵的一部分行或列，仅对所选子集执行正交化，使更新保持稀疏，减小参与矩阵的规模，降低成本。

Result: 通过减少被正交化的矩阵规模和实现稀疏更新，降低计算与通信成本，从而提升 Muon 的可扩展性。

Conclusion: 相比于先前减少矩阵规模的做法，Dion2 更简单、易实现，提供一种更具扩展性的替代方案来缓解正交化阶段的瓶颈。

Abstract: The Muon optimizer enjoys strong empirical performance and theoretical grounding. However, the super-linear cost of its orthonormalization step introduces increasing overhead with scale. To alleviate this cost, several works have attempted to reduce the size of the matrix entering the orthonormalization step. We introduce Dion2, a much simpler method for shrinking the matrix involved in Muon's computation compared to prior approaches. At a high level, Dion2 selects a fraction of rows or columns at each iteration and orthonormalizes only those. This sampling procedure makes the update sparse, reducing both computation and communication costs which in turn improves the scalability of Muon.

</details>


### [9] [BIONIX: A Wireless, Low-Cost Prosthetic Arm with Dual-Signal EEG and EMG Control](https://arxiv.org/abs/2512.16929)
*Pranesh Sathish Kumar*

Main category: cs.LG

TL;DR: 开发了一套低成本的双模态脑肌控制系统，结合EEG和EMG实现实时多自由度上肢假肢控制，总成本约240美元，原型具备四指伺服与两肘伺服的控制能力。


<details>
  <summary>Details</summary>
Motivation: 解决资源匮乏地区的可负担与直观的上肢假肢控制问题，提升普及率与功能性。

Method: 使用 NeuroSky MindWave Mobile 2 采集EEG，经过 ThinkGear Bluetooth到ESP32，采用轻量分类模型与8帧稳定性阈值，检测强眨事件以切换手部开合；EMG通过 MyoWare 2.0 传感器与 SparkFun 无线 shield传到另一 ESP32，基于阈值进行检测；设定三个激活区间(rest、extension、contraction)控制肘部； EEG 控制ESP32驱动四指伺服，EMG控制两肘伺服。

Result: 构建了一个功能性原型，成本约240美元，手指伺服四个、肘部伺服两个；未来工作包括3D打印底盘、引入自回归模型以降低EMG延迟、提升伺服扭矩和抓握力。

Conclusion: 证明该低成本、生物学直观的控制方案在服务不足地区的全球健康应用中具有可行性与潜力。

Abstract: Affordable upper-limb prostheses often lack intuitive control systems, limiting functionality and accessibility for amputees in low-resource settings. This project presents a low-cost, dual-mode neuro-muscular control system integrating electroencephalography (EEG) and electromyography (EMG) to enable real-time, multi-degree-of-freedom control of a prosthetic arm. EEG signals are acquired using the NeuroSky MindWave Mobile 2 and transmitted via ThinkGear Bluetooth packets to an ESP32 microcontroller running a lightweight classification model. The model was trained on 1500 seconds of recorded EEG data using a 6-frame sliding window with low-pass filtering, excluding poor-signal samples and using a 70/20/10 training--validation--test split. The classifier detects strong blink events, which toggle the hand between open and closed states. EMG signals are acquired using a MyoWare 2.0 sensor and SparkFun wireless shield and transmitted to a second ESP32, which performs threshold-based detection. Three activation bands (rest: 0--T1; extension: T1--T2; contraction: greater than T2) enable intuitive elbow control, with movement triggered only after eight consecutive frames in a movement class to improve stability. The EEG-controlled ESP32 actuates four finger servos, while the EMG-controlled ESP32 drives two elbow servos. A functional prototype was constructed using low-cost materials (total cost approximately 240 dollars), with most expense attributed to the commercial EEG headset. Future work includes transitioning to a 3D-printed chassis, integrating auto-regressive models to reduce EMG latency, and upgrading servo torque for improved load capacity and grip strength. This system demonstrates a feasible pathway to low-cost, biologically intuitive prosthetic control suitable for underserved and global health applications.

</details>


### [10] [Compression is Routing: Reconstruction Error as an Intrinsic Signal for Modular Language Models](https://arxiv.org/abs/2512.16963)
*Zhongpan Tang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current Large Language Models (LLMs) face three major challenges: context length limitations, high inference costs, and catastrophic forgetting during continual learning. While Mixture-of-Experts (MoE) architectures mitigate some of these conflicts, their routing mechanisms typically rely on explicitly trained auxiliary classifiers. This not only increases system complexity but also often lacks interpretability when handling mixed-domain inputs.
  Building upon the premise that ``Compression is Intelligence,'' this paper proposes a novel architectural philosophy: \textbf{``Compression is Routing.''} We trained an 87M-parameter end-to-end Transformer Autoencoder, achieving a \textbf{64x sequence length compression} (compressing 512 tokens into 8 latent vectors). Experimental results demonstrate that this compressor possesses extreme domain discriminative capability: it achieves a reconstruction accuracy of \textbf{99.47\%} on the in-domain (code) validation set; accuracy drops sharply to \textbf{47.76\%} on a semi-out-of-distribution domain (Wiki text); and further plummets to just \textbf{0.57\%} on a fully out-of-distribution domain (random sequences).
  This extreme and systematic performance discrepancy establishes the validity of reconstruction error as an \textbf{Intrinsic Distribution Fingerprint}. Based on this, we propose that expert modules can be automatically scheduled using reconstruction residuals directly, without the need for explicit gating networks. This mechanism offers excellent scalability. Furthermore, this architecture provides a new perspective on ``VRAM compression'' for handling ultra-long contexts. This report aims to verify the physical validity of this foundational architecture, offering a new research perspective for the next generation of scalable modular neural networks.

</details>


### [11] [Learning solution operator of dynamical systems with diffusion maps kernel ridge regression](https://arxiv.org/abs/2512.17203)
*Jiwoo Song,Daning Huang,John Harlim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many scientific and engineering systems exhibit complex nonlinear dynamics that are difficult to predict accurately over long time horizons. Although data-driven models have shown promise, their performance often deteriorates when the geometric structures governing long-term behavior are unknown or poorly represented. We demonstrate that a simple kernel ridge regression (KRR) framework, when combined with a dynamics-aware validation strategy, provides a strong baseline for long-term prediction of complex dynamical systems. By employing a data-driven kernel derived from diffusion maps, the proposed Diffusion Maps Kernel Ridge Regression (DM-KRR) method implicitly adapts to the intrinsic geometry of the system's invariant set, without requiring explicit manifold reconstruction or attractor modeling, procedures that often limit predictive performance. Across a broad range of systems, including smooth manifolds, chaotic attractors, and high-dimensional spatiotemporal flows, DM-KRR consistently outperforms state-of-the-art random feature, neural-network and operator-learning methods in both accuracy and data efficiency. These findings underscore that long-term predictive skill depends not only on model expressiveness, but critically on respecting the geometric constraints encoded in the data through dynamically consistent model selection. Together, simplicity, geometry awareness, and strong empirical performance point to a promising path for reliable and efficient learning of complex dynamical systems.

</details>


### [12] [Physics-Informed Lightweight Machine Learning for Aviation Visibility Nowcasting Across Multiple Climatic Regimes](https://arxiv.org/abs/2512.16967)
*Marcelo Cerda Castillo*

Main category: cs.LG

TL;DR: 基于 METAR 的轻量级 XGBoost 现在预报框架在机场低能见度/降水事件上优于现有 TAF，具较高召回率和较低误警，且通过 SHAP 实现对物理过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决航行安全和运营效率对高时间分辨率现在预报的需求，克服依赖计算密集的 NWP 指引和保守的 TAF 的局限性，提出更轻量、可解释的解决方案。

Method: 在 11 个机场、2000-2024 年历史数据上，使用 METAR 构建特征并进行物理引导的特征工程，训练 XGBoost；与运营 TAF 在盲评中比较；利用 SHAP 进行可解释性分析。

Result: 在 3 小时战术时间窗内，模型检测率显著提高，召回率比 TAF 提升约 2.5–4 倍，误警下降；SHAP 表明模型能隐式重建平流、辐射、下沉等局部物理驱动，具备落地潜力。

Conclusion: 证明轻量级机器学习模型能捕捉局部物理过程并提供可操作的解释性，适用于边缘计算场景，有望替代或增强现有 TAF/预报体系。

Abstract: Short-term prediction (nowcasting) of low-visibility and precipitation events is critical for aviation safety and operational efficiency. Current operational approaches rely on computationally intensive numerical weather prediction guidance and human-issued TAF products, which often exhibit conservative biases and limited temporal resolution. This study presents a lightweight gradient boosting framework (XGBoost) trained exclusively on surface observation data (METAR) and enhanced through physics-guided feature engineering based on thermodynamic principles. The framework is evaluated across 11 international airports representing distinct climatic regimes (including SCEL, KJFK, KORD, KDEN, SBGR, and VIDP) using historical data from 2000 to 2024. Results suggest that the model successfully captures underlying local physical processes without manual configuration. In a blind comparative evaluation against operational TAF forecasts, the automated model achieved substantially higher detection rates at tactical horizons (3 hours), with a 2.5 to 4.0 times improvement in recall while reducing false alarms. Furthermore, SHAP analysis reveals that the model performs an implicit reconstruction of local physical drivers (advection, radiation, and subsidence), providing actionable explainability for operational situational awareness.
  Keywords: aviation meteorology; physics-guided machine learning; explainable artificial intelligence; lightweight machine learning; nowcasting; METAR; TAF verification; edge computing

</details>


### [13] [MINPO: Memory-Informed Neural Pseudo-Operator to Resolve Nonlocal Spatiotemporal Dynamics](https://arxiv.org/abs/2512.17273)
*Farinaz Mostajeran,Aruzhan Tleubek,Salah A Faroughi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many physical systems exhibit nonlocal spatiotemporal behaviors described by integro-differential equations (IDEs). Classical methods for solving IDEs require repeatedly evaluating convolution integrals, whose cost increases quickly with kernel complexity and dimensionality. Existing neural solvers can accelerate selected instances of these computations, yet they do not generalize across diverse nonlocal structures. In this work, we introduce the Memory-Informed Neural Pseudo-Operator (MINPO), a unified framework for modeling nonlocal dynamics arising from long-range spatial interactions and/or long-term temporal memory. MINPO, employing either Kolmogorov-Arnold Networks (KANs) or multilayer perceptron networks (MLPs) as encoders, learns the nonlocal operator and its inverse directly through neural representations, and then explicitly reconstruct the unknown solution fields. The learning is guarded by a lightweight nonlocal consistency loss term to enforce coherence between the learned operator and reconstructed solution. The MINPO formulation allows to naturally capture and efficiently resolve nonlocal spatiotemporal dependencies governed by a wide spectrum of IDEs and their subsets, including fractional PDEs. We evaluate the efficacy of MINPO in comparison with classical techniques and state-of-the-art neural-based strategies based on MLPs, such as A-PINN and fPINN, along with their newly-developed KAN variants, A-PIKAN and fPIKAN, designed to facilitate a fair comparison. Our study offers compelling evidence of the accuracy of MINPO and demonstrates its robustness in handling (i) diverse kernel types, (ii) different kernel dimensionalities, and (iii) the substantial computational demands arising from repeated evaluations of kernel integrals. MINPO, thus, generalizes beyond problem-specific formulations, providing a unified framework for systems governed by nonlocal operators.

</details>


### [14] [Turn-PPO: Turn-Level Advantage Estimation with PPO for Improved Multi-Turn RL in Agentic LLMs](https://arxiv.org/abs/2512.17008)
*Junbo Li,Peng Zhou,Rui Meng,Meet P. Vadera,Lihong Li,Yang Li*

Main category: cs.LG

TL;DR: PPO 相较 GRPO 在多轮任务中更稳健，提出的 turn-PPO 通过回合级 MDP 提升长程推理能力，在 WebShop 与 Sokoban 数据集上表现良好。


<details>
  <summary>Details</summary>
Motivation: 解决 GRPO 在长时推理中的局限，寻找更稳定的优势估计和多轮任务性能提升的方法。

Method: 比较 PPO 与 GRPO 的表现，提出 turn-PPO，构建基于回合级 MDP 的策略优化框架，适用于多轮任务；在是否需要长程推理的情形下测试。

Result: turn-PPO 在 WebShop 与 Sokoban 上表现出对多轮任务更好的鲁棒性和有效性，且在有无长推理组件的场景中都有效。

Conclusion: turn-PPO 将成为多轮对话/交互式仿真任务中的更稳健选择，为多轮 RL 提供更可靠的优势估计和策略更新。

Abstract: Reinforcement learning (RL) has re-emerged as a natural approach for training interactive LLM agents in real-world environments. However, directly applying the widely used Group Relative Policy Optimization (GRPO) algorithm to multi-turn tasks exposes notable limitations, particularly in scenarios requiring long-horizon reasoning. To address these challenges, we investigate more stable and effective advantage estimation strategies, especially for multi-turn settings. We first explore Proximal Policy Optimization (PPO) as an alternative and find it to be more robust than GRPO. To further enhance PPO in multi-turn scenarios, we introduce turn-PPO, a variant that operates on a turn-level MDP formulation, as opposed to the commonly used token-level MDP. Our results on the WebShop and Sokoban datasets demonstrate the effectiveness of turn-PPO, both with and without long reasoning components.

</details>


### [15] [Polyharmonic Cascade](https://arxiv.org/abs/2512.17671)
*Yuriy N. Bakhvalov*

Main category: cs.LG

TL;DR: 提出一种名为多超调级联（polyharmonic cascade）的深度学习架构，使用多调和样条的层级包组合，训练通过对每个批次进行全局线性系统求解而非梯度下降，具备全局光滑性和概率解释，并在GPU上以2D矩阵运算实现，MNIST上实现快速学习和较少过拟合。


<details>
  <summary>Details</summary>
Motivation: 在不牺牲全局光滑性和概率解释的前提下，提供一种可扩展、与原模型理论一致且避免梯度下降的训练方式，以更高效地拟合任意复杂度的非线性函数。

Method: 提出一个以多调和样条打包的深度层级结构；每个训练批次通过对固定节点星座的函数值建立一个全局线性系统来更新所有层，达到同步更新、保持各层的概率解释和理论一致性；所有计算降维为2D矩阵运算，在GPU上高效执行。

Result: 在MNIST数据集上演示了快速学习且对过拟合有抑制作用。

Conclusion: 该方法提供了一种严格且可扩展的无梯度下降训练框架，保留概率解释与理论一致性，适用于处理任意复杂度的非线性函数，具有良好的并行性和GPU实现潜力。

Abstract: This paper presents a deep machine learning architecture, the "polyharmonic cascade" -- a sequence of packages of polyharmonic splines, where each layer is rigorously derived from the theory of random functions and the principles of indifference. This makes it possible to approximate nonlinear functions of arbitrary complexity while preserving global smoothness and a probabilistic interpretation. For the polyharmonic cascade, a training method alternative to gradient descent is proposed: instead of directly optimizing the coefficients, one solves a single global linear system on each batch with respect to the function values at fixed "constellations" of nodes. This yields synchronized updates of all layers, preserves the probabilistic interpretation of individual layers and theoretical consistency with the original model, and scales well: all computations reduce to 2D matrix operations efficiently executed on a GPU. Fast learning without overfitting on MNIST is demonstrated.

</details>


### [16] [GB-DQN: Gradient Boosted DQN Models for Non-stationary Reinforcement Learning](https://arxiv.org/abs/2512.17034)
*Chang-Hwan Lee,Chanseung Lee*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Non-stationary environments pose a fundamental challenge for deep reinforcement learning, as changes in dynamics or rewards invalidate learned value functions and cause catastrophic forgetting. We propose \emph{Gradient-Boosted Deep Q-Networks (GB-DQN)}, an adaptive ensemble method that addresses model drift through incremental residual learning. Instead of retraining a single Q-network, GB-DQN constructs an additive ensemble in which each new learner is trained to approximate the Bellman residual of the current ensemble after drift. We provide theoretical results showing that each boosting step reduces the empirical Bellman residual and that the ensemble converges to the post-drift optimal value function under standard assumptions. Experiments across a diverse set of control tasks with controlled dynamics changes demonstrate faster recovery, improved stability, and greater robustness compared to DQN and common non-stationary baselines.

</details>


### [17] [Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space](https://arxiv.org/abs/2512.17884)
*Xinyue Yu,Hayden Schaeffer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student's $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers', Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.

</details>


### [18] [SFBD-OMNI: Bridge models for lossy measurement restoration with limited clean samples](https://arxiv.org/abs/2512.17051)
*Haoye Lu,Yaoliang Yu,Darren Ho*

Main category: cs.LG

TL;DR: 提出了一种从大量噪声观测中恢复真实分布的方法，利用单边熵最优传输框架并给出EM样算法；提出可判定可恢复性的测试准则，在信息损失严重时，通过少量干净样本也能实现较好恢复；基于此提出 SFBD-OMNI 桥接模型，将受损分布映射到真实分布，并将 SFBD 扩展到通用测量模型；在基准数据集和多种测量设置下获得显著提升。


<details>
  <summary>Details</summary>
Motivation: 在现实场景中，获取完全观测样本成本高甚至不可行，而部分且带噪声的观测更易获得。若能在知道或可访问的腐蚀过程（黑箱生成器）的前提下，恢复真实分布，将极大提升下游任务的性能与鲁棒性。

Method: 将分布恢复问题建模为单边信息熵最优传输，并通过类似 EM 的迭代算法求解。提出可判定“在每样本信息丢失下是否能恢复真实分布”的测试准则；在不可恢复的情形中，少量干净样本也能显著提升恢复能力。基于此，提出 SFBD-OMNI 桥接框架，将受损样本分布映射回 ground-truth 分布，并将 SFBD（Lu 等，2025）的思路推广至可处理任意测量模型（不仅限高斯腐蚀）。

Result: 在多组基准数据集与多样化测量设置下，方法在定性与定量指标上均显示显著提升，相比现有方法更具鲁棒性与广泛适用性。

Conclusion: 本工作建立了一个从受损观测到真实分布的系统化桥接方法，明确了可恢复性的边界，并证明在信息损失条件下通过少量干净数据也可实现较好恢复。SFBD-OMNI 为通用测量模型提供了一种有效的框架扩展，扩展了 SFBD 的适用范围。

Abstract: In many real-world scenarios, obtaining fully observed samples is prohibitively expensive or even infeasible, while partial and noisy observations are comparatively easy to collect. In this work, we study distribution restoration with abundant noisy samples, assuming the corruption process is available as a black-box generator. We show that this task can be framed as a one-sided entropic optimal transport problem and solved via an EM-like algorithm. We further provide a test criterion to determine whether the true underlying distribution is recoverable under per-sample information loss, and show that in otherwise unrecoverable cases, a small number of clean samples can render the distribution largely recoverable. Building on these insights, we introduce SFBD-OMNI, a bridge model-based framework that maps corrupted sample distributions to the ground-truth distribution. Our method generalizes Stochastic Forward-Backward Deconvolution (SFBD; Lu et al., 2025) to handle arbitrary measurement models beyond Gaussian corruption. Experiments across benchmark datasets and diverse measurement settings demonstrate significant improvements in both qualitative and quantitative performance.

</details>


### [19] [Dynamic Tool Dependency Retrieval for Efficient Function Calling](https://arxiv.org/abs/2512.17052)
*Bhrij Patel,Davide Belli,Amir Jalalirad,Maximilian Arnold,Aleksandr Ermovol,Bence Major*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Function calling agents powered by Large Language Models (LLMs) select external tools to automate complex tasks. On-device agents typically use a retrieval module to select relevant tools, improving performance and reducing context length. However, existing retrieval methods rely on static and limited inputs, failing to capture multi-step tool dependencies and evolving task context. This limitation often introduces irrelevant tools that mislead the agent, degrading efficiency and accuracy. We propose Dynamic Tool Dependency Retrieval (DTDR), a lightweight retrieval method that conditions on both the initial query and the evolving execution context. DTDR models tool dependencies from function calling demonstrations, enabling adaptive retrieval as plans unfold. We benchmark DTDR against state-of-the-art retrieval methods across multiple datasets and LLM backbones, evaluating retrieval precision, downstream task accuracy, and computational efficiency. Additionally, we explore strategies to integrate retrieved tools into prompts. Our results show that dynamic tool retrieval improves function calling success rates between $23\%$ and $104\%$ compared to state-of-the-art static retrievers.

</details>


### [20] [Universal consistency of the $k$-NN rule in metric spaces and Nagata dimension. III](https://arxiv.org/abs/2512.17058)
*Vladimir G. Pestov*

Main category: cs.LG

TL;DR: 证明了三性等价中的最后一个蕴含：若 k-NN 分类器在 X 上一致，则 X 的 Nagata 的 sigma-有限维数成立。这完成了 (1) ⇔ (2) ⇔ (3) 的完全等价，并纠正了前文中的错误断言。


<details>
  <summary>Details</summary>
Motivation: 将学习理论中的最近邻分类器的泛化性质与测度几何属性（Lebesgue-Besicovitch 微分性质）以及 Nagata 维数的几何-拓扑性质联系起来，揭示 metric space 上不同领域结果的统一结构。

Method: 给出直接的证明 (1) ⇒ (3)，在已有的 (2) ⇔ (3) 与 (2) ⇒ (1) 的基础上，整合 Lebesgue- Besicovitch 微分性质、可测测度条件以及 Nagata 维数的结构性质，修正了2024年的错误结论，并呼应 2020 年系列论文中的猜想。

Result: 证明在完整可分度量空间 X 上，若 k-NN 分类器在 X 上(weakly)普遍一致，则 X 是 Nagata 的 sigma-有限维数；因此三条件等价的最后一个方向成立，完整完成该系列的等价框架。

Conclusion: 这项工作将学习理论中的可泛化性与几何测度理论中的空间维数紧密联系起来，确立了在 X 的结构性质与分类器一致性之间的直接联系，验证了之前的猜想并修正了错误陈述，为后续在 metric spaces 的学习问题与几何分析之间的交叉研究提供了稳固基石。

Abstract: We prove the last remaining implication allowing to claim the equivalence of the following conditions for a complete separable metric space $X$:
  (1) The $k$-nearest neighbour classifier is (weakly) universally consistent in $X$, (2) The strong Lebesgue--Besicovitch differentiation property holds in $X$ for every locally finite Borel measure, (3) $X$ is sigma-finite dimensional in the sense of Nagata.
  The equivalence (2)$\iff$(3) was announced by Preiss (1983), while a detailed proof of the implication (3)$\Rightarrow$(2) has appeared in Assouad and Quentin de Gromard (2006). The implication (2)$\Rightarrow$(1) was established by Cérou and Guyader (2006). We prove the implication (1)$\Rightarrow$(3). The result was conjectured in the first article in the series (Collins, Kumari, Pestov 2020), and here we also correct a wrong claim made in the second article (Kumari and Pestov 2024).

</details>


### [21] [Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation](https://arxiv.org/abs/2512.17073)
*Zhenyu Liu,Yunzhen Liu,Zehao Fan,Garrett Gagnon,Yayue Hou,Nan Wu,Yangwook Kang,Liu Liu*

Main category: cs.LG

TL;DR: 在带宽受限的MoE推理中，通过低秩补偿实现路由引导的精度恢复，按 Top-n 专家传输低秩因子并应用补偿，以提升带宽-精度权衡和吞吐量。


<details>
  <summary>Details</summary>
Motivation: MoE 通过稀疏激活扩展容量，但对显存和带宽压力很大；离线加载（offloading）按需加载专家会导致 token 级不规则传输，成为 I/O 瓶颈；静态均量化在忽略专家异质性时会显著损失精度。

Method: 提出 Bandwidth-Efficient Adaptive MoE via Low-Rank Compensation：在推理时使用预计算的低秩补偿实现路由指导下的精度恢复；仅传输每个 token 的 Top-n（n<k）个专家的紧凑低秩因子，并对它们应用补偿，其他专家保持低比特；与 GPU offloading 和 GPU-NDP 系统集成，获得更好的带宽-精度权衡和吞吐量。

Result: 实现了更优的带宽与精度权衡，并提高了推理吞吐量。

Conclusion: 通过在跨专家的低秩结构和按需传输中引入路由指导的补偿，该方法缓解了 I/O 瓶颈，适用于大规模 MoE 推理加速。

Abstract: Mixture-of-Experts (MoE) models scale capacity via sparse activation but stress memory and bandwidth. Offloading alleviates GPU memory by fetching experts on demand, yet token-level routing causes irregular transfers that make inference I/O-bound. Static uniform quantization reduces traffic but degrades accuracy under aggressive compression by ignoring expert heterogeneity. We present Bandwidth-Efficient Adaptive Mixture-of-Experts via Low-Rank Compensation, which performs router-guided precision restoration using precomputed low-rank compensators. At inference time, our method transfers compact low-rank factors with Top-n (n<k) experts per token and applies compensation to them, keeping others low-bit. Integrated with offloading on GPU and GPU-NDP systems, our method delivers a superior bandwidth-accuracy trade-off and improved throughput.

</details>


### [22] [Smoothing DiLoCo with Primal Averaging for Faster Training of LLMs](https://arxiv.org/abs/2512.17131)
*Aaron Defazio,Konstantin Mishchenko,Parameswaran Raman,Hao-Jun Michael Shi,Lin Xiao*

Main category: cs.LG

TL;DR: Generalized Primal Averaging (GPA) decouples the interpolation constant in Nesterov's primal averaging, enabling stepwise averaging without a two-loop structure, improving over DiLoCo and Schedule-Free (SF) with lower memory and simpler hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Existing averaging-based optimizers (Schedule-Free and single-worker DiLoCo) improve base optimizers but in non-distributed settings suffer from memory, hyperparameter complexity, and a two-loop structure. There is a need for a single-loop, memory-efficient, tunable averaging scheme that preserves convergence guarantees.

Method: GPA introduces a decoupled interpolation constant in the primal averaging formulation of Nesterov, enabling continuous averaging of iterates at every step. It generalizes single-worker DiLoCo by removing its two-loop structure and reducing memory usage to one additional buffer. The approach yields convergence guarantees: for any base optimizer with regret O(√T), GPA can match or exceed its convergence depending on interpolation constants.

Result: Empirically, GPA yields 24.22% faster steps than AdamW on Llama-160M to reach the baseline validation loss, and 12% (small batch) / 27% (large batch) speedups on ImageNet ViT to reach AdamW validation accuracy. Theoretically, GPA can match or surpass the base optimizer's convergence bound O(√T) depending on interpolation choices.

Conclusion: GPA provides a simple, memory-efficient, single-loop generalized averaging framework that improves performance over prior averaging-based optimizers while preserving (and potentially improving) convergence guarantees. The results support its practicality on large language and vision tasks.

Abstract: We propose Generalized Primal Averaging (GPA), an extension of Nesterov's method in its primal averaging formulation that addresses key limitations of recent averaging-based optimizers such as single-worker DiLoCo and Schedule-Free (SF) in the non-distributed setting. These two recent algorithmic approaches improve the performance of base optimizers, such as AdamW, through different iterate averaging strategies. Schedule-Free explicitly maintains a uniform average of past weights, while single-worker DiLoCo performs implicit averaging by periodically aggregating trajectories, called pseudo-gradients, to update the model parameters. However, single-worker DiLoCo's periodic averaging introduces a two-loop structure, increasing its memory requirements and number of hyperparameters. GPA overcomes these limitations by decoupling the interpolation constant in the primal averaging formulation of Nesterov. This decoupling enables GPA to smoothly average iterates at every step, generalizing and improving upon single-worker DiLoCo. Empirically, GPA consistently outperforms single-worker DiLoCo while removing the two-loop structure, simplifying hyperparameter tuning, and reducing its memory overhead to a single additional buffer. On the Llama-160M model, GPA provides a 24.22% speedup in terms of steps to reach the baseline (AdamW's) validation loss. Likewise, GPA achieves speedups of 12% and 27% on small and large batch setups, respectively, to attain AdamW's validation accuracy on the ImageNet ViT workload. Furthermore, we prove that for any base optimizer with regret bounded by $O(\sqrt{T})$, where $T$ is the number of iterations, GPA can match or exceed the convergence guarantee of the original optimizer, depending on the choice of interpolation constants.

</details>


### [23] [Can Large Reasoning Models Improve Accuracy on Mathematical Tasks Using Flawed Thinking?](https://arxiv.org/abs/2512.17079)
*Saraswathy Amjith,Mihika Dusad,Neha Muramalla,Shweta Shah*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Chain-of-thought (CoT) prompting has become central to mathematical reasoning in large language models, yet models remain brittle to early errors: a single arithmetic slip or unjustified inference typically propagates uncorrected to an incorrect final answer. We investigate whether training on intentionally flawed reasoning traces can teach models to detect and recover from such errors without degrading standard problem-solving ability. Using competition-level problems from MATH-lighteval, we generate CoT prefixes containing exactly one controlled error, either a calculation error (sign flips, dropped terms) or a reasoning error (misapplied rules, unjustified logical steps), and fine-tune Qwen3-4B with GRPO using a binary final-answer reward. Our Mixed-CoT-RL model matches standard RL on clean problems (41% vs 41%) while substantially outperforming it on problems prefilled with flawed reasoning (24% vs 19%). Notably, clean-only RL fine-tuning degrades robustness below the untuned baseline 19% vs. 20%), indicating that conventional training increases susceptibility to misleading prefills. Among error types, training on reasoning errors yields greater robustness gains than calculation errors alone, with mixed training performing best. These findings demonstrate that exposure to flawed traces during training can improve error-recovery behavior without sacrificing accuracy, suggesting a path toward more robust mathematical reasoning in LLMs.

</details>


### [24] [meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis](https://arxiv.org/abs/2512.17409)
*Dishantkumar Sutariya,Eike Petersen*

Main category: cs.LG

TL;DR: 提出一个统计工具箱，用于在不同分组（患者属性、记录属性）下对医学影像模型的性能进行统计学上严谨的评估与子组发现，配套两个案例研究。


<details>
  <summary>Details</summary>
Motivation: 分析模型在不同人群和记录条件下的性能差异，是识别模型失效模式的关键。但在分组比较中，如何在不同样本量和基线率下进行有效比较、估计不确定性、纠正多重比较，以及在组合子组空间中找出“有趣”的子组，仍然具有较高的统计学挑战性。本研究提出一个通用且可用于医学影像的统计工具箱，旨在提供可重复、可解释的子组性能分析方法。

Method: 该工具箱包含：1) 适用于不同样本量和基线率的性能度量选择；2) 不确定性估计与显著性检验；3) 对多重比较的纠正（如纠错策略）；4) 针对交叉分组分析的高效子组发现机制，能在组合庞大的子组中筛选出潜在感兴趣的子组；5) 针对医学影像数据的适用性设计与实现。

Result: 通过两项案例研究演示：在 ISIC2020 数据集的皮肤病变恶性分类和 MIMIC-CXR 数据集的胸片疾病分类中，工具箱能够系统化地揭示子组层面的性能差异及不确定性，为潜在的失效模式提供统计证据。

Conclusion: 该工具箱提供了一个通用且强大的框架，帮助研究者对医学影像模型在子组层面的性能差异进行规范、可重复且统计学上合理的评估，具有广泛应用前景。

Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.

</details>


### [25] [A Systems-Theoretic View on the Convergence of Algorithms under Disturbances](https://arxiv.org/abs/2512.17598)
*Guner Dilsad Er,Sebastian Trimpe,Michael Muehlebach*

Main category: cs.LG

TL;DR: 通过逆Lyapunov定理，提出一个在存在扰动、噪声和系统耦合时的算法收敛性统一分析框架，给出稳定性界与收敛速率，并展示在分布式学习通信约束、机器学习泛化敏感性及隐私噪声注入等场景的应用。


<details>
  <summary>Details</summary>
Motivation: 现实系统中的算法常受扰动、噪声和与其他动态系统的耦合影响，孤立情形的收敛性难以直接推广。需要一个普适工具来度量扰动对算法性能的影响并给出界限。

Method: 基于对偶Lyapunov定理(converse Lyapunov) 构建不变量、导出关于扰动的关键不等式，推导稳定性边界和收敛速率；将理论框架推广到含扰动的情形，并给出可用于具体应用的量化步骤。

Result: 得到在扰动存在时的稳定性界与收敛速率，且该框架可作为分析工具，覆盖包括分布式学习的通信约束、ML泛化的敏感性评估，以及用于隐私保护的噪声注入等场景。

Conclusion: 该结果提供一个通用、连通的分析工具，帮助设计和评估在噪声、扰动和系统耦合下的算法，并在多种应用中统一分析框架。

Abstract: Algorithms increasingly operate within complex physical, social, and engineering systems where they are exposed to disturbances, noise, and interconnections with other dynamical systems. This article extends known convergence guarantees of an algorithm operating in isolation (i.e., without disturbances) and systematically derives stability bounds and convergence rates in the presence of such disturbances. By leveraging converse Lyapunov theorems, we derive key inequalities that quantify the impact of disturbances. We further demonstrate how our result can be utilized to assess the effects of disturbances on algorithmic performance in a wide variety of applications, including communication constraints in distributed learning, sensitivity in machine learning generalization, and intentional noise injection for privacy. This underpins the role of our result as a unifying tool for algorithm analysis in the presence of noise, disturbances, and interconnections with other dynamical systems.

</details>


### [26] [Learning to Plan, Planning to Learn: Adaptive Hierarchical RL-MPC for Sample-Efficient Decision Making](https://arxiv.org/abs/2512.17091)
*Toshiaki Hori,Jonathan DeCastro,Deepak Gopinath,Avinash Balachandran,Guy Rosman*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a new approach for solving planning problems with a hierarchical structure, fusing reinforcement learning and MPC planning. Our formulation tightly and elegantly couples the two planning paradigms. It leverages reinforcement learning actions to inform the MPPI sampler, and adaptively aggregates MPPI samples to inform the value estimation. The resulting adaptive process leverages further MPPI exploration where value estimates are uncertain, and improves training robustness and the overall resulting policies. This results in a robust planning approach that can handle complex planning problems and easily adapts to different applications, as demonstrated over several domains, including race driving, modified Acrobot, and Lunar Lander with added obstacles. Our results in these domains show better data efficiency and overall performance in terms of both rewards and task success, with up to a 72% increase in success rate compared to existing approaches, as well as accelerated convergence (x2.1) compared to non-adaptive sampling.

</details>


### [27] [Convergence Guarantees for Federated SARSA with Local Training and Heterogeneous Agents](https://arxiv.org/abs/2512.17688)
*Paul Mangold,Eloïse Berthier,Eric Moulines*

Main category: cs.LG

TL;DR: FedSARSA with linear function approximation and local updates converges under heterogeneous local transitions and rewards, with the first known sample and communication complexity bounds. It introduces an exact multi-step error expansion for single-agent SARSA, quantifies heterogeneity impact, and achieves linear speed-up in the number of agents (up to higher-order terms from Markovian sampling), with numerical validation.


<details>
  <summary>Details</summary>
Motivation: Federated reinforcement learning with non-IID local data (transitions and rewards) poses challenges for convergence and efficiency. There is a need for theoretical guarantees and complexity bounds for FedSARSA-like algorithms under heterogeneity, and for understanding how local updates affect performance.

Method: Develop an exact multi-step error expansion for single-agent SARSA and use it to analyze FedSARSA with linear function approximation and multiple local updates. Derive convergence guarantees and sample/communication complexity bounds in the presence of heterogeneity in local transitions and rewards. Quantify the impact of heterogeneity and establish linear speed-up with respect to the number of agents, accounting for higher-order terms due to Markovian sampling. Validate findings with numerical experiments.

Result: Provided convergence guarantees for FedSARSA under heterogeneous settings, along with sample and communication complexity bounds. Demonstrated linear speed-up with the number of agents (up to higher-order Markovian terms), and quantified the impact of heterogeneity. Numerical experiments corroborate the theoretical results.

Conclusion: FedSARSA with linear function approximation and local updates is provably efficient in heterogeneous federated RL. The analysis reveals how heterogeneity affects convergence and how system-level speed-ups scale with the number of agents, while acknowledging higher-order effects from Markovian sampling. Empirical results align with theory.

Abstract: We present a novel theoretical analysis of Federated SARSA (FedSARSA) with linear function approximation and local training. We establish convergence guarantees for FedSARSA in the presence of heterogeneity, both in local transitions and rewards, providing the first sample and communication complexity bounds in this setting. At the core of our analysis is a new, exact multi-step error expansion for single-agent SARSA, which is of independent interest. Our analysis precisely quantifies the impact of heterogeneity, demonstrating the convergence of FedSARSA with multiple local updates. Crucially, we show that FedSARSA achieves linear speed-up with respect to the number of agents, up to higher-order terms due to Markovian sampling. Numerical experiments support our theoretical findings.

</details>


### [28] [UniCoMTE: A Universal Counterfactual Framework for Explaining Time-Series Classifiers on ECG Data](https://arxiv.org/abs/2512.17100)
*Justin Li,Efe Sencan,Jasper Zheng Duan,Vitus J. Leung,Stephan Tsaur,Ayse K. Coskun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning models, particularly deep neural networks, have demonstrated strong performance in classifying complex time series data. However, their black-box nature limits trust and adoption, especially in high-stakes domains such as healthcare. To address this challenge, we introduce UniCoMTE, a model-agnostic framework for generating counterfactual explanations for multivariate time series classifiers. The framework identifies temporal features that most heavily influence a model's prediction by modifying the input sample and assessing its impact on the model's prediction. UniCoMTE is compatible with a wide range of model architectures and operates directly on raw time series inputs. In this study, we evaluate UniCoMTE's explanations on a time series ECG classifier. We quantify explanation quality by comparing our explanations' comprehensibility to comprehensibility of established techniques (LIME and SHAP) and assessing their generalizability to similar samples. Furthermore, clinical utility is assessed through a questionnaire completed by medical experts who review counterfactual explanations presented alongside original ECG samples. Results show that our approach produces concise, stable, and human-aligned explanations that outperform existing methods in both clarity and applicability. By linking model predictions to meaningful signal patterns, the framework advances the interpretability of deep learning models for real-world time series applications.

</details>


### [29] [Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting](https://arxiv.org/abs/2512.17696)
*Yuri Calleo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.

</details>


### [30] [Mitigating Forgetting in Low Rank Adaptation](https://arxiv.org/abs/2512.17720)
*Joanna Sliwa,Frank Schneider,Philipp Hennig,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: LaLoRA是在LoRA基础上引入 Laplace 近似的权重空间正则化，用以在提升特定领域适应性的同时减少对原有知识的遗忘。


<details>
  <summary>Details</summary>
Motivation: LoRA 等参数高效微调方法易导致对先验领域知识的灾难性遗忘，需要一种轻量且可控的正则化策略来保留原有知识并兼顾目标领域学习。

Method: 仅对 LoRA 权重应用 Laplace 近似，估计每个参数的置信度，约束在高曲率方向上的更新；正则化项与 LoRA 权重耦合，保持轻量级；并比较不同曲率近似、数据选取与超参数对鲁棒性的影响。

Result: 在 Llama 模型上进行数学推理微调时，显式地改善了学习-遗忘权衡，可通过正则化强度直接控制；展示对不同曲率估算、数据和超参的鲁棒性分析。

Conclusion: 将 Laplace 近似应用于 LoRA 权重的 LaLoRA 是一种有效的参数高效微调策略，能够在降低灾难性遗忘的同时实现可控的目标领域再学习，为进一步探索曲率估算和数据选择提供方向。

Abstract: Parameter-efficient fine-tuning methods, such as Low-Rank Adaptation (LoRA), enable fast specialization of large pre-trained models to different downstream applications. However, this process often leads to catastrophic forgetting of the model's prior domain knowledge. We address this issue with LaLoRA, a weight-space regularization technique that applies a Laplace approximation to Low-Rank Adaptation. Our approach estimates the model's confidence in each parameter and constrains updates in high-curvature directions, preserving prior knowledge while enabling efficient target-domain learning. By applying the Laplace approximation only to the LoRA weights, the method remains lightweight. We evaluate LaLoRA by fine-tuning a Llama model for mathematical reasoning and demonstrate an improved learning-forgetting trade-off, which can be directly controlled via the method's regularization strength. We further explore different loss landscape curvature approximations for estimating parameter confidence, analyze the effect of the data used for the Laplace approximation, and study robustness across hyperparameters.

</details>


### [31] [Atom: Efficient On-Device Video-Language Pipelines Through Modular Reuse](https://arxiv.org/abs/2512.17108)
*Kunjal Panchal,Saayan Mitra,Somdeb Sarkhel,Haoliang Wang,Ishita Dasgupta,Gang Wu,Hui Guan*

Main category: cs.LG

TL;DR: 在移动设备上通过模块重用实现对视频-语言任务的高效端到端执行，Atom 将大规模模型拆分为可重用模块并跨子任务复用以降低延迟，性能损失很小。


<details>
  <summary>Details</summary>
Motivation: 移动端视频语言管线存在重复加载和碎片化执行，导致高延迟和能耗。

Method: 将亿级参数模型拆分为可重用模块（视觉编码器、语言解码器等），跨字幕、推理、索引等子任务复用，跨子任务实现并行执行，避免重复加载；在常规智能手机上进行评估。

Result: 在普通智能手机上，Atom 相对非重用基线提高 27-33% 的端到端速度；在检索任务、字幕生成方面的性能下降有限（Recall@1 下降不超过约 2.3 点，CIDEr 下降不超过约 1.5 点）。

Conclusion: 提出一个可实际部署、可扩展的端到端视频语言理解解决方案，适用于边缘设备。

Abstract: Recent advances in video-language models have enabled powerful applications like video retrieval, captioning, and assembly. However, executing such multi-stage pipelines efficiently on mobile devices remains challenging due to redundant model loads and fragmented execution. We introduce Atom, an on-device system that restructures video-language pipelines for fast and efficient execution. Atom decomposes a billion-parameter model into reusable modules, such as the visual encoder and language decoder, and reuses them across subtasks like captioning, reasoning, and indexing. This reuse-centric design eliminates repeated model loading and enables parallel execution, reducing end-to-end latency without sacrificing performance. On commodity smartphones, Atom achieves 27--33% faster execution compared to non-reuse baselines, with only marginal performance drop ($\leq$ 2.3 Recall@1 in retrieval, $\leq$ 1.5 CIDEr in captioning). These results position Atom as a practical, scalable approach for efficient video-language understanding on edge devices.

</details>


### [32] [Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow](https://arxiv.org/abs/2512.17878)
*Herlock Rahimi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.

</details>


### [33] [Bridging Training and Merging Through Momentum-Aware Optimization](https://arxiv.org/abs/2512.17109)
*Alireza Moayedikia,Alicia Troncoso*

Main category: cs.LG

TL;DR: 提出一个统一框架，在训练过程中保持分解的动量与曲率统计，并在模型合并时重用这些信息，实现曲率感知的模型合并，具备内存效率、收敛性保证及在NLP基准上的实验优势。


<details>
  <summary>Details</summary>
Motivation: 现有工作在训练时计算曲率、在合并时重新计算，导致重复计算且丢失可重复轨迹信息，需要一个在训练阶段就积累并可用于合并的统一框架。

Method: 在训练中维持因式分解的动量与曲率统计，并将其用于几何感知的模型合并；通过任务重要性分数实现曲率感知的参数选择，避免后验Fisher计算；给出非凸目标下的收敛性保证，误差由梯度奇异值衰减来界定；在自然语言理解任务上验证曲率感知的参数选择优于幅度基线，并且多任务合并优于强基线；框架具备秩无关的收敛性与对超参数的鲁棒性。

Result: 实现的内存效率与最先进方法相当；曲率感知的参数选择在所有稀疏度级别优于仅幅度基线；多任务合并在性能上优于强基线；理论上对非凸目标有收敛保证，误差受梯度奇异值衰减约束。

Conclusion: 将优化轨迹视为可重复使用的资产，消除了冗余计算，从而实现更 principled 的模型组合。

Abstract: Training large neural networks and merging task-specific models both exploit low-rank structure and require parameter importance estimation, yet these challenges have been pursued in isolation. Current workflows compute curvature information during training, discard it, then recompute similar information for merging -- wasting computation and discarding valuable trajectory data. We introduce a unified framework that maintains factorized momentum and curvature statistics during training, then reuses this information for geometry-aware model composition. The proposed method achieves memory efficiency comparable to state-of-the-art approaches while accumulating task saliency scores that enable curvature-aware merging without post-hoc Fisher computation. We establish convergence guarantees for non-convex objectives with approximation error bounded by gradient singular value decay. On natural language understanding benchmarks, curvature-aware parameter selection outperforms magnitude-only baselines across all sparsity levels, with multi-task merging improving over strong baselines. The proposed framework exhibits rank-invariant convergence and superior hyperparameter robustness compared to existing low-rank optimizers. By treating the optimization trajectory as a reusable asset rather than discarding it, our approach eliminates redundant computation while enabling more principled model composition.

</details>


### [34] [Digitizing Nepal's Written Heritage: A Comprehensive HTR Pipeline for Old Nepali Manuscripts](https://arxiv.org/abs/2512.17111)
*Anjali Sarawgi,Esteban Garces Arias,Christof Zotter*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents the first end-to-end pipeline for Handwritten Text Recognition (HTR) for Old Nepali, a historically significant but low-resource language. We adopt a line-level transcription approach and systematically explore encoder-decoder architectures and data-centric techniques to improve recognition accuracy. Our best model achieves a Character Error Rate (CER) of 4.9\%. In addition, we implement and evaluate decoding strategies and analyze token-level confusions to better understand model behaviour and error patterns. While the dataset we used for evaluation is confidential, we release our training code, model configurations, and evaluation scripts to support further research in HTR for low-resource historical scripts.

</details>


### [35] [The Effect of Negation on CLIP in Medical Imaging: Limitations of Contrastive Language-Image Pretraining](https://arxiv.org/abs/2512.17121)
*Jasmine Vu,Shivanand Sheshappanavar*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large vision-language models like CLIP are increasingly used in medical imaging tasks due to their ability to align images and text without the need for extensive labeled data. This makes them particularly useful for applications like image retrieval, report generation, and classification in clinical settings. A potential issue to this approach is that CLIP-based models often under perform when interpreting negated phrases, which is especially problematic in the context of medical diagnosing. In this study, we evaluate the Stanford AIMI CheXagent model on its ability to correctly retrieve chest X-ray images using prompts with and without negation. The goal of this project is to understand where this model fails and then use it as a base model to improve its retrieval accuracy by fine tuning methods outlined in previous work. Results from this study show improvement in handling of negation in the CLIP model with a slight decrease in accuracy of positive prompt evaluation. Alongside retrieval accuracy, we examined internal model behavior through token attribution, t-SNE projection, and attention-head ablation to better characterize how each fine tuning approach reshaped the text encoders representation of negated clinical language. Through this work, we hope to better understand the internal behavior of CLIP and improve its handling of negation using clinically relevant language for improving its reliability in medical AI devices.

</details>


### [36] [DiffeoMorph: Learning to Morph 3D Shapes Using Differentiable Agent-Based Simulations](https://arxiv.org/abs/2512.17129)
*Seong Ho Pahng,Guoye Guan,Benjamin Fefferman,Sahand Hormoz*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Biological systems can form complex three-dimensional structures through the collective behavior of identical agents -- cells that follow the same internal rules and communicate without central control. How such distributed control gives rise to precise global patterns remains a central question not only in developmental biology but also in distributed robotics, programmable matter, and multi-agent learning. Here, we introduce DiffeoMorph, an end-to-end differentiable framework for learning a morphogenesis protocol that guides a population of agents to morph into a target 3D shape. Each agent updates its position and internal state using an attention-based SE(3)-equivariant graph neural network, based on its own internal state and signals received from other agents. To train this system, we introduce a new shape-matching loss based on the 3D Zernike polynomials, which compares the predicted and target shapes as continuous spatial distributions, not as discrete point clouds, and is invariant to agent ordering, number of agents, and rigid-body transformations. To enforce full SO(3) invariance -- invariant to rotations yet sensitive to reflections, we include an alignment step that optimally rotates the predicted Zernike spectrum to match the target before computing the loss. This results in a bilevel problem, with the inner loop optimizing a unit quaternion for the best alignment and the outer loop updating the agent model. We compute gradients through the alignment step using implicit differentiation. We perform systematic benchmarking to establish the advantages of our shape-matching loss over other standard distance metrics for shape comparison tasks. We then demonstrate that DiffeoMorph can form a range of shapes -- from simple ellipsoids to complex morphologies -- using only minimal spatial cues.

</details>


### [37] [Electric Vehicle Charging Load Forecasting: An Experimental Comparison of Machine Learning Methods](https://arxiv.org/abs/2512.17257)
*Iason Kyriakopoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the growing popularity of electric vehicles as a means of addressing climate change, concerns have emerged regarding their impact on electric grid management. As a result, predicting EV charging demand has become a timely and important research problem. While substantial research has addressed energy load forecasting in transportation, relatively few studies systematically compare multiple forecasting methods across different temporal horizons and spatial aggregation levels in diverse urban settings. This work investigates the effectiveness of five time series forecasting models, ranging from traditional statistical approaches to machine learning and deep learning methods. Forecasting performance is evaluated for short-, mid-, and long-term horizons (on the order of minutes, hours, and days, respectively), and across spatial scales ranging from individual charging stations to regional and city-level aggregations. The analysis is conducted on four publicly available real-world datasets, with results reported independently for each dataset. To the best of our knowledge, this is the first work to systematically evaluate EV charging demand forecasting across such a wide range of temporal horizons and spatial aggregation levels using multiple real-world datasets.

</details>


### [38] [SHARP-QoS: Sparsely-gated Hierarchical Adaptive Routing for joint Prediction of QoS](https://arxiv.org/abs/2512.17262)
*Suraj Kumar,Arvind Kumar,Soumi Chattopadhyay*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dependable service-oriented computing relies on multiple Quality of Service (QoS) parameters that are essential to assess service optimality. However, real-world QoS data are extremely sparse, noisy, and shaped by hierarchical dependencies arising from QoS interactions, and geographical and network-level factors, making accurate QoS prediction challenging. Existing methods often predict each QoS parameter separately, requiring multiple similar models, which increases computational cost and leads to poor generalization. Although recent joint QoS prediction studies have explored shared architectures, they suffer from negative transfer due to loss-scaling caused by inconsistent numerical ranges across QoS parameters and further struggle with inadequate representation learning, resulting in degraded accuracy. This paper presents an unified strategy for joint QoS prediction, called SHARP-QoS, that addresses these issues using three components. First, we introduce a dual mechanism to extract the hierarchical features from both QoS and contextual structures via hyperbolic convolution formulated in the Poincaré ball. Second, we propose an adaptive feature-sharing mechanism that allows feature exchange across informative QoS and contextual signals. A gated feature fusion module is employed to support dynamic feature selection among structural and shared representations. Third, we design an EMA-based loss balancing strategy that allows stable joint optimization, thereby mitigating the negative transfer. Evaluations on three datasets with two, three, and four QoS parameters demonstrate that SHARP-QoS outperforms both single- and multi-task baselines. Extensive study shows that our model effectively addresses major challenges, including sparsity, robustness to outliers, and cold-start, while maintaining moderate computational overhead, underscoring its capability for reliable joint QoS prediction.

</details>


### [39] [Understanding Generalization in Role-Playing Models via Information Theory](https://arxiv.org/abs/2512.17270)
*Yongqi Li,Hao Lang,Fei Huang,Tieyun Qian,Yongbin Li*

Main category: cs.LG

TL;DR: 提出 R-EMID 指标来量化 RPM 在用户/角色/对话分布偏移下的性能下降，并给出上界预测最坏情形；提出协同进化强化学习框架，以改进对话生成概率估计，从而更准确计算 R-EMID。结果显示用户偏移最具风险，RL 最有效提升泛化。


<details>
  <summary>Details</summary>
Motivation: 现实部署中分布偏移导致 RPM 泛化性能下降，缺乏可解释的、形式化的诊断框架来分析不同偏移对泛化的影响。

Method: 提出信息理论度量：R-EMID（reasoning-based effective mutual information difference），推导其上界以预测最坏泛化；提出协同进化强化学习框架以建模用户、角色与对话上下文之间的连接，提升对话生成概率估计以计算 R-EMID。

Result: 在多种 RPM 设置下评估 R-EMID，发现用户偏移风险最高；引入的强化学习方法显著提升 RPM 的泛化表现。

Conclusion: R-EMID 提供可解释且可预测的 RPM 泛化框架，揭示不同分布偏移的贡献，协同进化 RL 能有效提升对话生成概率估计，从而改善泛化。

Abstract: Role-playing models (RPMs) are widely used in real-world applications but underperform when deployed in the wild. This degradation can be attributed to distribution shifts, including user, character, and dialogue compositional shifts. Existing methods like LLM-as-a-judge fall short in providing a fine-grained diagnosis of how these shifts affect RPM generalization, and thus there lack formal frameworks to characterize RPM generalization behaviors. To bridge these gaps, we introduce an information-theoretic metric, named reasoning-based effective mutual information difference (R-EMID), to measure RPM performance degradation in an interpretable way. We also derive an upper bound on R-EMID to predict the worst-case generalization performance of RPMs and theoretically reveal how various shifts contribute to the RPM performance degradation. Moreover, we propose a co-evolving reinforcement learning framework to adaptively model the connection among user, character, and dialogue context and thus enhance the estimation of dialogue response generation probability, which is critical for calculating R-EMID. Finally, we evaluate the generalization performance of various RPMs using R-EMID, finding that user shift poses the highest risk among all shifts and reinforcement learning is the most effective approach for enhancing RPM generalization.

</details>


### [40] [Alzheimer's Disease Brain Network Mining](https://arxiv.org/abs/2512.17276)
*Alireza Moayedikia,Sara Fin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning approaches for Alzheimer's disease (AD) diagnosis face a fundamental challenges. Clinical assessments are expensive and invasive, leaving ground truth labels available for only a fraction of neuroimaging datasets. We introduce Multi view Adaptive Transport Clustering for Heterogeneous Alzheimer's Disease (MATCH-AD), a semi supervised framework that integrates deep representation learning, graph-based label propagation, and optimal transport theory to address this limitation. The framework leverages manifold structure in neuroimaging data to propagate diagnostic information from limited labeled samples to larger unlabeled populations, while using Wasserstein distances to quantify disease progression between cognitive states. Evaluated on nearly five thousand subjects from the National Alzheimer's Coordinating Center, encompassing structural MRI measurements from hundreds of brain regions, cerebrospinal fluid biomarkers, and clinical variables MATCHAD achieves near-perfect diagnostic accuracy despite ground truth labels for less than one-third of subjects. The framework substantially outperforms all baseline methods, achieving kappa indicating almost perfect agreement compared to weak agreement for the best baseline, a qualitative transformation in diagnostic reliability. Performance remains clinically useful even under severe label scarcity, and we provide theoretical convergence guarantees with proven bounds on label propagation error and transport stability. These results demonstrate that principled semi-supervised learning can unlock the diagnostic potential of the vast repositories of partially annotated neuroimaging data accumulating worldwide, substantially reducing annotation burden while maintaining accuracy suitable for clinical deployment.

</details>


### [41] [M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge](https://arxiv.org/abs/2512.17299)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.

</details>


### [42] [Explanation Beyond Intuition: A Testable Criterion for Inherent Explainability](https://arxiv.org/abs/2512.17316)
*Michael Merry,Pat Riddle,Jim Warren*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Inherent explainability is the gold standard in Explainable Artificial Intelligence (XAI). However, there is not a consistent definition or test to demonstrate inherent explainability. Work to date either characterises explainability through metrics, or appeals to intuition - "we know it when we see it". We propose a globally applicable criterion for inherent explainability. The criterion uses graph theory for representing and decomposing models for structure-local explanation, and recomposing them into global explanations. We form the structure-local explanations as annotations, a verifiable hypothesis-evidence structure that allows for a range of explanatory methods to be used. This criterion matches existing intuitions on inherent explainability, and provides justifications why a large regression model may not be explainable but a sparse neural network could be. We differentiate explainable -- a model that allows for explanation -- and \textit{explained} -- one that has a verified explanation. Finally, we provide a full explanation of PREDICT -- a Cox proportional hazards model of cardiovascular disease risk, which is in active clinical use in New Zealand. It follows that PREDICT is inherently explainable. This work provides structure to formalise other work on explainability, and allows regulators a flexible but rigorous test that can be used in compliance frameworks.

</details>


### [43] [Task Schema and Binding: A Double Dissociation Study of In-Context Learning](https://arxiv.org/abs/2512.17325)
*Chaeha Kim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:
  1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms
  2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)
  3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba
  These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.

</details>


### [44] [Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs](https://arxiv.org/abs/2512.17352)
*Ivan Kralj,Lodovico Giaretta,Gordan Ježić,Ivana Podnar Žarko,Šarūnas Girdzijauskas*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.

</details>


### [45] [Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach](https://arxiv.org/abs/2512.17367)
*Yidong Chai,Yi Liu,Mohammadreza Ebrahimi,Weifeng Li,Balaji Padmanabhan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

</details>


### [46] [AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens](https://arxiv.org/abs/2512.17375)
*Tung-Ling Li,Yuhao Wu,Hongliang Liu*

Main category: cs.LG

TL;DR: 提出 AdvJudge-Zero 攻击框架，利用短序列低困惑度控制令牌翻转二值评估；通过 next-token 分布 + 束搜索发现多样控制序列，攻击集中在低秩软模式，干扰评估器的拒绝方向；在大模型与专门化评估模型上造成高假阳性率；可通过 LoRA 对抗训练显著降低误判，同时保持评估质量。


<details>
  <summary>Details</summary>
Motivation: 在 RLHF、DPO、RLAIF 等后训练管线中，奖励模型/评判系统为模型选择与微调提供标量反馈与二元决策，但其鲁棒性不足，易被现实的控制令牌序列误导，从而产生对评估的鲁棒性风险与对抗性挑战。

Method: 提出 AdvJudge-Zero：基于评估模型的下一个词分布与束搜索，零样本地发现多样化的控制令牌序列；对产生的扰动进行分析，发现影响集中在一个低秩的软模态，与评估器的拒绝方向呈反向关系；在公开权重和专业化评估模型上测试攻击效果。

Result: 控制令牌可将大量正确的“No”评估误判为“是”，导致极高的假阳性率，尤其在数学与推理基准上；LoRA 基于对抗训练的小数据集能显著降低这些假阳性，同时保持评估质量。

Conclusion: 揭示奖励模型/判定系统的现实世界鲁棒性风险及其可观测的对抗性漏洞；提出基于 LoRA 的对抗训练作为缓解策略，但仍需在更广泛的评估场景中验证鲁棒性与可扩展性。

Abstract: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.

</details>


### [47] [Assessing Long-Term Electricity Market Design for Ambitious Decarbonization Targets using Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2512.17444)
*Javier Gonzalez-Ruiz,Carlos Rodriguez-Pardo,Iacopo Savelli,Alice Di Bella,Massimo Tavoni*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Electricity systems are key to transforming today's society into a carbon-free economy. Long-term electricity market mechanisms, including auctions, support schemes, and other policy instruments, are critical in shaping the electricity generation mix. In light of the need for more advanced tools to support policymakers and other stakeholders in designing, testing, and evaluating long-term markets, this work presents a multi-agent reinforcement learning model capable of capturing the key features of decarbonizing energy systems. Profit-maximizing generation companies make investment decisions in the wholesale electricity market, responding to system needs, competitive dynamics, and policy signals. The model employs independent proximal policy optimization, which was selected for suitability to the decentralized and competitive environment. Nevertheless, given the inherent challenges of independent learning in multi-agent settings, an extensive hyperparameter search ensures that decentralized training yields market outcomes consistent with competitive behavior. The model is applied to a stylized version of the Italian electricity system and tested under varying levels of competition, market designs, and policy scenarios. Results highlight the critical role of market design for decarbonizing the electricity sector and avoiding price volatility. The proposed framework allows assessing long-term electricity markets in which multiple policy and market mechanisms interact simultaneously, with market participants responding and adapting to decarbonization pathways.

</details>


### [48] [Learning What to Write: Write-Gated KV for Efficient Long-Context Inference](https://arxiv.org/abs/2512.17452)
*Yen-Chieh Huang,Rui Fang,Ming-Syan Chen,Pi-Cheng Hsiu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

</details>


### [49] [A lightweight Spatial-Temporal Graph Neural Network for Long-term Time Series Forecasting](https://arxiv.org/abs/2512.17453)
*Henok Tenaw Moges,Deshendran Moodley*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose Lite-STGNN, a lightweight spatial-temporal graph neural network for long-term multivariate forecasting that integrates decomposition-based temporal modeling with learnable sparse graph structure. The temporal module applies trend-seasonal decomposition, while the spatial module performs message passing with low-rank Top-$K$ adjacency learning and conservative horizon-wise gating, enabling spatial corrections that enhance a strong linear baseline. Lite-STGNN achieves state-of-the-art accuracy on four benchmark datasets for horizons up to 720 steps, while being parameter-efficient and substantially faster to train than transformer-based methods. Ablation studies show that the spatial module yields 4.6% improvement over the temporal baseline, Top-$K$ enhances locality by 3.3%, and learned adjacency matrices reveal domain-specific interaction dynamics. Lite-STGNN thus offers a compact, interpretable, and efficient framework for long-term multivariate time series forecasting.

</details>


### [50] [Deep Learning-Based Surrogate Creep Modelling in Inconel 625: A High-Temperature Alloy Study](https://arxiv.org/abs/2512.17477)
*Shubham Das,Kaushal Singhania,Amit Sadhu,Suprabhat Das,Arghya Nandi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time-dependent deformation, particularly creep, in high-temperature alloys such as Inconel 625 is a key factor in the long-term reliability of components used in aerospace and energy systems. Although Inconel 625 shows excellent creep resistance, finite-element creep simulations in tools such as ANSYS remain computationally expensive, often requiring tens of minutes for a single 10,000-hour run. This work proposes deep learning based surrogate models to provide fast and accurate replacements for such simulations. Creep strain data was generated in ANSYS using the Norton law under uniaxial stresses of 50 to 150 MPa and temperatures of 700 to 1000 $^\circ$C, and this temporal dataset was used to train two architectures: a BiLSTM Variational Autoencoder for uncertainty-aware and generative predictions, and a BiLSTM Transformer hybrid that employs self-attention to capture long-range temporal behavior. Both models act as surrogate predictors, with the BiLSTM-VAE offering probabilistic output and the BiLSTM-Transformer delivering high deterministic accuracy. Performance is evaluated using RMSE, MAE, and $R^2$. Results show that the BiLSTM-VAE provides stable and reliable creep strain forecasts, while the BiLSTM-Transformer achieves strong accuracy across the full time range. Latency tests indicate substantial speedup: while each ANSYS simulation requires 30 to 40 minutes for a given stress-temperature condition, the surrogate models produce predictions within seconds. The proposed framework enables rapid creep assessment for design optimization and structural health monitoring, and provides a scalable solution for high-temperature alloy applications.

</details>


### [51] [SafeBench-Seq: A Homology-Clustered, CPU-Only Baseline for Protein Hazard Screening with Physicochemical/Composition Features and Cluster-Aware Confidence Intervals](https://arxiv.org/abs/2512.17527)
*Muhammad Haris Khan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Foundation models for protein design raise concrete biosecurity risks, yet the community lacks a simple, reproducible baseline for sequence-level hazard screening that is explicitly evaluated under homology control and runs on commodity CPUs. We introduce SafeBench-Seq, a metadata-only, reproducible benchmark and baseline classifier built entirely from public data (SafeProtein hazards and UniProt benigns) and interpretable features (global physicochemical descriptors and amino-acid composition). To approximate "never-before-seen" threats, we homology-cluster the combined dataset at <=40% identity and perform cluster-level holdouts (no cluster overlap between train/test). We report discrimination (AUROC/AUPRC) and screening-operating points (TPR@1% FPR; FPR@95% TPR) with 95% bootstrap confidence intervals (n=200), and we provide calibrated probabilities via CalibratedClassifierCV (isotonic for Logistic Regression / Random Forest; Platt sigmoid for Linear SVM). We quantify probability quality using Brier score, Expected Calibration Error (ECE; 15 bins), and reliability diagrams. Shortcut susceptibility is probed via composition-preserving residue shuffles and length-/composition-only ablations. Empirically, random splits substantially overestimate robustness relative to homology-clustered evaluation; calibrated linear models exhibit comparatively good calibration, while tree ensembles retain slightly higher Brier/ECE. SafeBench-Seq is CPU-only, reproducible, and releases metadata only (accessions, cluster IDs, split labels), enabling rigorous evaluation without distributing hazardous sequences.

</details>


### [52] [Bayesian Optimisation: Which Constraints Matter?](https://arxiv.org/abs/2512.17569)
*Xietao Wang Lin,Juan Ungredda,Max Butler,James Town,Alma Rahat,Hemant Singh,Juergen Branke*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Bayesian optimisation has proven to be a powerful tool for expensive global black-box optimisation problems. In this paper, we propose new Bayesian optimisation variants of the popular Knowledge Gradient acquisition functions for problems with \emph{decoupled} black-box constraints, in which subsets of the objective and constraint functions may be evaluated independently. In particular, our methods aim to take into account that often only a handful of the constraints may be binding at the optimum, and hence we should evaluate only relevant constraints when trying to optimise a function. We empirically benchmark these methods against existing methods and demonstrate their superiority over the state-of-the-art.

</details>


### [53] [GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping](https://arxiv.org/abs/2512.17570)
*Yikang Yue,Yishu Yin,Xuehai Qian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

</details>


### [54] [Machine Learning for Static and Single-Event Dynamic Complex Network Analysis](https://arxiv.org/abs/2512.17577)
*Nikolaos Nakis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The primary objective of this thesis is to develop novel algorithmic approaches for Graph Representation Learning of static and single-event dynamic networks. In such a direction, we focus on the family of Latent Space Models, and more specifically on the Latent Distance Model which naturally conveys important network characteristics such as homophily, transitivity, and the balance theory. Furthermore, this thesis aims to create structural-aware network representations, which lead to hierarchical expressions of network structure, community characterization, the identification of extreme profiles in networks, and impact dynamics quantification in temporal networks. Crucially, the methods presented are designed to define unified learning processes, eliminating the need for heuristics and multi-stage processes like post-processing steps. Our aim is to delve into a journey towards unified network embeddings that are both comprehensive and powerful, capable of characterizing network structures and adeptly handling the diverse tasks that graph analysis offers.

</details>


### [55] [Learning Safe Autonomous Driving Policies Using Predictive Safety Representations](https://arxiv.org/abs/2512.17586)
*Mahesh Keswani,Raunak Bhattacharyya*

Main category: cs.LG

TL;DR: SRPL在WOMD和NuPlan上显示出在奖励-安全权衡中显著改进，提升成功率和成本降低，且对观测噪声鲁棒性和零-shot跨数据集泛化有积极作用，但成效依赖于底层策略优化器与数据分布。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶的SafeRL中，安全约束与性能之间存在根本张力。引入对未来约束违规的预测表征有望引导策略在保持安全的同时提高性能。

Method: 在WOMD和NuPlan数据集上，对安全表征的SRPL框架进行系统性实验，比较不同策略优化器的影响，评估在不同数据分布下的效果，并考察鲁棒性与跨数据集泛化。

Result: 成功率的效果量r为0.65–0.86，成本降低的效果量为0.70–0.83，均具有p<0.05的显著性。SRPL提升对观测噪声的鲁棒性，并在零-shot跨数据集评估中表现出较非SRPL方法更好的泛化能力。

Conclusion: 预测性安全表征具备在真实世界SafeRL中加强自动驾驶安全与性能平衡的潜力，但其效果受到底层策略优化器与数据分布的制约。未来研究可聚焦于提升在多场景下的鲁棒性与通用性。

Abstract: Safe reinforcement learning (SafeRL) is a prominent paradigm for autonomous driving, where agents are required to optimize performance under strict safety requirements. This dual objective creates a fundamental tension, as overly conservative policies limit driving efficiency while aggressive exploration risks safety violations. The Safety Representations for Safer Policy Learning (SRPL) framework addresses this challenge by equipping agents with a predictive model of future constraint violations and has shown promise in controlled environments. This paper investigates whether SRPL extends to real-world autonomous driving scenarios. Systematic experiments on the Waymo Open Motion Dataset (WOMD) and NuPlan demonstrate that SRPL can improve the reward-safety tradeoff, achieving statistically significant improvements in success rate (effect sizes r = 0.65-0.86) and cost reduction (effect sizes r = 0.70-0.83), with p < 0.05 for observed improvements. However, its effectiveness depends on the underlying policy optimizer and the dataset distribution. The results further show that predictive safety representations play a critical role in improving robustness to observation noise. Additionally, in zero-shot cross-dataset evaluation, SRPL-augmented agents demonstrate improved generalization compared to non-SRPL methods. These findings collectively demonstrate the potential of predictive safety representations to strengthen SafeRL for autonomous driving.

</details>


### [56] [Sharing Knowledge without Sharing Data: Stitches can improve ensembles of disjointly trained models](https://arxiv.org/abs/2512.17592)
*Arthur Guijt,Dirk Thierens,Ellen Kerkhof,Jan Wiersma,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep learning has been shown to be very capable at performing many real-world tasks. However, this performance is often dependent on the presence of large and varied datasets. In some settings, like in the medical domain, data is often fragmented across parties, and cannot be readily shared. While federated learning addresses this situation, it is a solution that requires synchronicity of parties training a single model together, exchanging information about model weights. We investigate how asynchronous collaboration, where only already trained models are shared (e.g. as part of a publication), affects performance, and propose to use stitching as a method for combining models.
  Through taking a multi-objective perspective, where performance on each parties' data is viewed independently, we find that training solely on a single parties' data results in similar performance when merging with another parties' data, when considering performance on that single parties' data, while performance on other parties' data is notably worse. Moreover, while an ensemble of such individually trained networks generalizes better, performance on each parties' own dataset suffers. We find that combining intermediate representations in individually trained models with a well placed pair of stitching layers allows this performance to recover to a competitive degree while maintaining improved generalization, showing that asynchronous collaboration can yield competitive results.

</details>


### [57] [A Unified Representation of Neural Networks Architectures](https://arxiv.org/abs/2512.17593)
*Christophe Prieur,Mircea Lazar,Bogdan Robu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogeneization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Differences and similarities with neural fields are discussed along with further possible generalizations and applications of the DiPaNet framework.

</details>


### [58] [More Consistent Accuracy PINN via Alternating Easy-Hard Training](https://arxiv.org/abs/2512.17607)
*Zhaoqian Gao,Min Yanga*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Physics-informed neural networks (PINNs) have recently emerged as a prominent paradigm for solving partial differential equations (PDEs), yet their training strategies remain underexplored. While hard prioritization methods inspired by finite element methods are widely adopted, recent research suggests that easy prioritization can also be effective. Nevertheless, we find that both approaches exhibit notable trade-offs and inconsistent performance across PDE types. To address this issue, we develop a hybrid strategy that combines the strengths of hard and easy prioritization through an alternating training algorithm. On PDEs with steep gradients, nonlinearity, and high dimensionality, the proposed method achieves consistently high accuracy, with relative L2 errors mostly in the range of O(10^-5) to O(10^-6), significantly surpassing baseline methods. Moreover, it offers greater reliability across diverse problems, whereas compared approaches often suffer from variable accuracy depending on the PDE. This work provides new insights into designing hybrid training strategies to enhance the performance and robustness of PINNs.

</details>


### [59] [SCOPE: Sequential Causal Optimization of Process Interventions](https://arxiv.org/abs/2512.17629)
*Jakob De Moor,Hans Weytjens,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Prescriptive Process Monitoring (PresPM) recommends interventions during business processes to optimize key performance indicators (KPIs). In realistic settings, interventions are rarely isolated: organizations need to align sequences of interventions to jointly steer the outcome of a case. Existing PresPM approaches fall short in this respect. Many focus on a single intervention decision, while others treat multiple interventions independently, ignoring how they interact over time. Methods that do address these dependencies depend either on simulation or data augmentation to approximate the process to train a Reinforcement Learning (RL) agent, which can create a reality gap and introduce bias. We introduce SCOPE, a PresPM approach that learns aligned sequential intervention recommendations. SCOPE employs backward induction to estimate the effect of each candidate intervention action, propagating its impact from the final decision point back to the first. By leveraging causal learners, our method can utilize observational data directly, unlike methods that require constructing process approximations for reinforcement learning. Experiments on both an existing synthetic dataset and a new semi-synthetic dataset show that SCOPE consistently outperforms state-of-the-art PresPM techniques in optimizing the KPI. The novel semi-synthetic setup, based on a real-life event log, is provided as a reusable benchmark for future work on sequential PresPM.

</details>


### [60] [Trust-Region Adaptive Policy Optimization](https://arxiv.org/abs/2512.17636)
*Mingyu Su,Jian Guan,Yuxian Gu,Minlie Huang,Hongning Wang*

Main category: cs.LG

TL;DR: 一种将 SFT 与 RL 融合的混合训练框架 TRAPO，通过在每个训练样本内交替优化专家前缀的 SFT 损失与模型自我完成的 RL 损失，并引入信任域 SFT 与自适应前缀选择，在五个数学推理基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的两阶段 SFT-再 RL 流程存在不一致：SFT 的模仿压制探索并引发遗忘，限制 RL 的潜在提升。需要一个能同时利用外部监督和自我探索的统一训练框架。

Method: 1) TRAPO 在每个训练样本内交替优化专家前缀的 SFT 损失与模型自我生成完成的 RL 损失，实现外部监督与自我探索的统一；2) 引入 TrSFT：在信任域内最小化前向 KL，在域外削弱优化，逐渐转向逆 KL，获得对 RL 有利的稳定、模式偏向更新；3) 引入自适应前缀选择机制，根据效用度量动态分配专家指导。

Result: 在五个数学推理基准上，TRAPO 持续优于标准 SFT、RL、SFT-then-RL 以及当前的 state-of-the-art 方法。

Conclusion: TRAPO 为增强推理能力的语言模型建立了新的范式，展示了将监督学习与强化学习在训练中深度融合、辅以信任域与自适应指引，可以实现更强的推理性能。

Abstract: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

</details>


### [61] [Estimating Spatially Resolved Radiation Fields Using Neural Networks](https://arxiv.org/abs/2512.17654)
*Felix Lehner,Pasquale Lombardo,Susana Castillo,Oliver Hupe,Marcus Magnor*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present an in-depth analysis on how to build and train neural networks to estimate the spatial distribution of scattered radiation fields for radiation protection dosimetry in medical radiation fields, such as those found in Interventional Radiology and Cardiology. Therefore, we present three different synthetically generated datasets with increasing complexity for training, using a Monte-Carlo Simulation application based on Geant4. On those datasets, we evaluate convolutional and fully connected architectures of neural networks to demonstrate which design decisions work well for reconstructing the fluence and spectra distributions over the spatial domain of such radiation fields. All used datasets as well as our training pipeline are published as open source in separate repositories.

</details>


### [62] [You Only Train Once: Differentiable Subset Selection for Omics Data](https://arxiv.org/abs/2512.17678)
*Daphné Chopard,Jorge da Silva Gonçalves,Irene Cannistraci,Thomas M. Sutter,Julia E. Vogt*

Main category: cs.LG

TL;DR: YOTO: 一个端到端的稀疏基因子集选择与预测框架，在单细胞转录组数据上同时完成子集筛选与预测，构成闭环训练，提升可解释性与预测与泛化。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA-seq特征选择常分步骤进行，或依赖事后特征归因，导致选择和预测耦合性差、流程复杂。需要一个端到端、可实现稀疏性、且能提升解释性和成本效益的框架。

Method: 在一个可微分架构中同时识别离散基因子集和执行预测，预测任务直接引导基因选择，选择的子集反过来塑造预测表示，形成一个闭环以迭代改进。通过显式稀疏性约束确保仅所选基因参与推断，省去额外下游分类器的训练。采用多任务学习以在相关目标间共享表示，利用部分标注数据互相 informing，并发现跨任务的泛化基因子集。

Result: 在两个代表性scRNA-seq数据集上持续优于最先进的基线，得到紧凑且具解释性的基因子集，同时提升预测性能，证明端到端稀疏多任务子集选择有利于生物标志物发现与单细胞分析。

Conclusion: 端到端的稀疏多任务学习框架能够在 jointly 选择基因子集与预测的同时提升泛化能力和解释性，简化工作流程，并推动生物标志物发现与单细管分析的发展。

Abstract: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.

</details>


### [63] [Can You Hear Me Now? A Benchmark for Long-Range Graph Propagation](https://arxiv.org/abs/2512.17762)
*Luca Miglior,Matteo Tolloso,Alessio Gravina,Davide Bacciu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Effectively capturing long-range interactions remains a fundamental yet unresolved challenge in graph neural network (GNN) research, critical for applications across diverse fields of science. To systematically address this, we introduce ECHO (Evaluating Communication over long HOps), a novel benchmark specifically designed to rigorously assess the capabilities of GNNs in handling very long-range graph propagation. ECHO includes three synthetic graph tasks, namely single-source shortest paths, node eccentricity, and graph diameter, each constructed over diverse and structurally challenging topologies intentionally designed to introduce significant information bottlenecks. ECHO also includes two real-world datasets, ECHO-Charge and ECHO-Energy, which define chemically grounded benchmarks for predicting atomic partial charges and molecular total energies, respectively, with reference computations obtained at the density functional theory (DFT) level. Both tasks inherently depend on capturing complex long-range molecular interactions. Our extensive benchmarking of popular GNN architectures reveals clear performance gaps, emphasizing the difficulty of true long-range propagation and highlighting design choices capable of overcoming inherent limitations. ECHO thereby sets a new standard for evaluating long-range information propagation, also providing a compelling example for its need in AI for science.

</details>


### [64] [Calibratable Disambiguation Loss for Multi-Instance Partial-Label Learning](https://arxiv.org/abs/2512.17788)
*Wei Tang,Yin-Fang Yang,Weijia Zhang,Min-Ling Zhang*

Main category: cs.LG

TL;DR: 提出一种可校准的消解损失 CDL，用于 MIPL/PLL，提供两种实例化方式，插件式嵌入，具备理论下界与正则化性质，实验显示在分类和校准性能上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有 MIPL/PLL 方法在校准方面表现欠佳，导致预测不可靠；需一个可插拔的损失函数同时提升准确性和校准性。

Method: 提出 CDL，两个实现：1) 基于候选标签集合的概率进行预测校准；2) 同时利用候选与非候选标签的概率进行校准；可以无缝嵌入现有 MIPL/PLL 框架；给出理论分析（下界与正则化性质）。

Result: 在基准数据集和真实世界数据集上，CDL 显著提升分类精度与校准性能表现。

Conclusion: CDL 能与现有框架无缝结合，优于传统的消解损失；理论与实验证据皆支持其在分类与校准任务中的优越性。

Abstract: Multi-instance partial-label learning (MIPL) is a weakly supervised framework that extends the principles of multi-instance learning (MIL) and partial-label learning (PLL) to address the challenges of inexact supervision in both instance and label spaces. However, existing MIPL approaches often suffer from poor calibration, undermining classifier reliability. In this work, we propose a plug-and-play calibratable disambiguation loss (CDL) that simultaneously improves classification accuracy and calibration performance. The loss has two instantiations: the first one calibrates predictions based on probabilities from the candidate label set, while the second one integrates probabilities from both candidate and non-candidate label sets. The proposed CDL can be seamlessly incorporated into existing MIPL and PLL frameworks. We provide a theoretical analysis that establishes the lower bound and regularization properties of CDL, demonstrating its superiority over conventional disambiguation losses. Experimental results on benchmark and real-world datasets confirm that our CDL significantly enhances both classification and calibration performance.

</details>


### [65] [Exploiting ID-Text Complementarity via Ensembling for Sequential Recommendation](https://arxiv.org/abs/2512.17820)
*Liam Collins,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Donald Loveland,Leonardo Neves,Neil Shah*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern Sequential Recommendation (SR) models commonly utilize modality features to represent items, motivated in large part by recent advancements in language and vision modeling. To do so, several works completely replace ID embeddings with modality embeddings, claiming that modality embeddings render ID embeddings unnecessary because they can match or even exceed ID embedding performance. On the other hand, many works jointly utilize ID and modality features, but posit that complex fusion strategies, such as multi-stage training and/or intricate alignment architectures, are necessary for this joint utilization. However, underlying both these lines of work is a lack of understanding of the complementarity of ID and modality features. In this work, we address this gap by studying the complementarity of ID- and text-based SR models. We show that these models do learn complementary signals, meaning that either should provide performance gain when used properly alongside the other. Motivated by this, we propose a new SR method that preserves ID-text complementarity through independent model training, then harnesses it through a simple ensembling strategy. Despite this method's simplicity, we show it outperforms several competitive SR baselines, implying that both ID and text features are necessary to achieve state-of-the-art SR performance but complex fusion architectures are not.

</details>
