<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 8]
- [cs.LG](#cs.LG) [Total: 65]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing](https://arxiv.org/abs/2512.13892)
*Albert Dorador*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.

</details>


### [2] [Maximum Mean Discrepancy with Unequal Sample Sizes via Generalized U-Statistics](https://arxiv.org/abs/2512.13997)
*Aaron Wei,Milad Jalali,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 扩展广义U统计量理论到不等样本量的MMD估计量，给出不等样本量下的渐近分布新刻画，并提供提高不等样本量MMD检验功效的新准则；在保持数据完整性的前提下，获得更清晰的方差表述，并讨论在某些情形下零MMD与退化估计量的关系。


<details>
  <summary>Details</summary>
Motivation: 现有基于MMD的二样本检验多假设两组样本量相等，实际应用往往样本量不等，导致需要丢弃数据、降低检验功效。需在不丢弃数据的前提下正确刻画不等样本量情形的统计性质并提升功效。

Method: 将广义U统计量理论应用于常用的MMD估计量，推导不等样本量条件下的渐近分布，超越以往对比例关系的局限；给出用于优化不等样本量下MMD检验功效的新准则；给出方差的更清晰表述以及对退化与非退化MMD的关系分析。

Result: 给出不等样本量下MMD估计量的渐近分布的新刻画，提供提升功效的优化准则；方差推导更清晰，指出在某些情形下即使MMD为零也可能出现退化估计量，但在常见情形不成立，并给出构造与证明。

Conclusion: 保持全部数据可用性，提升MMD检验在现实设定中的准确性和可用性，扩展了二样本MMD检验在不等样本量上的理论与应用范围。

Abstract: Existing two-sample testing techniques, particularly those based on choosing a kernel for the Maximum Mean Discrepancy (MMD), often assume equal sample sizes from the two distributions. Applying these methods in practice can require discarding valuable data, unnecessarily reducing test power. We address this long-standing limitation by extending the theory of generalized U-statistics and applying it to the usual MMD estimator, resulting in new characterization of the asymptotic distributions of the MMD estimator with unequal sample sizes (particularly outside the proportional regimes required by previous partial results). This generalization also provides a new criterion for optimizing the power of an MMD test with unequal sample sizes. Our approach preserves all available data, enhancing test accuracy and applicability in realistic settings. Along the way, we give much cleaner characterizations of the variance of MMD estimators, revealing something that might be surprising to those in the area: while zero MMD implies a degenerate estimator, it is sometimes possible to have a degenerate estimator with nonzero MMD as well; we give a construction and a proof that it does not happen in common situations.

</details>


### [3] [On the Hardness of Conditional Independence Testing In Practice](https://arxiv.org/abs/2512.14000)
*Zheng He,Roman Pogodin,Yazhe Li,Namrata Deka,Arthur Gretton,Danica J. Sutherland*

Main category: stat.ML

TL;DR: 对条件独立性检验的核方法（KCI）进行深入分析，揭示实现有效控制I型误差与提升检验功效之间的权衡，强调条件均值嵌入误差与条件核选择在实际测试中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 理解为何在有限样本下，条件独立性检验难以获得非平凡功效，以及现有KCI类测试在实践中不稳定的原因。

Method: 分析KCI测试及其相关的广义协方差度量，强调条件均值嵌入误差对I型误差的影响，以及选择合适的条件核对提升功效但可能放大I型误差的作用。

Result: 发现I型误差主要受条件均值嵌入估计误差驱动；条件核的选取对功效有决定性影响，同时也可能导致I型误差上涨。Generalized Covariance Measure在多种测试中近似成立，且多种CI测试可视为其特例。

Conclusion: 要提升KCI类测试的实用性，需要在控制I型误差和保持检验功效之间作出权衡，并在核选择和嵌入估计的估计策略上进行谨慎设计；未来工作应关注更稳健的条件均值嵌入和自适应核策略。

Abstract: Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on "hiding" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test - of which we show the Generalized Covariance Measure underlying many recent tests is nearly a special case - and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type-I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type-I error.

</details>


### [4] [Weighted Conformal Prediction Provides Adaptive and Valid Mask-Conditional Coverage for General Missing Data Mechanisms](https://arxiv.org/abs/2512.14221)
*Jiarong Fan,Juhyun Park. Thi Phuong Thuy Vo,Nicolas Brunel*

Main category: stat.ML

TL;DR: 提出在缺失协变量情形下通过预插补-掩码-纠正的分割式CP框架，实现边际覆盖和MCV，结合分布性插补的重加权预测集修正，给出两种近似可证算法，实验显示在保留 garantías 的同时显著缩小区间宽度。


<details>
  <summary>Details</summary>
Motivation: 解决缺失数据导致的CP覆盖性下降及对异质性缺失模式的需求，推动MCV在现实场景中的有效应用。

Method: 提出 preimpute-mask-then-correct 框架：1) 对 calibration/测试集进行分布性插补（多重插补）以处理缺失；2) 基于掩码信息进行条件性分割并构造预测集；3) 对预测集进行重加权修正以恢复覆盖；给出两种实现算法并证明其近似边际有效性与MCV。

Result: 理论层面：在一般缺失数据机制下获得边际覆盖与MCV的保证。经验层面：在合成与真实数据集上，区间宽度显著小于标准MCV方法，同时保持目标覆盖。

Conclusion: 该框架与标准插补流程兼容，提供在缺失数据下可证的、更紧凑的分割式CP解决方案，提升实际应用中的覆盖性与效率。

Abstract: Conformal prediction (CP) offers a principled framework for uncertainty quantification, but it fails to guarantee coverage when faced with missing covariates. In addressing the heterogeneity induced by various missing patterns, Mask-Conditional Valid (MCV) Coverage has emerged as a more desirable property than Marginal Coverage. In this work, we adapt split CP to handle missing values by proposing a preimpute-mask-then-correct framework that can offer valid coverage. We show that our method provides guaranteed Marginal Coverage and Mask-Conditional Validity for general missing data mechanisms. A key component of our approach is a reweighted conformal prediction procedure that corrects the prediction sets after distributional imputation (multiple imputation) of the calibration dataset, making our method compatible with standard imputation pipelines. We derive two algorithms, and we show that they are approximately marginally valid and MCV. We evaluate them on synthetic and real-world datasets. It reduces significantly the width of prediction intervals w.r.t standard MCV methods, while maintaining the target guarantees.

</details>


### [5] [Improving the Accuracy of Amortized Model Comparison with Self-Consistency](https://arxiv.org/abs/2512.14308)
*Šimon Kucharský,Aayush Mishra,Daniel Habermann,Stefan T. Radev,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 在自恰性（SC）训练下，基于近似参数后验的模型比较在鲁棒性和准确性上优于直接估计模型证据的方法；对有解析似然的情形尤为有利，且对无解析似然的情形收益有限。


<details>
  <summary>Details</summary>
Motivation: ABI 在给定模型下快速近似后验，但对模型 misspecification 极为敏感；在多模型比较场景中，至少有一个模型可能不正确。SC 提供一种可应用于经验数据的自洽训练方式，以缓解外推偏差。本文在四种不同的模型比较概念框架下评估 SC 的作用。

Method: 在两组合成数据和两组真实数据的案例研究中，比较四种概念化的 ABI 模型比较：一是通过近似后验估计边缘似然的方法，二是直接估计模型证据或后验模型概率的方法；同时比较是否使用 SC 训练。评估在存在明显模型 misspecification 时的鲁棒性，以及对有无解析似然的情形。

Result: 总体上，基于近似参数后验来估计边缘似然的方法在四个案例中优于直接估计证据/后验模型概率的方法；SC 在有解析似然的情形下提升鲁棒性，即使在严重 misspecification 下也有效；对于无解析似然的情形，SC 的收益不稳定且受限。

Conclusion: 实用建议：优先选择参数后验为基础的模型比较方法，并在经验数据上使用 SC 训练以缓解在模型 misspecification 下的外推偏差。

Abstract: Amortized Bayesian inference (ABI) offers fast, scalable approximations to posterior densities by training neural surrogates on data simulated from the statistical model. However, ABI methods are highly sensitive to model misspecification: when observed data fall outside the training distribution (generative scope of the statistical models), neural surrogates can behave unpredictably. This makes it a challenge in a model comparison setting, where multiple statistical models are considered, of which at least some are misspecified. Recent work on self-consistency (SC) provides a promising remedy to this issue, accessible even for empirical data (without ground-truth labels). In this work, we investigate how SC can improve amortized model comparison conceptualized in four different ways. Across two synthetic and two real-world case studies, we find that approaches for model comparison that estimate marginal likelihoods through approximate parameter posteriors consistently outperform methods that directly approximate model evidence or posterior model probabilities. SC training improves robustness when the likelihood is available, even under severe model misspecification. The benefits of SC for methods without access of analytic likelihoods are more limited and inconsistent. Our results suggest practical guidance for reliable amortized Bayesian model comparison: prefer parameter posterior-based methods and augment them with SC training on empirical datasets to mitigate extrapolation bias under model misspecification.

</details>


### [6] [Continual Learning at the Edge: An Agnostic IIoT Architecture](https://arxiv.org/abs/2512.14311)
*Pablo García-Santaclara,Bruno Fernández-Castro,Rebeca P. Díaz-Redondo,Carlos Calvo-Moa,Henar Mariño-Bodelón*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The exponential growth of Internet-connected devices has presented challenges to traditional centralized computing systems due to latency and bandwidth limitations. Edge computing has evolved to address these difficulties by bringing computations closer to the data source. Additionally, traditional machine learning algorithms are not suitable for edge-computing systems, where data usually arrives in a dynamic and continual way. However, incremental learning offers a good solution for these settings. We introduce a new approach that applies the incremental learning philosophy within an edge-computing scenario for the industrial sector with a specific purpose: real time quality control in a manufacturing system. Applying continual learning we reduce the impact of catastrophic forgetting and provide an efficient and effective solution.

</details>


### [7] [From STLS to Projection-based Dictionary Selection in Sparse Regression for System Identification](https://arxiv.org/abs/2512.14404)
*Hangjun Cho,Fabio V. G. Amaral,Andrei A. Klishin,Cassio M. Oishi,Steven L. Brunton*

Main category: stat.ML

TL;DR: 提出基于分数的字典筛选策略，结合 STLS/SINDy 的稀疏回归，以提升数据驱动建模的准确性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决字典基稀疏回归中字典选择对模型鲁棒性和可解释性的影响，利用分数（scores）和字典项相干性进行指导。

Method: 对 STLS 的迭代过程，分析分数与字典相干性对系数的影响，提出分数引导的字典选择策略，并在常微分方程(ODE)和偏微分方程(PDE)上进行数值实验。

Result: 引入分数指导的筛选提高了准确性与可解释性，提升了多种 SINDy 类型算法的鲁棒性。

Conclusion: 将分数-guided 的字典 refinements 融入 SINDy 家族方法，可以在某些数据驱动方程识别任务中提升鲁棒性和可解释性。

Abstract: In this work, we revisit dictionary-based sparse regression, in particular, Sequential Threshold Least Squares (STLS), and propose a score-guided library selection to provide practical guidance for data-driven modeling, with emphasis on SINDy-type algorithms. STLS is an algorithm to solve the $\ell_0$ sparse least-squares problem, which relies on splitting to efficiently solve the least-squares portion while handling the sparse term via proximal methods. It produces coefficient vectors whose components depend on both the projected reconstruction errors, here referred to as the scores, and the mutual coherence of dictionary terms. The first contribution of this work is a theoretical analysis of the score and dictionary-selection strategy. This could be understood in both the original and weak SINDy regime. Second, numerical experiments on ordinary and partial differential equations highlight the effectiveness of score-based screening, improving both accuracy and interpretability in dynamical system identification. These results suggest that integrating score-guided methods to refine the dictionary more accurately may help SINDy users in some cases to enhance their robustness for data-driven discovery of governing equations.

</details>


### [8] [LLmFPCA-detect: LLM-powered Multivariate Functional PCA for Anomaly Detection in Sparse Longitudinal Texts](https://arxiv.org/abs/2512.14604)
*Prasanjit Dubey,Aritra Guha,Zhengyi Zhou,Qiong Wu,Xiaoming Huo,Paromita Dubey*

Main category: stat.ML

TL;DR: 将LLM嵌入与稀疏多变量函数主成分分析相结合的LLmFPCA-detect框架，用于在稀疏纵向文本数据中检测簇和异常，且在亚马逊评论轨迹与维基百科对话流等公开数据集上优于基线。


<details>
  <summary>Details</summary>
Motivation: 稀疏纵向文本数据因观察频率不均、噪声与异常点多而难以检测关键模式；需要将高维文本信息转化为可比的数值空间，并通过统计方法提取群体特征、实现分群与异常检测，以及支持下游预测任务。

Method: 使用LLM提示将每段文本嵌入应用场景特定的数值空间；在该数值空间进行稀疏多变量函数主成分分析（mFPCA），得到个体水平分数，与基线静态协变量结合实现数据分段、无监督异常检测及推断；基于发现的分段和异常进行动态关键词分析，并将簇特异的主成分分数作为新特征融入现有管线以提升预测性能。通过对亚马逊评论轨迹与维基百科对话流的公开数据集进行稳定性与性能实验，结果显示在多域任务中优于最先进基线。

Result: 框架成功地揭示了SL文本数据中的主要群体特征与异常模式，提供了动态关键词分析以提升解释性；簇特异的FPCA分数在下游预测中能提升性能；在两个公开数据集上对比基线显示出显著的改进，且实验验证了方法的稳健性。

Conclusion: LLmFPCA-detect为稀疏纵向文本数据的聚类、异常检测和推断提供了一个灵活且可扩展的分析框架，跨域适用且具备提升下游任务性能的潜力，且可通过动态关键词配置增强解释性。

Abstract: Sparse longitudinal (SL) textual data arises when individuals generate text repeatedly over time (e.g., customer reviews, occasional social media posts, electronic medical records across visits), but the frequency and timing of observations vary across individuals. These complex textual data sets have immense potential to inform future policy and targeted recommendations. However, because SL text data lack dedicated methods and are noisy, heterogeneous, and prone to anomalies, detecting and inferring key patterns is challenging. We introduce LLmFPCA-detect, a flexible framework that pairs LLM-based text embeddings with functional data analysis to detect clusters and infer anomalies in large SL text datasets. First, LLmFPCA-detect embeds each piece of text into an application-specific numeric space using LLM prompts. Sparse multivariate functional principal component analysis (mFPCA) conducted in the numeric space forms the workhorse to recover primary population characteristics, and produces subject-level scores which, together with baseline static covariates, facilitate data segmentation, unsupervised anomaly detection and inference, and enable other downstream tasks. In particular, we leverage LLMs to perform dynamic keyword profiling guided by the data segments and anomalies discovered by LLmFPCA-detect, and we show that cluster-specific functional PC scores from LLmFPCA-detect, used as features in existing pipelines, help boost prediction performance. We support the stability of LLmFPCA-detect with experiments and evaluate it on two different applications using public datasets, Amazon customer-review trajectories, and Wikipedia talk-page comment streams, demonstrating utility across domains and outperforming state-of-the-art baselines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 混合训练可防止在数学专用任务微调中发生灾难性遗忘：在 Flan-T5-Base 上进行数学数据集微调后，保持一般能力（NLI）不被显著削弱，同时仍能维持数学性能。1:1 混合时数学准确度为 12.0%，NLI 保持在 86.2%；即使极小的 NLI 曝露（约 6.2%）也能实现有效正则化。


<details>
  <summary>Details</summary>
Motivation: 研究在对大型语言模型进行专门化微调时的灾难性遗忘现象，探索是否通过混合训练可以在保持原有能力的同时提升特定任务性能。

Method: 在 Flan-T5-Base（250M 参数）上对 DeepMind Mathematics 数据集进行微调，并在 MultiNLI 上衡量遗忘。比较 math-only、NLI 训练和混合训练（不同混合比例，从 1:1 到 15:1），评估数学准确度和 NLI 精度。

Result: math-only 将数学准确度从 3.1% 提升至 12.0%，但 NLI 精度从 81.0% 降至 16.5%（在前 1,000 步就出现 64.5 个百分点的下降）。混合训练在 1:1 比例下完全消除了灾难性遗忘，数学保持 12.0%（与 math-only 相同），NLI 提升至 86.2%。系统性探索混合比例（1:1 至 15:1），即使 NLI 曝露仅 6.2%，也对正则化有效。

Conclusion: 专门化并不必然带来遗忘，通过引入少量对广义能力的暴露（混合训练），可以同时获得专门任务性能和对通用能力的保留，且对未来更大模型的扩展也具有潜在的额外益处。

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [10] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [11] [Hybrid Iterative Solvers with Geometry-Aware Neural Preconditioners for Parametric PDEs](https://arxiv.org/abs/2512.14596)
*Youngkyu Lee,Francesc Levrero Florencio,Jay Pathak,George Em Karniadakis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The convergence behavior of classical iterative solvers for parametric partial differential equations (PDEs) is often highly sensitive to the domain and specific discretization of PDEs. Previously, we introduced hybrid solvers by combining the classical solvers with neural operators for a specific geometry 1, but they tend to under-perform in geometries not encountered during training. To address this challenge, we introduce Geo-DeepONet, a geometry-aware deep operator network that incorporates domain information extracted from finite element discretizations. Geo-DeepONet enables accurate operator learning across arbitrary unstructured meshes without requiring retraining. Building on this, we develop a class of geometry-aware hybrid preconditioned iterative solvers by coupling Geo-DeepONet with traditional methods such as relaxation schemes and Krylov subspace algorithms. Through numerical experiments on parametric PDEs posed over diverse unstructured domains, we demonstrate the enhanced robustness and efficiency of the proposed hybrid solvers for multiple real-world applications.

</details>


### [12] [Dropout Neural Network Training Viewed from a Percolation Perspective](https://arxiv.org/abs/2512.13853)
*Finley Devlin,Jaron Sanders*

Main category: cs.LG

TL;DR: Dropout is analyzed as a percolation process in neural networks, revealing a percolation effect on connectivity that can break training without biases and is likely relevant for networks with biases as well.


<details>
  <summary>Details</summary>
Motivation: To understand whether dropout-induced random removal of connections can create a percolation phenomenon that affects the ability of a network to propagate information from input to output and consequently its trainability, and to identify how network topology and biases influence this effect.

Method: Develop percolation-based models that mimic dropout in feedforward networks; analyze the probability of existence of a path from input to output under edge removal; theoretically characterize percolation effects and connection to training breakdown, with heuristic arguments for networks with biases.

Result: The theory demonstrates the existence of a percolative effect in dropout and shows that this effect can cause a breakdown when training bias-free networks with dropout, with heuristic arguments suggesting the breakdown extends to networks with biases.

Conclusion: Dropout can induce percolation phenomena that impact trainability; network topology and the presence of biases interact with dropout, indicating careful consideration of connectivity and architecture when using dropout regularization.

Abstract: In this work, we investigate the existence and effect of percolation in training deep Neural Networks (NNs) with dropout. Dropout methods are regularisation techniques for training NNs, first introduced by G. Hinton et al. (2012). These methods temporarily remove connections in the NN, randomly at each stage of training, and update the remaining subnetwork with Stochastic Gradient Descent (SGD). The process of removing connections from a network at random is similar to percolation, a paradigm model of statistical physics.
  If dropout were to remove enough connections such that there is no path between the input and output of the NN, then the NN could not make predictions informed by the data. We study new percolation models that mimic dropout in NNs and characterise the relationship between network topology and this path problem. The theory shows the existence of a percolative effect in dropout. We also show that this percolative effect can cause a breakdown when training NNs without biases with dropout; and we argue heuristically that this breakdown extends to NNs with biases.

</details>


### [13] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: 提出一种名为 Hessian Reassignment 的两步模型无关的文档分类类级清除方法。通过一次影响性更新和共轭梯度求解的 Hessian-向量系统，去除目标类别的训练数据贡献；再通过 Top-1 分类约束，确保清除过程保持决策空间的一致性。实验显示在标准文本基准上接近完全重新训练的保留类别准确率，且速度远超全量再训练，并降低对删除类别的成员身份推断攻击优势。


<details>
  <summary>Details</summary>
Motivation: 随着对数据隐私与可控删除的需求增强，能够在不重新训练整个模型的情况下从模型中有效去除特定训练数据的影响尤为重要。尽管对大语言模型的无忘记研究有所进展，面向文档分类模型的类级无忘记研究仍相对薄弱，缺乏高效、可解释的解决方案。

Method: 提出 Hessian Reassignment：第一步进行一次影响力风格的更新，通过求解带共轭梯度的 Hessian-向量系统，去掉目标类别的所有训练点贡献，只需要梯度和 Hessian-向量乘积；第二步引入 Top-1 分类约束，确保在清除后仍以该类别最可能被预测的方式执行，避免常见基线中对删除类别样本的随机重新分类。该方法是模型无关的。

Result: 在标准文本基准上，保留类别的准确率接近在去除该类别后进行的全量重训练，同时运行速度比完全重训练快数量级；并且在删除类别上，成员身份推断攻击的优势显著下降，使用了聚合的多影子攻击评估。

Conclusion: 给出一种实用且理论上合理的路径用于高效的文档分类类级无忘记，兼具性能接近完全重训练的效果与隐私安全评估的提升。该方法通过对目标类别的影响进行高效的 Hessian-向量化处理并以 Top-1 约束强化决策稳定性，具有良好的可扩展性和对其他模型的泛化潜力。

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [14] [Understanding the Gain from Data Filtering in Multimodal Contrastive Learning](https://arxiv.org/abs/2512.14230)
*Divyansh Pareek,Sewoong Oh,Simon S. Du*

Main category: cs.LG

TL;DR: 在双模态数据生成模型下，使用教师（教师模型）过滤数据可 provably 提升对比学习的误差界；未过滤的误差近似为 1/(η√n)，而经过过滤在η较大时的上界为 1/√(ηn)，在η较小时上界为 1/√n。


<details>
  <summary>Details</summary>
Motivation: 海量网页数据的质量差是多模态表示学习的瓶颈。教师过滤作为一种常用的数据筛选方法，理论上解释其在改进对比学习中的表现的原因。

Method: 在线性对比学习框架下，设 n 个成对样本中有 η∈(0,1] 的数据对正确匹配，分析有/无教师过滤两种情形的误差界，给出在不同 η 情况下的收敛界。

Result: 结论性地给出无过滤时的误差级数为 O(1/(η√n))，有过滤时的误差上界为 O(1/√(ηn))（当 η 较大时），以及 O(1/√n)（当 η 较小时）。

Conclusion: 在双模态数据产生模型下，教师基于过滤的对比学习理论上可解释其经验上的优势；过滤的效果随正确匹配比例 η 增大而增强，提供了对数据筛选在大规模多模态学习中有效性的理论支撑。

Abstract: The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $η\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\frac{1}{η\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\frac{1}{\sqrt{ηn}}$ in the large $η$ regime, and by $\frac{1}{\sqrt{n}}$ in the small $η$ regime.

</details>


### [15] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [16] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [17] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 提出一个时间预算约束下的滑栈推荐统一框架，将其建模为带预算的MDP，并给出仿真框架，实证显示在紧凑时间预算下，基于策略梯度的on-policy和off-policy RL方法优于传统上下文带宽方法。


<details>
  <summary>Details</summary>
Motivation: 用户在移动电商等场景中具有有限的时间预算，需要在相关性和评估成本之间权衡以提升参与度。高相关但评估成本高的物品若超出预算将降低点击率，因此需学习同时考虑用户偏好和时间预算的策略。

Method: 将时间预算下的滑栈推荐建模为MDP，定义预算感知效用；设计仿真框架用于在再排序数据上研究策略行为；对比on-policy和off-policy RL控制与传统上下文带宽方法在预算受限下的表现，数据源为阿里巴巴的个性化再排序数据集。

Result: 实验结果表明，在紧缩时间预算条件下，基于RL的控制策略（包括on-policy和off-policy）相较传统上下文带宽方法，可以提升用户参与度等指标。

Conclusion: 时间预算约束下的滑栈推荐具备显著潜力，MDP建模和仿真框架有助于系统性研究策略在资源约束中的行为；RL方法在资源受限环境中可实现更高的 engagement。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [18] [RAST-MoE-RL: A Regime-Aware Spatio-Temporal MoE Framework for Deep Reinforcement Learning in Ride-Hailing](https://arxiv.org/abs/2512.13727)
*Yuhan Tang,Kangxin Cui,Jung Ho Park,Yibo Zhao,Xuan Jiang,Haoze He,Dingyi Zhuang,Shenhao Wang,Jiangbo Yu,Haris Koutsopoulos,Jinhua Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Ride-hailing platforms face the challenge of balancing passenger waiting times with overall system efficiency under highly uncertain supply-demand conditions. Adaptive delayed matching creates a trade-off between matching and pickup delays by deciding whether to assign drivers immediately or batch requests. Since outcomes accumulate over long horizons with stochastic dynamics, reinforcement learning (RL) is a suitable framework. However, existing approaches often oversimplify traffic dynamics or use shallow encoders that miss complex spatiotemporal patterns.
  We introduce the Regime-Aware Spatio-Temporal Mixture-of-Experts (RAST-MoE), which formalizes adaptive delayed matching as a regime-aware MDP equipped with a self-attention MoE encoder. Unlike monolithic networks, our experts specialize automatically, improving representation capacity while maintaining computational efficiency. A physics-informed congestion surrogate preserves realistic density-speed feedback, enabling millions of efficient rollouts, while an adaptive reward scheme guards against pathological strategies.
  With only 12M parameters, our framework outperforms strong baselines. On real-world Uber trajectory data (San Francisco), it improves total reward by over 13%, reducing average matching and pickup delays by 10% and 15% respectively. It demonstrates robustness across unseen demand regimes and stable training. These findings highlight the potential of MoE-enhanced RL for large-scale decision-making with complex spatiotemporal dynamics.

</details>


### [19] [CurvaDion: Curvature-Adaptive Distributed Orthonormalization](https://arxiv.org/abs/2512.13728)
*Bhavesh Kumar,Roger Jin,Jeffrey Quesnelle*

Main category: cs.LG

TL;DR: CurvaDion introduces Relative Maximum Momentum Change (RMMC) to detect high-curvature regions and synchronize gradients adaptively, achieving ~99% communication reduction while maintaining baseline convergence from 160M to 1.3B parameters.


<details>
  <summary>Details</summary>
Motivation: Distributed training of extremely large language models faces a critical bottleneck in gradient synchronization over networks. Existing methods like Dion sync at every step, which is inefficient in flat regions; hence there is a need for curvature-aware synchronization to reduce communication without harming convergence.

Method: Propose CurvaDion that uses RMMC, a momentum-based metric computed during optimization, as a proxy for directional curvature. RMMC triggers synchronization in high-curvature regions; it costs O(d) per layer and leverages momentum dynamics already computed during optimization, linking to loss curvature. The method aims to adapt synchronization frequency dynamically during training.

Result: Achieves ~99% reduction in communication while matching baseline convergence across models ranging from 160M to 1.3B parameters.

Conclusion: RMMC provides a practical and theoretically linked proxy for curvature that enables adaptive, communication-efficient synchronization in large-scale distributed training without sacrificing convergence.

Abstract: As language models scale to trillions of parameters, distributed training across many GPUs becomes essential, yet gradient synchronization over high-bandwidth, low-latency networks remains a critical bottleneck. While recent methods like Dion reduce per-step communication through low-rank updates, they synchronize at every step regardless of the optimization landscape. We observe that synchronization requirements vary dramatically throughout training: workers naturally compute similar gradients in flat regions, making frequent synchronization redundant, while high-curvature regions require coordination to prevent divergence. We introduce CurvaDion, which uses Relative Maximum Momentum Change (RMMC) to detect high-curvature regions requiring synchronization. RMMC leverages momentum dynamics which are already computed during optimization as a computationally tractable proxy for directional curvature, adding only $\mathcal{O}(d)$ operations per layer. We establish theoretical connections between RMMC and loss curvature and demonstrate that CurvaDion achieves 99\% communication reduction while matching baseline convergence across models from 160M to 1.3B parameters.

</details>


### [20] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: 基于扩散模型的风场超分辨与多条件引导方法WindDM，结合复合无条件引导（CCFG），实现高保真且低成本的风场重建。


<details>
  <summary>Details</summary>
Motivation: 获取高分辨率、准确的风场数据成本高，传统重建方法在成本和精度之间存在权衡；扩散模型可利用大量条件变量，提升风数据重建质量，但风数据的通道数远多于自然图像。

Method: 将多条件输入的分类无引导（CFG）泛化为复合 CFG（CCFG），可在任何预训练且带 CFG dropout 的扩散模型中使用；开发 WindDM，针对工业规模风动力学重建，结合 CCFG。

Result: CCFG 相比 CFG 在风超分任务中具有更高保真度；WindDM 在深度学习模型中达到最优重建质量，成本比传统方法低至约 1000 倍。

Conclusion: 多条件 CFG 泛化与 WindDM 提供了高保真、低成本的风场重建解决方案，适用于大规模条件输入场景，证明扩散模型在风场重建中的有效性。

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [21] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: 提出一个集合条件扩散模型PIS，用于在任意观测集合下进行PDE约束的物理参数反演，具备不确定性量化、对极度稀疏观测的鲁棒性，并优于现有算子学习基线。


<details>
  <summary>Details</summary>
Motivation: 在观测稀疏、不规则且受传感器布置限制的条件下，PDE参数反演变得病态且难以获得可靠的不确定性量化。现有深度/算子学习方法往往依赖固定网格、对稀疏观测敏感，且缺乏有效的后验不确定性。需要一个能处理任意观测几何的统一框架并提供校准的后验分布。

Method: 提出Physical Inversion Solver (PIS)，一个集合条件扩散框架。使用Set Transformer编码器处理任意数量与几何的观测；引入cosine-annealed sparsity curriculum提升鲁棒性；进行信息论分析以揭示极端稀疏下观测熵的分布规律；在Darcy流、Helmholtz波场反演及Hooke定律的结构健康监测等三类PDE反问题上评估。

Result: 在极端稀疏观测率（0.29%）下，现有算子学习基线往往崩溃或发散，而PIS保持稳定、精确， inversion误差降低12.28%–88.73%，并产生经过校准的后验样本，真实反映数据稀缺性与物理不确定性。

Conclusion: PIS是一种强大、通用且对极度欠采样观测具有鲁棒性的物理反演解决方案，能够在任意观测几何与极端稀疏条件下实现高质量反演与不确定性量化。

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [22] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: 提出 LLRC：一种梯度驱动、无微调的低秩压缩方法，通过学习掩码权重来选择奇异值，逐步减少保留的奇异值，达到更优的压缩与下游任务性能权衡。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的低秩分解中，难以为每层选择最优秩以同时优化压缩率和下游任务准确率。现有方法要么依赖启发式搜索，范围有限、性能次优；要么是梯度法但在无后期微调的情况下不如启发式方法。

Method: 在校准数据集上仅训练掩码权重，以选择更少的奇异值，同时最小化中间激活与原模型之间的分布差异；整个过程不进行后置微调。

Result: 在多种压缩率下，LLRC在无后微调的前提下超越同类需要无后续微调的排名选择方法：以 Llama-2-13B 的 20% 压缩率为例，LLRC 在 MMLU、BoolQ、OpenbookQA 上分别比 STRS 高出 12%、3.5%、4.4%。相比其他压缩技术，LLRC 在多数据集和压缩率上持续优于 SVD-LLM 和 LLM-Pruner 的无微调变体，并且在与 LLM-Pruner 的微调变体相比时也具有竞争力。

Conclusion: 该工作证明了在无后续微调的前提下，通过学习掩码权重进行梯度优化的低秩压缩方法，能够在压缩与性能之间取得优越权衡，并且在多任务上与需要微调的方法相当甚至优于部分基线。

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [23] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 提出在联邦推荐系统中使用参数高效微调（PEFT）来对嵌入进行降维/压缩，以显著降低嵌入参数的传输开销，同时提升推荐准确性，提供插件式框架并结合LoRA、哈希编码以及新颖的RQ-VAE策略。


<details>
  <summary>Details</summary>
Motivation: 嵌入参数规模（尤其是巨量物品嵌入）在分布式联邦推荐中造成巨大的通信开销。现有工作多关注模型推理效率，而对嵌入参数传输成本的关注不足。需要在不牺牲性能的前提下降低嵌入相关的通信负担，并实现与现有FR方法的无缝集成。

Method: 提出一个基于PEFT的嵌入设计的FR训练框架，采用插件式结构可嵌入现有FR方法。核心包括：1) 采用LoRA等PEFT技术对嵌入进行参数高效调整；2) 基于哈希编码等编码策略降低嵌入参数数量的传输量；3) 引入残差量化变分自编码器（RQ-VAE）作为一种新颖的PEFT策略。框架与多种FR模型骨架及数据集进行广泛实验。

Result: 在不同FR骨架和数据集上，框架显著降低通信开销的同时，提升或至少保持了准确性。提供了可复用的代码实现。

Conclusion: 通过将PEFT嵌入引入FR训练框架，成功降低嵌入参数传输量并提升模型性能，证明该策略在分布式隐私保护场景中的实用性与可扩展性；并为未来在FR中的嵌入优化提供了有效路径。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [24] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [25] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 提出了一种基于时间-频率融合的对比学习框架 TF-MCL，用于EEG-MDD检测，通过 FMH 将时频信息映射到融合域，并以多域交叉损失重建时频与融合域的表示分布，在 MODMA 与 PRED+CT 数据集上显著超越 SOTA。


<details>
  <summary>Details</summary>
Motivation: 在抑郁症诊断中，基于 EEG 的监督方法依赖大量标注数据，且现有对比学习未能充分捕捉EEG 的时间-频率特征分布，导致低语义表示不足以用于 MDD 检测。

Method: 提出时间-频率融合与多域交叉损失模型 TF-MCL。通过融合映射头（FMH）将时频域信息高效映射到融合域，提升模型对时间-频率信息的综合能力；通过优化多域交叉损失，使时频域与融合域表示的分布得以重建，提升融合表示的获取。

Result: 在公开数据集 MODMA 与 PRED+CT 上，TF-MCL 显著提升准确率，分别超越现有 SOTA 5.87% 与 9.96%。

Conclusion: TF-MCL 通过时间-频率融合与多域对比学习，提升对 EEG 时间-频率特征的表征能力，改善 MDD 检测的自监督学习效果，并在两大数据集上实现显著性能提升。

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [26] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: 提出 Laminar Flow Hypothesis，认为良性输入在LLM的高维潜在空间中产生平滑、渐进的轨迹，而对抗性提示引发混沌、高方差的语义湍流。提出零样本指标：层间余弦速度的方差，用于实时检测和诊断模型的安全架构；在小型模型上实验，Qwen2-1.5B 在对抗攻击下涡动显著增加（75.4%，p<0.001），Gemma-2B 则呈现涡动下降（22.0%），显示不同安全机制的内在冲突与反射式拒绝机制。


<details>
  <summary>Details</summary>
Motivation: 解决当前基于外部分类器或词汇过滤的防御方法的计算开销与易脆性问题，利用LLM内部推理过程的动力学特征来检测与诊断对抗性攻击与安全架构。

Method: 提出一个零-shot指标：层间 cosinevelocity 的方差来量化潜在空间的动态；在多种小型语言模型上进行对抗攻击实验，比较 RLHF 调整下的模型（如 Qwen2-1.5B）与不同安全架构模型（如 Gemma-2B）的涡动变化。

Result: Qwen2-1.5B 在攻击下涡动显著增加 75.4%（p<0.001），验证内在冲突。Gemma-2B 出现 22.0% 的涡动下降，表现出低熵的“reflex-based”拒绝机制。该涡动度量在轻量、实时检测和黑箱模型安全架构诊断方面具有潜力。

Conclusion: Semantic Turbulence 可作为低成本的实时 jailbreak 检测器以及非侵入式的安全架构诊断工具，支持对比不同模型的对抗鲁棒性与内部机制。

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [27] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 资源有限情境下的金融新闻情感分析：嵌入表示在小数据集上的收益有限，验证集与测试集存在显著差距，可能低于基线表现。


<details>
  <summary>Details</summary>
Motivation: 解决小数据环境下情感分类的泛化与性能问题，评估不同嵌入表示（Word2Vec、GloVe、句子变换器）在金融新闻中的有效性。

Method: 在人工标注的头条上，将 Word2Vec、GloVe、句子变换器嵌入与梯度提升分类器结合，比较在有限数据条件下的训练、验证、测试表现，分析数据充足性阈值、过拟合风险，以及嵌入质量对结果的影响；同时展示周度情感聚合与市场监控的应用。

Result: 验证性能与测试性能之间存在显著差距；即使验证指标良好，测试结果往往低于简单基线，且嵌入的收益在数据不足时递减；小验证集容易导致模型选择的过拟合；预训练嵌入在达到数据充足性阈值前的收益有限甚至为负。

Conclusion: 单纯提高嵌入质量不能解决数据稀缺问题，应考虑 few-shot 学习、数据增强或基于词典的混合方法等来应对标注样本不足场景。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [28] [Network-Wide Traffic Volume Estimation from Speed Profiles using a Spatio-Temporal Graph Neural Network with Directed Spatial Attention](https://arxiv.org/abs/2512.13758)
*Léo Hein,Giovanni de Nunzio,Giovanni Chierchia,Aurélie Pirayre,Laurent Najman*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Existing traffic volume estimation methods typically address either forecasting traffic on sensor-equipped roads or spatially imputing missing volumes using nearby sensors. While forecasting models generally disregard unmonitored roads by design, spatial imputation methods explicitly address network-wide estimation; yet this approach relies on volume data at inference time, limiting its applicability in sensor-scarce cities. Unlike traffic volume data, probe vehicle speeds and static road attributes are more broadly accessible and support full coverage of road segments in most urban networks. In this work, we present the Hybrid Directed-Attention Spatio-Temporal Graph Neural Network (HDA-STGNN), an inductive deep learning framework designed to tackle the network-wide volume estimation problem. Our approach leverages speed profiles, static road attributes, and road network topology to predict daily traffic volume profiles across all road segments in the network. To evaluate the effectiveness of our approach, we perform extensive ablation studies that demonstrate the model's capacity to capture complex spatio-temporal dependencies and highlight the value of topological information for accurate network-wide traffic volume estimation without relying on volume data at inference time.

</details>


### [29] [Enhancing Semi-Supervised Multi-View Graph Convolutional Networks via Supervised Contrastive Learning and Self-Training](https://arxiv.org/abs/2512.13770)
*Huaiyuan Xiao,Fadi Dornaika,Jingjun Bi*

Main category: cs.LG

TL;DR: MV-SupGCN is a semi-supervised multi-view GCN framework that combines supervised contrastive loss with cross-entropy, dual graph construction (KNN-based and semi-supervised) per view, and multi-view contrastive learning with pseudo-labeling to leverage unlabeled data; achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing multi-view GCN methods often fail to fully exploit complementary information across views. Single-graph constructions can be unstable or incomplete, and unlabeled data are underutilized, leading to suboptimal representations and generalization.

Method: Propose MV-SupGCN: (1) joint loss of Cross-Entropy and Supervised Contrastive loss to enhance discriminative features; (2) per-view dual graph construction using both KNN-based and semi-supervised graphs for robustness; (3) a unified framework integrating cross-view contrastive learning with pseudo-labeling to leverage unlabeled data and align multi-view embeddings.

Result: Extensive experiments show MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks.

Conclusion: An integrated semi-supervised multi-view GCN that effectively leverages cross-view information, robust graph construction, and unlabeled data to improve discrimination and generalization; source code available.

Abstract: The advent of graph convolutional network (GCN)-based multi-view learning provides a powerful framework for integrating structural information from heterogeneous views, enabling effective modeling of complex multi-view data. However, existing methods often fail to fully exploit the complementary information across views, leading to suboptimal feature representations and limited performance. To address this, we propose MV-SupGCN, a semi-supervised GCN model that integrates several complementary components with clear motivations and mutual reinforcement. First, to better capture discriminative features and improve model generalization, we design a joint loss function that combines Cross-Entropy loss with Supervised Contrastive loss, encouraging the model to simultaneously minimize intra-class variance and maximize inter-class separability in the latent space. Second, recognizing the instability and incompleteness of single graph construction methods, we combine both KNN-based and semi-supervised graph construction approaches on each view, thereby enhancing the robustness of the data structure representation and reducing generalization error. Third, to effectively utilize abundant unlabeled data and enhance semantic alignment across multiple views, we propose a unified framework that integrates contrastive learning in order to enforce consistency among multi-view embeddings and capture meaningful inter-view relationships, together with pseudo-labeling, which provides additional supervision applied to both the cross-entropy and contrastive loss functions to enhance model generalization. Extensive experiments demonstrate that MV-SupGCN consistently surpasses state-of-the-art methods across multiple benchmarks, validating the effectiveness of our integrated approach. The source code is available at https://github.com/HuaiyuanXiao/MVSupGCN

</details>


### [30] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: SCPO 通过在参数空间投影梯度更新以满足未知安全约束，结合轨迹回放与光滑性界限构建局部安全域，使用凸二次规划实现安全的一阶步长；提供安全自归一性和在含稳定备份策略时的闭环稳定性，并在带有恶意监督的任务中表现出鲁棒性与可行性提升。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，需在不破坏安全前提下提升性能，但约束未知且通常不可直接对约束函数求梯度，因此需要直接在参数空间强制安全更新，确保训练过程的安全性与稳定性。

Method: 构建一个局部安全区域，结合轨迹回放与参数变化对安全度量的光滑界限；每次梯度更新通过一个凸二次规划（SOCP）投影，得到安全的一阶步长；给出从安全初始化出发的安全性归纳保证；在存在稳定性备份策略时，确保闭环稳定并支持对保守备份的安全扩展。

Result: 在带有有害监督的回归任务和带有恶意专家的受约束双积分任务中，SCPO 能 consistently 拒绝不安全的更新，训练过程保持可行性，同时实现有意义的原始目标提升。

Conclusion: SCPO 提供了一种无需直接梯度访问就可 enforcing 的约束安全学习方法，具备安全自归一性和闭环稳定性保证，并在实际任务中展现出鲁棒性与有效的目标优化。

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [31] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: CTVP提出基于语义轨道分析的代码生成安全控制框架，通过对模型在等价程序变换中的执行轨迹预测进行一致性分析，以检测潜在后门，并引入ARQ来量化验证成本，给出信息论界限，表明对手训练无法降低成本。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型日益生成代码且缺乏充分人类监督的环境下，需要一个可扩展且理论有据的安全控制方案，用以发现并抑制后门和恶意行为，确保代码生成的安全性与可控性。

Method: 提出“语义轨道”(semantic orbit)的概念：对等价的程序变换集合进行分析，要求模型预测不同轨道上的执行轨迹并寻找预测不一致的模式以暴露后门；不直接执行代码，而是基于模型的预测轨迹进行验证。引入ARQ（Adversarial Robustness Quotient）来衡量验证成本相对于基线生成的计算开销，并证明成本随轨道大小呈指数级增长。给出信息论层面的边界，论证对手通过训练无法降低成本（非对赌性）。

Result: 提出CTVP框架、ARQ指标以及非博弈性（non-gamifiability）的理论证明；在概念层面对代码生成安全性提供一种可扩展且有理论支撑的控制方法。

Conclusion: 语义轨道分析为AI代码生成的安全控制提供了一种可扩展、理论扎实的方法，可用于对未可信生成模型的行为进行监控、评估与约束。

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [32] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 提出一个两段式框架来改进 RLHF 的对齐：第一段通过后验解释解释不满意输出，找出最接近的训练数据子集，使输入输出对在这些数据的凸表示中可表示；第二段通过“去学习”移除导致不满意回答的训练数据，同时尽量不损害对其他提示的正确性，从而提升整体 RLHF。


<details>
  <summary>Details</summary>
Motivation: 人类纠错的思维是先找出原因再纠正。尽管对话模型经 RLHF 已经进行微调，但仍可能产生不满意输出，因此需要通过分析训练数据对输出的影响来改进对齐。

Method: 第一部分：后验解释。将解释问题建模为一个约束的组合优化：在特征空间中寻找一组训练数据，使得目标 prompt- risposta（输出）对尽量接近该集合，并且该对可以被表示为该集合的凸组合。给出一个高效的迭代数据选择算法以求解该问题。第二部分：unlearning。通过移除导致不满意输出的训练数据，同时尽量保持对其他提示的满意输出，达到“注释掉”不良数据的效果。

Result: 实验显示该算法能够提升 RLHF 的表现，即在输出质量或对齐方面取得改进。

Conclusion: 基于纠错思维的两阶段框架为 RLHF 对齐提供了一种可操作路径：先通过后验解释识别造成不良输出的训练数据，再通过数据去学习降低这些来源的影响，从而改善整体性能，同时尽量保持对良好输出的保持。

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [33] [Topologically-Stabilized Graph Neural Networks: Empirical Robustness Across Domains](https://arxiv.org/abs/2512.13852)
*Jelena Losic*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Neural Networks (GNNs) have become the standard for graph representation learning but remain vulnerable to structural perturbations. We propose a novel framework that integrates persistent homology features with stability regularization to enhance robustness. Building on the stability theorems of persistent homology \cite{cohen2007stability}, our method combines GIN architectures with multi-scale topological features extracted from persistence images, enforced by Hiraoka-Kusano-inspired stability constraints. Across six diverse datasets spanning biochemical, social, and collaboration networks , our approach demonstrates exceptional robustness to edge perturbations while maintaining competitive accuracy. Notably, we observe minimal performance degradation (0-4\% on most datasets) under perturbation, significantly outperforming baseline stability. Our work provides both a theoretically-grounded and empirically-validated approach to robust graph learning that aligns with recent advances in topological regularization

</details>


### [34] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 提出非渐近、分布无关的L1校准误差分析：在校准函数有界变差的前提下给出上界，并给出一种对任意分类器的高效修改方法以确保校准误差上界，同时给出实践中的测量建议。


<details>
  <summary>Details</summary>
Motivation: 从有限数据中估计二分类器的L1校准误差具有挑战性，需要非渐近、分布无关的保证，且希望有可落地的修改策略和测量建议。

Method: （1）对任何校准函数具有有界变差的分类器，给出L1校准误差的上界；（2）提出对任意分类器的修改方案，使其校准误差可被上界，同时尽量不显著影响分类性能，且不依赖额外假设。所有结果均为非渐近且分布无关。

Result: 给出非渐近、分布无关的上界，以及可实现的修改方法和实际可运行的流程，适用于现实数据集且开销较小。

Conclusion: 并给出在实践中测量校准误差的建议，提供可落地、可执行的流程用于实际数据集的校准评估与调整。

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [35] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 提出一个端到端婴儿啼哭分析管线，通过去噪自编码器、卷积分词器和Transformer编码器，结合联邦学习，实现隐私友好、抗噪和低通信量的婴儿啼哭分类，具备边缘推理和OOD abstention。


<details>
  <summary>Details</summary>
Motivation: 隐私保护和现实环境中的背景噪声，以及不同录音环境导致的领域偏移，是婴儿啼哭分类在实际部署中的主要挑战。

Method: 集成DAE、卷积tokenizer、Transformer编码器，端到端架构；在边缘设备实现去噪、分段、后验校准和基于能量的OOD abstention；采用带8位适配器增量的正则化对比变分更新的安全聚合联邦学习；数据使用 Baby Chillanto、Donate-a-Cry、ESC-50噪声叠加；边缘推理在Jetson Nano上 96 ms/1s spectrogram frame。

Result: 宏F1 0.938、AUC 0.962、ECE 0.032；每轮客户端上报从约36-42 MB降至3.3 MB；实时边缘推理在Jetson Nano上实现96 ms/1s spectrogram frame。

Conclusion: 展示了隐私保留、抗噪、通信高效的婴儿啼哭分类，在联邦部署中具备实际可行性。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [36] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: OPTIMA 是一种实用的单次后训练剪枝方法，通过在掩码选择后对每行进行独立的行级二次规划（QPs），并共享同一层海森矩阵来获得全局最优的逐行更新，从而实现可扩展的一次性剪枝，同时在不微调的情况下实现高效的准确性提升。


<details>
  <summary>Details</summary>
Motivation: 后训练剪枝面临速度与准确性的折中：简单启发式的零权重方法快速但精度下降，联合优化方法虽然有效但在现代大规模模型上不可实现。需要在准确性与可扩展性之间取得平衡。

Method: 将层级权重重建与掩码选择分开，视为独立的逐行QPs，使用共享的层级海森矩阵；通过一个加速器友好的求解器，对每层累积一个海森矩阵，批量求解大量小QPs，完成单次剪枝；兼容现有掩码选择器；在NVIDIA H100等硬件上实现端到端剪枝，且无需微调。

Result: 实现了最高 3.97% 的绝对准确度提升；在单个加速器上可在 40 小时内完成对一个 80 亿参数的 Transformer 的端到端剪枝，峰值显存 60GB；在多种LLM家族和稀疏度下，显著提升零-shot 性能，树立了新一代的一次性后训练剪枝的准确性-效率权衡。

Conclusion: 提出了一个可扩展且高效的一次性后训练剪枝框架，显著提升准确性并在单机加速器上实现规模化剪枝，确立了新的最优权衡。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [37] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 通过针对上下文进行少量的上下文特定训练，超过当前推理时策略（如生成更多思考标记）的效果，在长上下文任务中实现显著提升。


<details>
  <summary>Details</summary>
Motivation: 长上下文的语言模型在更长文本上未必能有效利用信息，静态自注意力导致分数稀释，推理阶段的额外计算（生成思考标记）效果递减且对长上下文无效，因此需要更有效的推理资源分配策略。

Method: 提出一种简单的方法，在给定上下文上进行定向梯度更新，以解决静态自注意力的分数分散问题，从而实现对上下文信号的更有效检索与利用。该方法与现有的推理时策略不同，不是简单地生成更多推理标记，而是对上下文进行微调以适应任务。

Result: 在Qwen3-4B模型上，对LongBench-v2和ZeroScrolls子集的平均提升分别达到12.6和14.1个百分点，且在多个模型与长上下文基准上表现出显著的改进。

Conclusion: 对于长上下文任务，少量的上下文特定训练比当前的推理时扩展（如产生更多思考标记）更有效地利用推理计算资源。对静态自注意力的局限性进行了诊断，并给出一种能在推理阶段实现持续改进的可验证方法。

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [38] [Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America](https://arxiv.org/abs/2512.13910)
*Matheus Corrêa Domingos,Valdivino Alexandre de Santiago Júnior,Juliana Aparecida Anochi,Elcio Hideiti Shiguemori,Luísa Mirelle Costa dos Santos,Hércules Carlos dos Santos Pereira,André Estevam Costa Oliveira*

Main category: cs.LG

TL;DR: DL/ML竞争中，LSTM表现最佳，传统动态模型BAM最差；XGBoost响应更快但略逊于准确性；总体上DL对气候预报可行，显示数据驱动方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 检视纯数据驱动的方法在降水预报中的可行性，比较经典机器学习、深度学习与传统动态模型的表现，并评估解释性AI的作用。

Method: 对比多种建模方法：经典ML（随机森林、XGBoost）、DL（1D-CNN、LSTM、GRU），以及传统动态模型BAM。数据来自2019年南美降水序列；并使用可解释性AI（XAI）对模型行为进行解释。评估指标涉及预测精度与延迟（XGBoost的低延迟与精度权衡）。

Result: LSTM在预测性能上领先，BAM作为动态模型表现最差。对于强降水，LSTM最为准确；若成本/延迟重要，XGBoost提供更低延迟但略低的准确性。研究证实DL在气候预测中的可行性，与全球气象机构的趋势一致。

Conclusion: 数据驱动的DL/ML方法具有实际可行性，尤其在强降水情景中优势显著，且DL的应用正成为气象和气候预测的全球趋势的一部分；未来研究可扩展数据集、优化模型与解释性分析，以提升对复杂气象过程的理解与预测能力。

Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.

</details>


### [39] [Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations](https://arxiv.org/abs/2512.13913)
*Patrick Egenlauf,Iva Březinová,Sabine Andergassen,Miriam Klopotek*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.

</details>


### [40] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 提出了滑动窗口递归（SWR）和Phalanx层，通过与GPU内存层次结构对齐的分层分解框架，针对线性递归进行硬件友好截断。实现可直接替代窗口化注意力或线性递归的Phalanx，在1B参数的多混合模型上，对4K–32K上下文长度实现10–40%的加速，同时保持困惑度一致性。


<details>
  <summary>Details</summary>
Motivation: 提升大语言模型的计算效率，特别是在多混合架构中，使递归和注意力计算更好地与GPU内存层次结构相匹配，减少跨-warp通信的开销。

Method: 提出对线性递归的分层分解框架，将递归截断为硬件对齐的窗口，形成自然的锯齿状边界；据此设计滑动窗口递归（SWR）和Phalanx层，作为窗口化注意力或线性递归的即插即用替代方案。

Result: 在1B参数的多混合模型中，Phalanx在4K到32K的上下文长度范围内，较优化的Transformer实现获得10%–40%的加速，且保持困惑度相近。

Conclusion: SWR和Phalanx为硬件对齐的递归计算提供了有效的加速方案，能在不显著损害模型质量的前提下提升大规模语言模型的推理和训练速度，具备对其他架构的推广潜力。

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [41] [A Complete Guide to Spherical Equivariant Graph Transformers](https://arxiv.org/abs/2512.13927)
*Sophia Tang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Spherical equivariant graph neural networks (EGNNs) provide a principled framework for learning on three-dimensional molecular and biomolecular systems, where predictions must respect the rotational symmetries inherent in physics. These models extend traditional message-passing GNNs and Transformers by representing node and edge features as spherical tensors that transform under irreducible representations of the rotation group SO(3), ensuring that predictions change in physically meaningful ways under rotations of the input. This guide develops a complete, intuitive foundation for spherical equivariant modeling - from group representations and spherical harmonics, to tensor products, Clebsch-Gordan decomposition, and the construction of SO(3)-equivariant kernels. Building on this foundation, we construct the Tensor Field Network and SE(3)-Transformer architectures and explain how they perform equivariant message-passing and attention on geometric graphs. Through clear mathematical derivations and annotated code excerpts, this guide serves as a self-contained introduction for researchers and learners seeking to understand or implement spherical EGNNs for applications in chemistry, molecular property prediction, protein structure modeling, and generative modeling.

</details>


### [42] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [43] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [44] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [45] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [46] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: 提出了 SonicMoE，一种内存高效的MoE前向/反向计算算法，配合GPU内核实现与记忆 IO 的重叠，以及一种“token rounding”的新方法以减少分组 GEMM 的填充浪费。相较于 ScatterMoE 的 BF16 MoE内核，在高专家粒度和高稀疏性MoE场景下显著提升吞吐与显存利用率。


<details>
  <summary>Details</summary>
Motivation: Mixture of Experts (MoE) 模型通过提高专家粒度和稀疏性来提升每单位 FLOP 的模型质量，但细粒度MoE会增加激活记忆占用与 IO 成本，且高稀疏MoE会在分组 GEMM 中产生填充浪费。需要一种在保持精度的同时降低激活缓存、降低 IO 成本并减少填充浪费的解决方案，以提升训练吞吐和硬件效率。

Method: 提出一个内存高效的前向与反向计算算法，尽量减少反向对激活缓存的依赖；设计可覆盖所有 MoE 架构的 GPU 内核，实现 IO 与计算的重叠；提出“token rounding”方法，减小分组 GEMM 中的填充浪费，并在高稀疏场景下通过 tile-aware 的令牌舍入进一步加速；将这些内核开放源代码。

Result: 在 Hopper GPU 上相较于 ScatterMoE 的 BF16 MoE 内核，SonicMoE 将激活内存降低约45%，实现约1.86x 的计算吞吐提升；在 64 个 H100 上的训练吞吐达到 213B token/日，接近 96H100 的 225B token/日（用于 7B MoE、FSDP-2、lm-engine 代码库）；在高稀疏性设置下，tile-aware token rounding 相比常规 top-K 路由在内核执行时间上再提升约1.16x，同时保持下游性能；并开放源代码。

Conclusion: SonicMoE 提供一套能显著降低激活内存、提升吞吐的 MoE 训练方案，覆盖所有 MoE 架构并通过更高效的内核实现提升硬件利用率，且具备实际可落地的开源实现，促进大规模 MoE 的高效训练。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [47] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: 通过将 Any-to-Any 多模态模型拆分为可部署的组件并进行自动规划，Cornserve 在在线服务中实现了对异质计算路径的高效处理，显著提升吞吐量并降低尾延时。


<details>
  <summary>Details</summary>
Motivation: 新兴的 Any-to-Any 模型同时支持文本和多模态数据的输入/输出，导致计算路径和扩展性具有异质性，现有服务框架难以高效地按需求分解与调度各组件。需要一个能够描述计算图并自动规划部署、分解组件的系统来提高在线服务性能。

Method: 提出 Cornserve：一个用于描述通用 Any-to-Any 模型计算图的框架，组件包括多模态编码器、自回归模型（如大语言模型）以及多模态生成器（如 Diffusion Transformers）。其规划器自动在模型与工作负载特征基础上生成优化的部署计划，决定是否以及如何将模型分解成更小的组件；分布式运行时根据计划执行，在线服务中有效处理异质性。

Result: 评估显示 Cornserve 能高效地服务多样化的 Any-to-Any 模型和工作负载，相比现有解决方案，吞吐量提升可达 3.81 倍，尾延时下降可达 5.79 倍。

Conclusion: Cornserve 能有效服务多样化的 Any-to-Any 模型与工作负载，为未来多模态 AI 系统的在线部署提供了可扩展、低延迟的解决方案。

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [48] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 提出了一种基于逻辑相似度的奖励机制S-GRPO，作为对传统奖励建模的替代，通过逻辑一致性引导模型对齐人类偏好，并在GRPO框架上增加监督分支以避免多视角下的崩溃。实验显示S-GRPO优于SFT并提升鲁棒性，代码开源。


<details>
  <summary>Details</summary>
Motivation: RLHF中奖励模型质量直接决定对齐效果，PPO等方法对奖励模型高度依赖；现实问题往往具有多视角解释，单一奖励信号易导致不稳定或崩溃；因此需要一种基于逻辑一致性的稳定对齐机制，并通过监督变体提升鲁棒性。

Method: 提出基于逻辑相似度的奖励机制，用形式逻辑的一致性来引导模型对齐；在GRPO框架基础上设计S-GRPO，加入额外的监督组件，联合优化生成项、KL正则化和标签-based目标，以避免在多视角场景下的崩溃。

Result: 实验结果显示，S-GRPO在性能和鲁棒性方面持续优于标准的SFT，并可扩展至GRPO、DPO等偏好学习框架，提供更灵活、任务自适应的对齐训练路径；代码公开。

Conclusion: 基于逻辑一致性的奖励机制结合监督化GRPO能提升对齐的稳定性和性能，并对其他偏好学习框架具有良好的迁移性，适用于更广泛的对齐任务。

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [49] [PathFinder: Advancing Path Loss Prediction for Single-to-Multi-Transmitter Scenario](https://arxiv.org/abs/2512.14150)
*Zhijie Zhong,Zhiwen Yu,Pengyu Li,Jianming Lv,C. L. Philip Chen,Min Chen*

Main category: cs.LG

TL;DR: PathFinder提出一种用于射频路径损耗预测的新架构，通过主动建模建筑与发射器、引入Mask-Guided Low-rank Attention，以及发射器导向混合训练策略，并建立单对多发射器RPP基准S2MT-RPP，在多发射器场景下显著提升泛化与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的RPP方法在环境建模上往往是被动的，忽视发射器信息和关键环境特征，且难以处理真实世界中的多发射器场景与分布偏移；训练和测试环境在建筑密度、发射器配置等方面存在差异，导致泛化性能不足。

Method: 提出PathFinder架构，通过拆解特征编码来主动建模建筑与发射器；引入Mask-Guided Low-rank Attention，能够在接收端区域和建筑区域之间独立聚焦以提升感知能力；采用Transmitter-Oriented Mixup进行鲁棒训练；新基准S2MT-RPP用于评估单发射器训练下的多发射器外推性能。

Result: 在多发射器场景下，PathFinder显著优于现有方法，尤其在跨发射器的挑战性场景中表现突出。

Conclusion: PathFinder及其伴随的训练策略和基准为RPP的跨分布鲁棒性和对多发射器的适应性提供了有效解决方案，代码与项目站点已公开。

Abstract: Radio path loss prediction (RPP) is critical for optimizing 5G networks and enabling IoT, smart city, and similar applications. However, current deep learning-based RPP methods lack proactive environmental modeling, struggle with realistic multi-transmitter scenarios, and generalize poorly under distribution shifts, particularly when training/testing environments differ in building density or transmitter configurations. This paper identifies three key issues: (1) passive environmental modeling that overlooks transmitters and key environmental features; (2) overemphasis on single-transmitter scenarios despite real-world multi-transmitter prevalence; (3) excessive focus on in-distribution performance while neglecting distribution shift challenges. To address these, we propose PathFinder, a novel architecture that actively models buildings and transmitters via disentangled feature encoding and integrates Mask-Guided Low-rank Attention to independently focus on receiver and building regions. We also introduce a Transmitter-Oriented Mixup strategy for robust training and a new benchmark, single-to-multi-transmitter RPP (S2MT-RPP), tailored to evaluate extrapolation performance (multi-transmitter testing after single-transmitter training). Experimental results show PathFinder outperforms state-of-the-art methods significantly, especially in challenging multi-transmitter scenarios. Our code and project site are available at: https://emorzz1g.github.io/PathFinder/.

</details>


### [50] [On Improving Deep Active Learning with Formal Verification](https://arxiv.org/abs/2512.14170)
*Jonathan Spiegelman,Guy Amir,Guy Katz*

Main category: cs.LG

TL;DR: 将通过形式化验证生成的对抗样本引入深度主动学习（DAL）训练，以提升数据效率与泛化能力，相比常规基于梯度的对抗样本，这些对抗样本贡献更大，并适用于多种 DAL 策略与新方法。


<details>
  <summary>Details</summary>
Motivation: DAL 旨在在不增加人工标注成本的前提下提高模型性能，但仍然依赖于数据集的有效性。通过引入无需额外标注的合成输入，尤其是违背鲁棒性约束的对抗样本，可能进一步提升数据利用率和泛化能力。

Method: 使用通过形式化验证得到的对抗样本来扩充训练集，比较其与标准基于梯度的对抗攻击的效果。将该扩充策略应用于多种现有的 DAL 技术，以及作者提出的一个新技术，进行跨基准评估。

Result: 用形式化验证生成的对抗样本对 DAL 的提升显著高于梯度方式，并且在多个现代 DAL 技术以及新提出的技术上均带来泛化能力的显著改进，且在标准基准上表现良好。

Conclusion: 利用形式化验证所得的对抗输入可以显著提升 DAL 的数据效率和模型泛化，且具有广泛适用性，适用于不同的 DAL 框架和策略。

Abstract: Deep Active Learning (DAL) aims to reduce labeling costs in neural-network training by prioritizing the most informative unlabeled samples for annotation. Beyond selecting which samples to label, several DAL approaches further enhance data efficiency by augmenting the training set with synthetic inputs that do not require additional manual labeling. In this work, we investigate how augmenting the training data with adversarial inputs that violate robustness constraints can improve DAL performance. We show that adversarial examples generated via formal verification contribute substantially more than those produced by standard, gradient-based attacks. We apply this extension to multiple modern DAL techniques, as well as to a new technique that we propose, and show that it yields significant improvements in model generalization across standard benchmarks.

</details>


### [51] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 使用随机桥作为生成模型中的随机传输工具，在较少步骤内生成高质量样本，且计算成本低，FID表现竞争力。


<details>
  <summary>Details</summary>
Motivation: 在生成建模中需要高效的、灵活的随机传输机制；将分布之间的桥接视为受控条件化过程，支持马尔可夫/非马尔可夫、连续/离散或混合模式，以提升采样效率和表达能力。

Method: 从一般概率论出发建立随机桥的表示，并将其转化为可学习与仿真的信息处理框架；在具体实现中以高斯随机桥为例，给出学习和仿真算法的表示。

Result: 以高斯随机桥为基础的实验显示相比传统方法，样本在更少的步骤中就能达到高质量，同时在Frechet Inception Distance (FID) 上具有竞争力；框架在计算成本上更为低廉，适合高速生成任务。

Conclusion: 随机桥框架为生成建模提供了一种高效、灵活的随机传输机制，能够在多种模式下实现快速且质量可观的样本生成。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [52] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: 提出一种称为 LLM compare 的难度估计方法：通过让大语言模型进行成对难度比较，并基于 Bradley-Terry 分数计算难度，具备连续性、模型无关性、无需 ground truth 的优点，且与人类标注高度相关并对幻觉具有鲁棒性；可用于 curriculum design、模型评估和 AI 辅助研究创意。


<details>
  <summary>Details</summary>
Motivation: 当前关于数据难度的估计方法（如人工校准或基于性能的评分）难以推广到分布外的问题，成本高、不可扩展且依赖 Ground Truth。需要一种可扩展、无 Ground Truth、能适应未解决问题的难度估计方法以支撑合成数据生成和模型评估。

Method: 让 LLM 进行成对难度比较，基于比较结果应用 Bradley-Terry 模型得到难度分数；提出一个三平面正交框架（构建、尺度、依赖性），定位现有度量在这三个维度上的位置，识别出可覆盖分布外问题的象限。 LLМ compare 困难在于：连续性、模型无关性、独立于 Ground Truth；并在实验中与人类标注高相关性（Pearson r ≥ 0.80，n=1876）以及对幻觉的鲁棒性（10% 注入噪声时相关性下降＜6%）。

Result: LLM compare 与人类标注高度一致，且对噪声具有鲁棒性，展示出了作为替代人工标注和合成数据生成的潜力，并能直接用于课程设计、模型评估和 AI 辅助研究思路生成。

Conclusion: 该方法代表了在大规模语言模型微调数据生成中替代耗时人工标注的一大步，具有在课程设计、评估以及 AI 辅助研究构思中的重要应用潜力。

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [53] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 提出在学习反应-扩散(RD)系统时嵌入质量守恒与准正性的正则化框架，并给出学习解的收敛性与唯一性结果。


<details>
  <summary>Details</summary>
Motivation: 解决数据驱动RD模型缺乏物理一致性的问题，确保非负性、质量守恒，并提升模型的可解释性与稳健性。

Method: 系统性修改参数化反应项使其内在满足质量守恒与准正性；在该物理一致的前提下扩展正则化学习理论，证明极限系统的解存在唯一性并收敛到正则化极小解；给出准正性函数的近似结果。

Result: 得到具物理一致性的RD学习模型；证明解的存在性、唯一性与收敛性；提供构造物理一致参数化的近似工具。

Conclusion: 为数据驱动RD系统提供更具解释性和可靠性的建模框架，确保遵循基本物理规律并保持良态性与稳健性。

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [54] [Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning](https://arxiv.org/abs/2512.14241)
*Salvatore Romano,Marco Grassia,Giuseppe Mangioni*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.

</details>


### [55] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: FLAME 是一组极其轻量且强大的时间序列基础模型，支持确定性与概率性预测。通过 Legendre Memory 的变体（LegT、LegS）在编码/解码阶段，以及基于正态化流的预测头，实现高效且鲁棒的生成式 probabilistic forecasting；在 TSFM-Bench、ProbTS 上实现零样本状态最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决高效、鲁棒的时间序列预测需求，兼顾长距离推断和不确定性建模；通过轻量化结构和 Legendre memory 提高泛化能力和推断效率。

Method: 在编码/解码阶段融入 Legendre Memory 的 LegT 与 LegS 变体；采用基于正态化流的 forecasting head 来建模复杂分布；在 TSFM-Bench、ProbTS 等基准上进行零样本评估。

Result: 在确定性和概率性预测任务上实现持续的零样本最优性能（state-of-the-art zero-shot performance）。

Conclusion: FLAME 提供一种高效、鲁棒的时间序列基模型，结合强的归纳偏置和生成式建模，能够执行长距离推断并得到高质量的不确定性预测。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [56] [Implicit Bias and Invariance: How Hopfield Networks Efficiently Learn Graph Orbits](https://arxiv.org/abs/2512.14338)
*Michael Murray,Tenzin Chan,Kedar Karhadker,Christopher J. Hillar*

Main category: cs.LG

TL;DR: Hopfield networks can infer graph isomorphism classes from small samples; isomorphism classes live in a 3D invariant subspace; a gradient-descent energy-flow bias favors norm-efficient solutions, enabling polynomial sample complexity; parameters converge toward invariance subspace as data grows, suggesting norm-efficiency as a generalization mechanism under group-structured data.


<details>
  <summary>Details</summary>
Motivation: To understand how invariance to symmetries can emerge from training on group-structured data, by analyzing classical Hopfield networks and their learning dynamics.

Method: The paper analyzes Hopfield networks, examines their ability to learn graph isomorphism classes from limited samples, introduces gradient descent to minimize energy flow (MEF) as a learning rule, and studies the representation of isomorphism classes in an invariant subspace and the convergence of parameters across different learning rules as sample size grows.

Result: 1) Graph isomorphism classes can be represented within a three-dimensional invariant subspace. 2) MEF induces an implicit bias toward norm-efficient solutions, yielding a polynomial bound on sample complexity for learning isomorphism classes. 3) Across multiple learning rules, parameters converge toward the invariant subspace as sample size increases.

Conclusion: A unifying mechanism for generalization in Hopfield networks emerges: bias toward norm efficiency during learning drives the appearance of approximate invariance under group-structured data.

Abstract: Many learning problems involve symmetries, and while invariance can be built into neural architectures, it can also emerge implicitly when training on group-structured data. We study this phenomenon in classical Hopfield networks and show they can infer the full isomorphism class of a graph from a small random sample. Our results reveal that: (i) graph isomorphism classes can be represented within a three-dimensional invariant subspace, (ii) using gradient descent to minimize energy flow (MEF) has an implicit bias toward norm-efficient solutions, which underpins a polynomial sample complexity bound for learning isomorphism classes, and (iii) across multiple learning rules, parameters converge toward the invariant subspace as sample sizes grow. Together, these findings highlight a unifying mechanism for generalization in Hopfield networks: a bias toward norm efficiency in learning drives the emergence of approximate invariance under group-structured data.

</details>


### [57] [Causal Structure Learning for Dynamical Systems with Theoretical Score Analysis](https://arxiv.org/abs/2512.14361)
*Nicholas Tagliapietra,Katharina Ensinger,Christoph Zimmer,Osman Mian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Real world systems evolve in continuous-time according to their underlying causal relationships, yet their dynamics are often unknown. Existing approaches to learning such dynamics typically either discretize time -- leading to poor performance on irregularly sampled data -- or ignore the underlying causality. We propose CaDyT, a novel method for causal discovery on dynamical systems addressing both these challenges. In contrast to state-of-the-art causal discovery methods that model the problem using discrete-time Dynamic Bayesian networks, our formulation is grounded in Difference-based causal models, which allow milder assumptions for modeling the continuous nature of the system. CaDyT leverages exact Gaussian Process inference for modeling the continuous-time dynamics which is more aligned with the underlying dynamical process. We propose a practical instantiation that identifies the causal structure via a greedy search guided by the Algorithmic Markov Condition and Minimum Description Length principle. Our experiments show that CaDyT outperforms state-of-the-art methods on both regularly and irregularly-sampled data, discovering causal networks closer to the true underlying dynamics.

</details>


### [58] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 提出了基于 Lifted Quantum Differential Privacy 的黑箱隐私审计框架，用量子窃取子(canaries)检测记忆化并量化训练过程中的隐私泄漏；在量子态偏移与迹距离之间建立严格联系，给出实际隐私预算消耗的下界；在模拟和实际量子硬件上验证框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 量子机器学习在处理敏感数据时易出现对个体记录的记忆化，造成隐私风险；虽然理论上存在量子差分隐私（QDP）保障，但缺乏用于已部署模型的实证验证工具，需要可操作的隐私审计方法来桥接理论与实践。

Method: 提出首个面向量子机器学习的黑箱隐私审计框架，基于 Lifted QDP，并通过量子窃取子（quantum canaries，经过特殊偏移编码的量子态）来检测记忆化；建立 canary 偏移与迹距离界之间的严格数学关系，推导出关于隐私预算消耗的经验下界；在模拟与实际量子硬件上进行全面评估。

Result: 框架能够测量实际的隐私损失，给出对隐私预算消耗的经验下界，提供可操作的隐私验证工具，验证结果支持在 QML 系统中进行鲁棒隐私验证。

Conclusion: 弥合理论与实践之间的差距，首次提供面向 QML 的黑箱隐私审计框架及可操作的审计工具，为 QML 系统的隐私验证提供实证能力。

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [59] [RePo: Language Models with Context Re-Positioning](https://arxiv.org/abs/2512.14391)
*Huayang Li,Tianyu Zhao,Richard Sproat*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In-context learning is fundamental to modern Large Language Models (LLMs); however, prevailing architectures impose a rigid and fixed contextual structure by assigning linear or constant positional indices. Drawing on Cognitive Load Theory (CLT), we argue that this uninformative structure increases extraneous cognitive load, consuming finite working memory capacity that should be allocated to deep reasoning and attention allocation. To address this, we propose RePo, a novel mechanism that reduces extraneous load via context re-positioning. Unlike standard approaches, RePo utilizes a differentiable module, $f_φ$, to assign token positions that capture contextual dependencies, rather than replying on pre-defined integer range. By continually pre-training on the OLMo-2 1B backbone, we demonstrate that RePo significantly enhances performance on tasks involving noisy contexts, structured data, and longer context length, while maintaining competitive performance on general short-context tasks. Detailed analysis reveals that RePo successfully allocate higher attention to distant but relevant information, assign positions in dense and non-linear space, and capture the intrinsic structure of the input context. Our code is available at https://github.com/SakanaAI/repo.

</details>


### [60] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: 提出 GRAFT：一种将文本信息对齐并与半小时负载进行文本引导融合的网格感知 forecasting 框架，扩展 STanHOP，具备外部记忆接口，并在 2019-2021 的澳大利亚五州基准上实现多时尺度的鲁棒预测与可解释性。并提供可重复的基线评估资源。


<details>
  <summary>Details</summary>
Motivation: 电力负载受天气、日历、突发事件、政策等多时间尺度的外部因素影响，需要整合来自新闻、社媒、政策文本等多源信息以提升预测准确性并实现时间定位解释。

Method: 提出 GRAFT 框架，对文本与负载进行严格对齐并通过跨注意力进行文本引导的融合；引入可插拔的外部记忆接口以支持不同信息源；基于 STanHOP 的改进，兼容日对齐的文本与半小时负载的对齐，训练与滚动预测阶段均应用文本-载荷对齐与融合；构建并公开一个覆盖 2019-2021 的五州统一基准，包含半小时负载、日对齐的天气/日历变量和三类外部文本，跨三个尺度进行评测。

Result: 实验表明 GRAFT 在多个区域和预测区间显著优于强基线，达到或超过现有方法的水平，且对事件驱动场景具有鲁棒性；通过注意力读出实现对文本到负载效应的时序定位和源头级解释。还公开基准、预处理脚本和预测结果以促进可重复性。

Conclusion: GRAFT 证明了将多源文本信息对齐并与半小时载荷结合进行网格感知预测的有效性与可解释性，并提供可部署的外部记忆接口和统一评测资源，推动电网负载预测的跨区域、跨源、跨尺度研究。

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [61] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [62] [AnySleep: a channel-agnostic deep learning system for high-resolution sleep staging in multi-center cohorts](https://arxiv.org/abs/2512.14461)
*Niklas Grieger,Jannik Raskob,Siamak Mehrkanoon,Stephan Bialonski*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sleep is essential for good health throughout our lives, yet studying its dynamics requires manual sleep staging, a labor-intensive step in sleep research and clinical care. Across centers, polysomnography (PSG) recordings are traditionally scored in 30-s epochs for pragmatic, not physiological, reasons and can vary considerably in electrode count, montage, and subject characteristics. These constraints present challenges in conducting harmonized multi-center sleep studies and discovering novel, robust biomarkers on shorter timescales. Here, we present AnySleep, a deep neural network model that uses any electroencephalography (EEG) or electrooculography (EOG) data to score sleep at adjustable temporal resolutions. We trained and validated the model on over 19,000 overnight recordings from 21 datasets collected across multiple clinics, spanning nearly 200,000 hours of EEG and EOG data, to promote robust generalization across sites. The model attains state-of-the-art performance and surpasses or equals established baselines at 30-s epochs. Performance improves as more channels are provided, yet remains strong when EOG is absent or when only EOG or single EEG derivations (frontal, central, or occipital) are available. On sub-30-s timescales, the model captures short wake intrusions consistent with arousals and improves prediction of physiological characteristics (age, sex) and pathophysiological conditions (sleep apnea), relative to standard 30-s scoring. We make the model publicly available to facilitate large-scale studies with heterogeneous electrode setups and to accelerate the discovery of novel biomarkers in sleep.

</details>


### [63] [Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics](https://arxiv.org/abs/2512.14471)
*Additi Pandey,Liang Wei,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.

</details>


### [64] [Improving Slow Transfer Predictions: Generative Methods Compared](https://arxiv.org/abs/2512.14522)
*Jacob Taegon Kim,Alex Sim,Kesheng Wu,Jinoh Kim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Monitoring data transfer performance is a crucial task in scientific computing networks. By predicting performance early in the communication phase, potentially sluggish transfers can be identified and selectively monitored, optimizing network usage and overall performance. A key bottleneck to improving the predictive power of machine learning (ML) models in this context is the issue of class imbalance. This project focuses on addressing the class imbalance problem to enhance the accuracy of performance predictions. In this study, we analyze and compare various augmentation strategies, including traditional oversampling methods and generative techniques. Additionally, we adjust the class imbalance ratios in training datasets to evaluate their impact on model performance. While augmentation may improve performance, as the imbalance ratio increases, the performance does not significantly improve. We conclude that even the most advanced technique, such as CTGAN, does not significantly improve over simple stratified sampling.

</details>


### [65] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [66] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [67] [Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection](https://arxiv.org/abs/2512.14563)
*Tejaswani Dash,Gautam Datla,Anudeep Vurity,Tazeem Ahmad,Mohd Adnan,Saima Rafi,Saisha Patro,Saina Patro*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

</details>


### [68] [Hierarchical Persistence Velocity for Network Anomaly Detection: Theory and Applications to Cryptocurrency Markets](https://arxiv.org/abs/2512.14615)
*Omid Khormali*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce the Overlap-Weighted Hierarchical Normalized Persistence Velocity (OW-HNPV), a novel topological data analysis method for detecting anomalies in time-varying networks. Unlike existing methods that measure cumulative topological presence, we introduce the first velocity-based perspective on persistence diagrams, measuring the rate at which features appear and disappear, automatically downweighting noise through overlap-based weighting. We also prove that OW-HNPV is mathematically stable. It behaves in a controlled, predictable way, even when comparing persistence diagrams from networks with different feature types. Applied to Ethereum transaction networks (May 2017-May 2018), OW-HNPV demonstrates superior performance for cryptocurrency anomaly detection, achieving up to 10.4% AUC gain over baseline models for 7-day price movement predictions. Compared with established methods, including Vector of Averaged Bettis (VAB), persistence landscapes, and persistence images, velocity-based summaries excel at medium- to long-range forecasting (4-7 days), with OW-HNPV providing the most consistent and stable performance across prediction horizons. Our results show that modeling topological velocity is crucial for detecting structural anomalies in dynamic networks.

</details>


### [69] [Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes](https://arxiv.org/abs/2512.14617)
*Alessandro Trapasso,Luca Iocchi,Fabio Patrizi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

</details>


### [70] [ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning](https://arxiv.org/abs/2512.14619)
*Chaohao Yuan,Zhenjie Song,Ercan Engin Kuruoglu,Kangfei Zhao,Yang Liu,Deli Zhao,Hong Cheng,Yu Rong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

</details>


### [71] [gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation](https://arxiv.org/abs/2512.14658)
*Alban Puech,Matteo Mazzonelli,Celia Cintas,Tamara R. Govindasamy,Mangaliso Mngomezulu,Jonas Weiss,Matteo Baù,Anna Varbella,François Mirallès,Kibaek Kim,Le Xie,Hendrik F. Hamann,Etienne Vos,Thomas Brunschwiler*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.

</details>


### [72] [Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks](https://arxiv.org/abs/2512.14675)
*Rae Chipera,Jenny Du,Irene Tsapara*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.

</details>


### [73] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>
