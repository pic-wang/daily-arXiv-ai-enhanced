<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 6]
- [cs.LG](#cs.LG) [Total: 64]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Overspecified Mixture Discriminant Analysis: Exponential Convergence, Statistical Guarantees, and Remote Sensing Applications](https://arxiv.org/abs/2510.27056)
*Arman Bolatov,Alan Legg,Igor Melnykov,Amantay Nurlanuly,Maxat Tezekbayev,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 对overspecified的多成分混合判别分析（MDA）在每类用两成分高斯混合拟合、数据实际来自单成分高斯的情形下的理论与实验分析。


<details>
  <summary>Details</summary>
Motivation: 研究拟合阶段超规格（overspecification）下的算法与统计性能：在多成分模型超过真实分布成分数的情形，EM算法的收敛性及分类误差行为尚未清晰。

Method: 构建在每类数据中使用两成分高斯混合的模型来拟合真实单高斯数据，分析EM算法在总体层面的收敛到Bayes风险的速度，并给出有限样本情形下的分类误差收敛率。理论上给出充分条件下的指数收敛与n^{-1/2}的收敛率；并通过遥感数据集进行实验验证。

Result: 在合适的初始化下，EM算法在总体层面以指数速率收敛到Bayes风险；在有限样本条件下，分类误差以n^{-1/2}速率收敛到Bayes风险。

Conclusion: 为overspecified MDA的性能提供严格理论框架，解释了该方法在图像、文本等复杂数据场景中的应用现象，并通过遥感数据集验证理论。

Abstract: This study explores the classification error of Mixture Discriminant Analysis
(MDA) in scenarios where the number of mixture components exceeds those present
in the actual data distribution, a condition known as overspecification. We use
a two-component Gaussian mixture model within each class to fit data generated
from a single Gaussian, analyzing both the algorithmic convergence of the
Expectation-Maximization (EM) algorithm and the statistical classification
error. We demonstrate that, with suitable initialization, the EM algorithm
converges exponentially fast to the Bayes risk at the population level.
Further, we extend our results to finite samples, showing that the
classification error converges to Bayes risk with a rate $n^{-1/2}$ under mild
conditions on the initial parameter estimates and sample size. This work
provides a rigorous theoretical framework for understanding the performance of
overspecified MDA, which is often used empirically in complex data settings,
such as image and text classification. To validate our theory, we conduct
experiments on remote sensing datasets.

</details>


### [2] [Decreasing Entropic Regularization Averaged Gradient for Semi-Discrete Optimal Transport](https://arxiv.org/abs/2510.27340)
*Ferdinand Genans,Antoine Godichon-Baggioni,François-Xavier Vialard,Olivier Wintenberger*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Adding entropic regularization to Optimal Transport (OT) problems has become
a standard approach for designing efficient and scalable solvers. However,
regularization introduces a bias from the true solution. To mitigate this bias
while still benefiting from the acceleration provided by regularization, a
natural solver would adaptively decrease the regularization as it approaches
the solution. Although some algorithms heuristically implement this idea, their
theoretical guarantees and the extent of their acceleration compared to using a
fixed regularization remain largely open. In the setting of semi-discrete OT,
where the source measure is continuous and the target is discrete, we prove
that decreasing the regularization can indeed accelerate convergence. To this
end, we introduce DRAG: Decreasing (entropic) Regularization Averaged Gradient,
a stochastic gradient descent algorithm where the regularization decreases with
the number of optimization steps. We provide a theoretical analysis showing
that DRAG benefits from decreasing regularization compared to a fixed scheme,
achieving an unbiased $\mathcal{O}(1/t)$ sample and iteration complexity for
both the OT cost and the potential estimation, and a $\mathcal{O}(1/\sqrt{t})$
rate for the OT map. Our theoretical findings are supported by numerical
experiments that validate the effectiveness of DRAG and highlight its practical
advantages.

</details>


### [3] [On the Equivalence of Optimal Transport Problem and Action Matching with Optimal Vector Fields](https://arxiv.org/abs/2510.27385)
*Nikita Kornilov,Alexander Korotin*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Flow Matching (FM) method in generative modeling maps arbitrary probability
distributions by constructing an interpolation between them and then learning
the vector field that defines ODE for this interpolation. Recently, it was
shown that FM can be modified to map distributions optimally in terms of the
quadratic cost function for any initial interpolation. To achieve this, only
specific optimal vector fields, which are typical for solutions of Optimal
Transport (OT) problems, need to be considered during FM loss minimization. In
this note, we show that considering only optimal vector fields can lead to OT
in another approach: Action Matching (AM). Unlike FM, which learns a vector
field for a manually chosen interpolation between given distributions, AM
learns the vector field that defines ODE for an entire given sequence of
distributions.

</details>


### [4] [Interpretable Model-Aware Counterfactual Explanations for Random Forest](https://arxiv.org/abs/2510.27397)
*Joshua S. Harvey,Guanchao Feng,Sai Anusha Meesala,Tina Zhao,Dhagash Mehta*

Main category: stat.ML

TL;DR: 通过在随机森林表示中进行相似性搜索来生成可解释的反事实解释，结果比Shapley值更稀疏、也更具可操作性。


<details>
  <summary>Details</summary>
Motivation: 受监管行业对可解释性需求强烈，但常用的Shapley值并不总能对齐因果解释；反事实解释更直观、可操作，因此需要一种更符合因果直觉的解释框架。

Method: 在随机森林预测模型自身所学习的表示上进行相似性学习来寻找反事实样本。获得反事实后，解释的特征重要性通过从原始样本到反事实路径中跨越的随机森林分区数量来计算。将该方法在MNIST和德国信用数据集上进行演示。

Result: 该方法能生成比Shapley值更稀疏、且更有用的解释，并在MNIST与德国信用数据集上得到演示结果。

Conclusion: 基于随机森林表示的反事实搜索提供了一种更直观、可操作的解释途径，适用于需要清晰因果解释的应用场景。

Abstract: Despite their enormous predictive power, machine learning models are often
unsuitable for applications in regulated industries such as finance, due to
their limited capacity to provide explanations. While model-agnostic frameworks
such as Shapley values have proved to be convenient and popular, they rarely
align with the kinds of causal explanations that are typically sought after.
Counterfactual case-based explanations, where an individual is informed of
which circumstances would need to be different to cause a change in outcome,
may be more intuitive and actionable. However, finding appropriate
counterfactual cases is an open challenge, as is interpreting which features
are most critical for the change in outcome. Here, we pose the question of
counterfactual search and interpretation in terms of similarity learning,
exploiting the representation learned by the random forest predictive model
itself. Once a counterfactual is found, the feature importance of the
explanation is computed as a function of which random forest partitions are
crossed in order to reach it from the original instance. We demonstrate this
method on both the MNIST hand-drawn digit dataset and the German credit
dataset, finding that it generates explanations that are sparser and more
useful than Shapley values.

</details>


### [5] [Minimax-Optimal Two-Sample Test with Sliced Wasserstein](https://arxiv.org/abs/2510.27498)
*Binh Thuan Tran,Nicolas Schreuder*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We study the problem of nonparametric two-sample testing using the sliced
Wasserstein (SW) distance. While prior theoretical and empirical work indicates
that the SW distance offers a promising balance between strong statistical
guarantees and computational efficiency, its theoretical foundations for
hypothesis testing remain limited. We address this gap by proposing a
permutation-based SW test and analyzing its performance. The test inherits
finite-sample Type I error control from the permutation principle. Moreover, we
establish non-asymptotic power bounds and show that the procedure achieves the
minimax separation rate $n^{-1/2}$ over multinomial and bounded-support
alternatives, matching the optimal guarantees of kernel-based tests while
building on the geometric foundations of Wasserstein distances. Our analysis
further quantifies the trade-off between the number of projections and
statistical power. Finally, numerical experiments demonstrate that the test
combines finite-sample validity with competitive power and scalability, and --
unlike kernel-based tests, which require careful kernel tuning -- it performs
consistently well across all scenarios we consider.

</details>


### [6] [Optimal Convergence Analysis of DDPM for General Distributions](https://arxiv.org/abs/2510.27562)
*Yuchen Jiao,Yuchen Zhou,Gen Li*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Score-based diffusion models have achieved remarkable empirical success in
generating high-quality samples from target data distributions. Among them, the
Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used
samplers, generating samples via estimated score functions. Despite its
empirical success, a tight theoretical understanding of DDPM -- especially its
convergence properties -- remains limited.
  In this paper, we provide a refined convergence analysis of the DDPM sampler
and establish near-optimal convergence rates under general distributional
assumptions. Specifically, we introduce a relaxed smoothness condition
parameterized by a constant $L$, which is small for many practical
distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler
with accurate score estimates achieves a convergence rate of
$$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)~\text{in
Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the
number of iterations, and $\widetilde{O}$ hides polylogarithmic factors in $T$.
This result substantially improves upon the best-known $d^2/T^2$ rate when $L <
\sqrt{d}$. By establishing a matching lower bound, we show that our convergence
analysis is tight for a wide array of target distributions. Moreover, it
reveals that DDPM and DDIM share the same dependence on $d$, raising an
interesting question of why DDIM often appears empirically faster.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Learning Sparse Approximate Inverse Preconditioners for Conjugate Gradient Solvers on GPUs](https://arxiv.org/abs/2510.27517)
*Zherui Yang,Zhehao Li,Kangbo Lyu,Yixuan Li,Tao Du,Ligang Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The conjugate gradient solver (CG) is a prevalent method for solving
symmetric and positive definite linear systems Ax=b, where effective
preconditioners are crucial for fast convergence. Traditional preconditioners
rely on prescribed algorithms to offer rigorous theoretical guarantees, while
limiting their ability to exploit optimization from data. Existing
learning-based methods often utilize Graph Neural Networks (GNNs) to improve
the performance and speed up the construction. However, their reliance on
incomplete factorization leads to significant challenges: the associated
triangular solve hinders GPU parallelization in practice, and introduces
long-range dependencies which are difficult for GNNs to model. To address these
issues, we propose a learning-based method to generate GPU-friendly
preconditioners, particularly using GNNs to construct Sparse Approximate
Inverse (SPAI) preconditioners, which avoids triangular solves and requires
only two matrix-vector products at each CG step. The locality of matrix-vector
product is compatible with the local propagation mechanism of GNNs. The
flexibility of GNNs also allows our approach to be applied in a wide range of
scenarios. Furthermore, we introduce a statistics-based scale-invariant loss
function. Its design matches CG's property that the convergence rate depends on
the condition number, rather than the absolute scale of A, leading to improved
performance of the learned preconditioner. Evaluations on three PDE-derived
datasets and one synthetic dataset demonstrate that our method outperforms
standard preconditioners (Diagonal, IC, and traditional SPAI) and previous
learning-based preconditioners on GPUs. We reduce solution time on GPUs by
40%-53% (68%-113% faster), along with better condition numbers and superior
generalization performance. Source code available at
https://github.com/Adversarr/LearningSparsePreconditioner4GPU

</details>


### [8] [Layer of Truth: Probing Belief Shifts under Continual Pre-Training Poisoning](https://arxiv.org/abs/2510.26829)
*Svetlana Churina,Niranjan Chebrolu,Kokil Jaidka*

Main category: cs.LG

TL;DR: 提出 Layer of Truth 框架，通过在持续预训练中注入受污染数据，量化模型对重复暴露于虚假但自信陈述的信念偏移，以及跨层/模型规模的敏感性。


<details>
  <summary>Details</summary>
Motivation: 研究持续预训练中的信息污染及其对模型内部信念的影响，受 illusory truth effect 启发，探究重复暴露于虚假事实是否会使模型内部表征偏离真相。

Method: 提出 Layer of Truth 框架与数据集，在可控供给中注入受污染数据，跨检查点、跨模型规模、跨问题类型探测中间表征，量化信念漂移。

Result: 结果表明，即使最小程度的暴露也会造成对已建立事实的持久表征漂移，且对层级和模型规模的易感性存在差异。

Conclusion: 持续更新的大模型存在将错误信息内部化的风险，需要在模型更新过程中加强对事实完整性的监控与防护。

Abstract: Large language models (LLMs) continually evolve through pre-training on
ever-expanding web data, but this adaptive process also exposes them to subtle
forms of misinformation. While prior work has explored data poisoning during
static pre-training, the effects of such manipulations under continual
pre-training remain largely unexplored. Drawing inspiration from the illusory
truth effect in human cognition - where repeated exposure to falsehoods
increases belief in their accuracy - we ask whether LLMs exhibit a similar
vulnerability. We investigate whether repeated exposure to false but
confidently stated facts can shift a model's internal representation away from
the truth.
  We introduce Layer of Truth, a framework and dataset for probing belief
dynamics in continually trained LLMs. By injecting controlled amounts of
poisoned data and probing intermediate representations across checkpoints,
model scales, and question types, we quantify when and how factual beliefs
shift. Our findings reveal that even minimal exposure can induce persistent
representational drift in well-established facts, with susceptibility varying
across layers and model sizes. These results highlight an overlooked
vulnerability of continually updated LLMs: their capacity to internalize
misinformation analogously to humans, underscoring the need for robust
monitoring of factual integrity during model updates.

</details>


### [9] [SmoothGuard: Defending Multimodal Large Language Models with Noise Perturbation and Clustering Aggregation](https://arxiv.org/abs/2510.26830)
*Guangzhi Su,Shuchang Huang,Yutong Ke,Zhuohang Liu,Long Qian,Kaizhu Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal large language models (MLLMs) have achieved impressive performance
across diverse tasks by jointly reasoning over textual and visual inputs.
Despite their success, these models remain highly vulnerable to adversarial
manipulations, raising concerns about their safety and reliability in
deployment. In this work, we first generalize an approach for generating
adversarial images within the HuggingFace ecosystem and then introduce
SmoothGuard, a lightweight and model-agnostic defense framework that enhances
the robustness of MLLMs through randomized noise injection and clustering-based
prediction aggregation. Our method perturbs continuous modalities (e.g., images
and audio) with Gaussian noise, generates multiple candidate outputs, and
applies embedding-based clustering to filter out adversarially influenced
predictions. The final answer is selected from the majority cluster, ensuring
stable responses even under malicious perturbations. Extensive experiments on
POPE, LLaVA-Bench (In-the-Wild), and MM-SafetyBench demonstrate that
SmoothGuard improves resilience to adversarial attacks while maintaining
competitive utility. Ablation studies further identify an optimal noise range
(0.1-0.2) that balances robustness and utility.

</details>


### [10] [Accurate Target Privacy Preserving Federated Learning Balancing Fairness and Utility](https://arxiv.org/abs/2510.26841)
*Kangkang Sun,Jun Wu,Minyi Guo,Jianhua Li,Jianwei Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Federated Learning (FL) enables collaborative model training without data
sharing, yet participants face a fundamental challenge, e.g., simultaneously
ensuring fairness across demographic groups while protecting sensitive client
data. We introduce a differentially private fair FL algorithm (\textit{FedPF})
that transforms this multi-objective optimization into a zero-sum game where
fairness and privacy constraints compete against model utility. Our theoretical
analysis reveals a surprising inverse relationship, i.e., stricter privacy
protection fundamentally limits the system's ability to detect and correct
demographic biases, creating an inherent tension between privacy and fairness.
Counterintuitively, we prove that moderate fairness constraints initially
improve model generalization before causing performance degradation, where a
non-monotonic relationship that challenges conventional wisdom about
fairness-utility tradeoffs. Experimental validation demonstrates up to 42.9 %
discrimination reduction across three datasets while maintaining competitive
accuracy, but more importantly, reveals that the privacy-fairness tension is
unavoidable, i.e., achieving both objectives simultaneously requires carefully
balanced compromises rather than optimization of either in isolation. The
source code for our proposed algorithm is publicly accessible at
https://github.com/szpsunkk/FedPF.

</details>


### [11] [CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs](https://arxiv.org/abs/2510.26843)
*Zhiyuan Ning,Jiawei Shao,Ruge Xu,Xinfei Guo,Jun Zhang,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Speculative decoding has become a widely adopted as an effective technique
for lossless inference acceleration when deploying large language models
(LLMs). While on-the-fly self-speculative methods offer seamless integration
and broad utility, they often fall short of the speed gains achieved by methods
relying on specialized training. Cascading a hierarchy of draft models promises
further acceleration and flexibility, but the high cost of training multiple
models has limited its practical application. In this paper, we propose a novel
Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs
speculative draft models by leveraging dynamically switchable inference
acceleration (DSIA) strategies, including layer sparsity and activation
quantization. Furthermore, traditional vertical and horizontal cascade
algorithms are inefficient when applied to self-speculative decoding methods.
We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the
multi-level draft models and assigns the draft lengths, based on the heuristics
of acceptance rates and latency prediction. Our CAS-Spec method achieves
state-of-the-art acceleration compared to existing on-the-fly speculative
decoding methods, with an average speedup from $1.1\times$ to $2.3\times$ over
autoregressive decoding across various LLMs and datasets. DyTC improves the
average speedup by $47$\% and $48$\% over cascade-based baseline and tree-based
baseline algorithms, respectively. CAS-Spec can be easily integrated into most
existing LLMs and holds promising potential for further acceleration as
self-speculative decoding techniques continue to evolve.

</details>


### [12] [Quantitative Bounds for Length Generalization in Transformers](https://arxiv.org/abs/2510.27015)
*Zachary Izzo,Eshaan Nichani,Jason D. Lee*

Main category: cs.LG

TL;DR: 本文给出 transformers 的长度泛化（LG）的第一个定量界限，给出训练序列长度的要求，使模型在面对更长未见输入时仍维持性能，并在多种设定中进行验证。


<details>
  <summary>Details</summary>
Motivation: 此前工作证明当训练长度超过某个有限阈值时会出现长度泛化，但未给出阈值的具体大小。本研究旨在给出训练数据长度的定量界限，并阐明在何种条件下较长输入的内部行为可以被对照的较短输入行为所模拟，从而揭示外推机制。

Method: 在多个设定下推导 LG 的界限：1) 对于 L∞ 误差控制与面向输入分布的平均误差控制；2) 无穷精度 softmax 注意力 vs. 有限精度注意力（简化为 argmax）；3) 一层 vs 两层 Transformer。核心思想是若 longer 序列的内部行为可以被训练中看到的 shorter 序列行为所模拟，则 LG 发生。给出训练数据长度的定性估计和界限，并辅以实验验证。

Result: 给出关于需要的训练数据长度的第一组定性与定量界限，并在若干实验中验证这些洞察，深化对 Transformer 外推机制的理解。

Conclusion: 理论与实验结果共同表明，外推性（LG）依赖于训练数据中对更复杂序列行为的覆盖程度；更丰富的训练数据提升在更长输入上的泛化能力。这些结果正式化了对 Transformer 外推机制的直觉，并为设计更具外推性的模型与数据提供指引。

Abstract: We study the problem of length generalization (LG) in transformers: the
ability of a model trained on shorter sequences to maintain performance when
evaluated on much longer, previously unseen inputs. Prior work by Huang et al.
(2025) established that transformers eventually achieve length generalization
once the training sequence length exceeds some finite threshold, but left open
the question of how large it must be. In this work, we provide the first
quantitative bounds on the required training length for length generalization
to occur. Motivated by previous empirical and theoretical work, we analyze LG
in several distinct problem settings: $\ell_\infty$ error control vs. average
error control over an input distribution, infinite-precision softmax attention
vs. finite-precision attention (which reduces to an argmax) in the transformer,
and one- vs. two-layer transformers. In all scenarios, we prove that LG occurs
when the internal behavior of the transformer on longer sequences can be
"simulated" by its behavior on shorter sequences seen during training. Our
bounds give qualitative estimates for the length of training data required for
a transformer to generalize, and we verify these insights empirically. These
results sharpen our theoretical understanding of the mechanisms underlying
extrapolation in transformers, and formalize the intuition that richer training
data is required for generalization on more complex tasks.

</details>


### [13] [BI-DCGAN: A Theoretically Grounded Bayesian Framework for Efficient and Diverse GANs](https://arxiv.org/abs/2510.26892)
*Mahsa Valizadeh,Rui Tuo,James Caverlee*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative Adversarial Networks (GANs) are proficient at generating synthetic
data but continue to suffer from mode collapse, where the generator produces a
narrow range of outputs that fool the discriminator but fail to capture the
full data distribution. This limitation is particularly problematic, as
generative models are increasingly deployed in real-world applications that
demand both diversity and uncertainty awareness. In response, we introduce
BI-DCGAN, a Bayesian extension of DCGAN that incorporates model uncertainty
into the generative process while maintaining computational efficiency.
BI-DCGAN integrates Bayes by Backprop to learn a distribution over network
weights and employs mean-field variational inference to efficiently approximate
the posterior distribution during GAN training. We establishes the first
theoretical proof, based on covariance matrix analysis, that Bayesian modeling
enhances sample diversity in GANs. We validate this theoretical result through
extensive experiments on standard generative benchmarks, demonstrating that
BI-DCGAN produces more diverse and robust outputs than conventional DCGANs,
while maintaining training efficiency. These findings position BI-DCGAN as a
scalable and timely solution for applications where both diversity and
uncertainty are critical, and where modern alternatives like diffusion models
remain too resource-intensive.

</details>


### [14] [Exploring Landscapes for Better Minima along Valleys](https://arxiv.org/abs/2510.27153)
*Tong Zhao,Jiacheng Li,Yuanchang Zhou,Guangming Tan,Weile Jia*

Main category: cs.LG

TL;DR: 提出一种用于梯度优化器的适配器E（ALTO），在达到局部极小值后继续在损失梯度较低的“山谷”中探索，以寻找更低且更平的极小值，从而提升泛化能力；给出convex与non-convex情形下的收敛性证明，并在大批量训练场景中以Lamb为基线进行验证，平均提升测试准确率约2.5%，显示出潜在的优化算法研究新方向。


<details>
  <summary>Details</summary>
Motivation: 深度学习的损失表面具有复杂几何特性，常规优化器在达到局部极小值时停止搜索，未能保证全局最优或最佳泛化。大批量训练往往带来泛化能力下降，需要新的优化策略来寻找更低、更平的极小值以提升泛化。

Method: 在梯度基优化器上引入适配器E，使优化过程在到达局部极小值后仍沿着低损失且近似相等的山谷继续搜索，以发现潜在的更优局部极小值。给出适配后优化器（ALTO）的理论收敛性证明（包括凸与非凸情形），并在大规模批量训练任务中以Lamb为基准进行实验验证。

Result: 实验结果显示，ALTO在多种大批量训练任务中相较于当前最先进的Lamb基线提升测试准确率平均约2.5%。

Conclusion: 该方法为优化算法设计打开了新的研究方向，结合收敛性保证与实际性能提升，尤其在大批量训练场景中表现出潜在的广泛适用性。

Abstract: Finding lower and better-generalizing minima is crucial for deep learning.
However, most existing optimizers stop searching the parameter space once they
reach a local minimum. Given the complex geometric properties of the loss
landscape, it is difficult to guarantee that such a point is the lowest or
provides the best generalization. To address this, we propose an adaptor "E"
for gradient-based optimizers. The adapted optimizer tends to continue
exploring along landscape valleys (areas with low and nearly identical losses)
in order to search for potentially better local minima even after reaching a
local minimum. This approach increases the likelihood of finding a lower and
flatter local minimum, which is often associated with better generalization. We
also provide a proof of convergence for the adapted optimizers in both convex
and non-convex scenarios for completeness. Finally, we demonstrate their
effectiveness in an important but notoriously difficult training scenario,
large-batch training, where Lamb is the benchmark optimizer. Our testing
results show that the adapted Lamb, ALTO, increases the test accuracy
(generalization) of the current state-of-the-art optimizer by an average of
2.5% across a variety of large-batch training tasks. This work potentially
opens a new research direction in the design of optimization algorithms.

</details>


### [15] [Integrating Ontologies with Large Language Models for Enhanced Control Systems in Chemical Engineering](https://arxiv.org/abs/2510.26898)
*Crystal Su,Kuai Yu,Jingrui Zhang,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work presents an ontology-integrated large language model (LLM)
framework for chemical engineering that unites structured domain knowledge with
generative reasoning. The proposed pipeline aligns model training and inference
with the COPE ontology through a sequence of data acquisition, semantic
preprocessing, information extraction, and ontology mapping steps, producing
templated question-answer pairs that guide fine-tuning. A control-focused
decoding stage and citation gate enforce syntactic and factual grounding by
constraining outputs to ontology-linked terms, while evaluation metrics
quantify both linguistic quality and ontological accuracy. Feedback and future
extensions, including semantic retrieval and iterative validation, further
enhance the system's interpretability and reliability. This integration of
symbolic structure and neural generation provides a transparent, auditable
approach for applying LLMs to process control, safety analysis, and other
critical engineering contexts.

</details>


### [16] [Soft Task-Aware Routing of Experts for Equivariant Representation Learning](https://arxiv.org/abs/2510.27222)
*Jaebyeong Jeon,Hyeonseo Jang,Jy-yong Sohn,Kibok Lee*

Main category: cs.LG

TL;DR: 通过 Soft Task-Aware Routing (STAR)，把投影头视为专家来区分共享信息与任务特定信息，从而减少冗余特征并提升跨任务传输性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在联合学习Invariant与Equivariant表征时，通常采用分离的投影头，但未能充分利用它们之间的共享信息，导致特征冗余和模型容量浪费；需要一种机制显式建模共享与任务特定信息。

Method: 提出 STAR 路由策略，将投影头视为专家并让它们在共享信息与任务特定信息上专门化，通过路由机制实现不同头部对不同信息的捕捉，从而降低 invariant 与 equivariant 表征之间的冗余相关性。

Result: 实验结果在多样的迁移学习任务上显示稳定提升；并通过观察 invariant 与 equivariant 嵌入之间的典型相关系数降低，验证了专家化的有效性；代码公开。

Conclusion: STAR 能有效减少冗余特征学习，促进对共享与任务特定信息的捕捉，从而提升下游传输任务的表现。

Abstract: Equivariant representation learning aims to capture variations induced by
input transformations in the representation space, whereas invariant
representation learning encodes semantic information by disregarding such
transformations. Recent studies have shown that jointly learning both types of
representations is often beneficial for downstream tasks, typically by
employing separate projection heads. However, this design overlooks information
shared between invariant and equivariant learning, which leads to redundant
feature learning and inefficient use of model capacity. To address this, we
introduce Soft Task-Aware Routing (STAR), a routing strategy for projection
heads that models them as experts. STAR induces the experts to specialize in
capturing either shared or task-specific information, thereby reducing
redundant feature learning. We validate this effect by observing lower
canonical correlations between invariant and equivariant embeddings.
Experimental results show consistent improvements across diverse transfer
learning tasks. The code is available at https://github.com/YonseiML/star.

</details>


### [17] [Discovering EV Charging Site Archetypes Through Few Shot Forecasting: The First U.S.-Wide Study](https://arxiv.org/abs/2510.26910)
*Kshitij Nikhal,Luke Ackerknecht,Benjamin S. Riggan,Phil Stahlfeld*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The decarbonization of transportation relies on the widespread adoption of
electric vehicles (EVs), which requires an accurate understanding of charging
behavior to ensure cost-effective, grid-resilient infrastructure. Existing work
is constrained by small-scale datasets, simple proximity-based modeling of
temporal dependencies, and weak generalization to sites with limited
operational history. To overcome these limitations, this work proposes a
framework that integrates clustering with few-shot forecasting to uncover site
archetypes using a novel large-scale dataset of charging demand. The results
demonstrate that archetype-specific expert models outperform global baselines
in forecasting demand at unseen sites. By establishing forecast performance as
a basis for infrastructure segmentation, we generate actionable insights that
enable operators to lower costs, optimize energy and pricing strategies, and
support grid resilience critical to climate goals.

</details>


### [18] [MM-OPERA: Benchmarking Open-ended Association Reasoning for Large Vision-Language Models](https://arxiv.org/abs/2510.26937)
*Zimeng Huang,Jinxin Ke,Xiaoxuan Fan,Yufeng Yang,Yang Liu,Liu Zhonghan,Zedi Wang,Junteng Dai,Haoyi Jiang,Yuyu Zhou,Keze Wang,Ziliang Chen*

Main category: cs.LG

TL;DR: 提出MM-OPERA，一个包含11,497条样本、覆盖远程物项联想(RIA)与情境内联想(ICA)的开端性开放式评估基准，用以衡量LVLMs的联想推理能力；通过自由回答和显式推理路径，并结合LLM作为评审进行过程奖励型判断，全面评估并分析模型在多语言、文化及领域的联想能力与不足。


<details>
  <summary>Details</summary>
Motivation: 当前LVLM基准多限于闭合任务，难以捕捉人类认知中的联想与创造性整合能力；需要与心理测量学原则对齐的开放式评估以衡量推理与创造性。

Method: 设计11,497实例的开端任务，覆盖RIA与ICA；提供自由文本回答和推理路径；采用LLM作为评审，基于过程强化信号进行判断；对模型进行敏感性、有效性、多样性等分析，覆盖能力、领域、语言、文化等维度。

Result: 对当前SOTA LVLMs进行广泛评估，揭示它们在联想推理方面的局限性；给出对数据集、评估方法及评审策略的系统分析；数据集和代码公开。

Conclusion: 该基准有助于推动更具人类类比的广义智能发展，提升LVLM在开放式联想与创造性推理方面的评估与对齐，促成更强的一致性与泛化能力。

Abstract: Large Vision-Language Models (LVLMs) have exhibited remarkable progress.
However, deficiencies remain compared to human intelligence, such as
hallucination and shallow pattern matching. In this work, we aim to evaluate a
fundamental yet underexplored intelligence: association, a cornerstone of human
cognition for creative thinking and knowledge integration. Current benchmarks,
often limited to closed-ended tasks, fail to capture the complexity of
open-ended association reasoning vital for real-world applications. To address
this, we present MM-OPERA, a systematic benchmark with 11,497 instances across
two open-ended tasks: Remote-Item Association (RIA) and In-Context Association
(ICA), aligning association intelligence evaluation with human psychometric
principles. It challenges LVLMs to resemble the spirit of divergent thinking
and convergent associative reasoning through free-form responses and explicit
reasoning paths. We deploy tailored LLM-as-a-Judge strategies to evaluate
open-ended outputs, applying process-reward-informed judgment to dissect
reasoning with precision. Extensive empirical studies on state-of-the-art
LVLMs, including sensitivity analysis of task instances, validity analysis of
LLM-as-a-Judge strategies, and diversity analysis across abilities, domains,
languages, cultures, etc., provide a comprehensive and nuanced understanding of
the limitations of current LVLMs in associative reasoning, paving the way for
more human-like and general-purpose AI. The dataset and code are available at
https://github.com/MM-OPERA-Bench/MM-OPERA.

</details>


### [19] [Can machines think efficiently?](https://arxiv.org/abs/2510.26954)
*Adam Winchell*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Turing Test is no longer adequate for distinguishing human and machine
intelligence. With advanced artificial intelligence systems already passing the
original Turing Test and contributing to serious ethical and environmental
concerns, we urgently need to update the test. This work expands upon the
original imitation game by accounting for an additional factor: the energy
spent answering the questions. By adding the constraint of energy, the new test
forces us to evaluate intelligence through the lens of efficiency, connecting
the abstract problem of thinking to the concrete reality of finite resources.
Further, this proposed new test ensures the evaluation of intelligence has a
measurable, practical finish line that the original test lacks. This additional
constraint compels society to weigh the time savings of using artificial
intelligence against its total resource cost.

</details>


### [20] [Fine-Grained Iterative Adversarial Attacks with Limited Computation Budget](https://arxiv.org/abs/2510.26981)
*Zhichao Hou,Weizhi Gao,Xiaorui Liu*

Main category: cs.LG

TL;DR: 在有限计算预算下，通过对迭代和层级进行细粒度的重计算控制，提高对抗攻击的强度与成本效率，方法优于基线，且在对抗训练中可用更少预算达到相当效果。


<details>
  <summary>Details</summary>
Motivation: 在计算资源有限的情境下，如何最大化迭代对抗攻击的有效性，是AI安全研究中的一个关键挑战。本 Work 试图在固定预算内提高攻击强度，权衡多次迭代和成本之间的关系。

Method: 提出一种细粒度控制机制，按迭代与层级两个维度选择性地重计算层激活，从而在预算约束下提升攻击效能。

Result: 实验证明该方法在等成本下持续优于现有基线；将其整合到对抗训练中，达到原有预算的30%时仍具备可比性能。

Conclusion: 通过将计算资源更精细地分配到关键激活的重计算上，这一方法在有限预算下提升攻击强度，并对AI安全研究中的资源受限场景具有潜在影响。

Abstract: This work tackles a critical challenge in AI safety research under limited
compute: given a fixed computation budget, how can one maximize the strength of
iterative adversarial attacks? Coarsely reducing the number of attack
iterations lowers cost but substantially weakens effectiveness. To fulfill the
attainable attack efficacy within a constrained budget, we propose a
fine-grained control mechanism that selectively recomputes layer activations
across both iteration-wise and layer-wise levels. Extensive experiments show
that our method consistently outperforms existing baselines at equal cost.
Moreover, when integrated into adversarial training, it attains comparable
performance with only 30% of the original budget.

</details>


### [21] [HADSF: Aspect Aware Semantic Control for Explainable Recommendation](https://arxiv.org/abs/2510.26994)
*Zheng Nie,Peijie Sun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in large language models (LLMs) promise more effective
information extraction for review-based recommender systems, yet current
methods still (i) mine free-form reviews without scope control, producing
redundant and noisy representations, (ii) lack principled metrics that link LLM
hallucination to downstream effectiveness, and (iii) leave the cost-quality
trade-off across model scales largely unexplored. We address these gaps with
the Hyper-Adaptive Dual-Stage Semantic Framework (HADSF), a two-stage approach
that first induces a compact, corpus-level aspect vocabulary via adaptive
selection and then performs vocabulary-guided, explicitly constrained
extraction of structured aspect-opinion triples. To assess the fidelity of the
resulting representations, we introduce Aspect Drift Rate (ADR) and Opinion
Fidelity Rate (OFR) and empirically uncover a nonmonotonic relationship between
hallucination severity and rating prediction error. Experiments on
approximately 3 million reviews across LLMs spanning 1.5B-70B parameters show
that, when integrated into standard rating predictors, HADSF yields consistent
reductions in prediction error and enables smaller models to achieve
competitive performance in representative deployment scenarios. We release
code, data pipelines, and metric implementations to support reproducible
research on hallucination-aware, LLM-enhanced explainable recommendation. Code
is available at https://github.com/niez233/HADSF

</details>


### [22] [Gradient Descent as Loss Landscape Navigation: a Normative Framework for Deriving Learning Rules](https://arxiv.org/abs/2510.26997)
*John J. Vastola,Samuel J. Gershman,Kanaka Rajan*

Main category: cs.LG

TL;DR: 将学习规则视为在部分可观测的损失景观中导航的策略；通过最优控制问题推导出梯度下降、动量、自然梯度、非梯度规则、Adam，以及权重重置等的统一理论。


<details>
  <summary>Details</summary>
Motivation: 解释为何不同学习规则表现不同、在何种假设下某规则是最优的；从直觉的经验主义药方转向原则性的推导。

Method: 把学习过程建模为对损失景观的策略优化（部分可观测），形成一个最优控制问题；在不同假设下推导出相应的规则。

Result: 在同一框架下，自然梯度、动量、Adam、非梯度规则等都作为特定情形自然显现；权重重置等持续学习策略被视为对任务不确定性的最优响应；为设计自适应算法提供统一基石。

Conclusion: 为学习规则提供原理性的基础，统一了多种现象，便于在单一目标下设计更自适应的优化算法。

Abstract: Learning rules -- prescriptions for updating model parameters to improve
performance -- are typically assumed rather than derived. Why do some learning
rules work better than others, and under what assumptions can a given rule be
considered optimal? We propose a theoretical framework that casts learning
rules as policies for navigating (partially observable) loss landscapes, and
identifies optimal rules as solutions to an associated optimal control problem.
A range of well-known rules emerge naturally within this framework under
different assumptions: gradient descent from short-horizon optimization,
momentum from longer-horizon planning, natural gradients from accounting for
parameter space geometry, non-gradient rules from partial controllability, and
adaptive optimizers like Adam from online Bayesian inference of loss landscape
shape. We further show that continual learning strategies like weight resetting
can be understood as optimal responses to task uncertainty. By unifying these
phenomena under a single objective, our framework clarifies the computational
structure of learning and offers a principled foundation for designing adaptive
algorithms.

</details>


### [23] [A Framework for Fair Evaluation of Variance-Aware Bandit Algorithms](https://arxiv.org/abs/2510.27001)
*Elise Wolf*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-armed bandit (MAB) problems serve as a fundamental building block for
more complex reinforcement learning algorithms. However, evaluating and
comparing MAB algorithms remains challenging due to the lack of standardized
conditions and replicability. This is particularly problematic for
variance-aware extensions of classical methods like UCB, whose performance can
heavily depend on the underlying environment. In this study, we address how
performance differences between bandit algorithms can be reliably observed, and
under what conditions variance-aware algorithms outperform classical ones. We
present a reproducible evaluation designed to systematically compare eight
classical and variance-aware MAB algorithms. The evaluation framework,
implemented in our Bandit Playground codebase, features clearly defined
experimental setups, multiple performance metrics (reward, regret, reward
distribution, value-at-risk, and action optimality), and an interactive
evaluation interface that supports consistent and transparent analysis. We show
that variance-aware algorithms can offer advantages in settings with high
uncertainty where the difficulty arises from subtle differences between arm
rewards. In contrast, classical algorithms often perform equally well or better
in more separable scenarios or if fine-tuned extensively. Our contributions are
twofold: (1) a framework for systematic evaluation of MAB algorithms, and (2)
insights into the conditions under which variance-aware approaches outperform
their classical counterparts.

</details>


### [24] [Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase](https://arxiv.org/abs/2510.27002)
*Mihir Mahajan,Alfred Nguyen,Franz Srambical,Stefan Bauer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While world models are increasingly positioned as a pathway to overcoming
data scarcity in domains such as robotics, open training infrastructure for
world modeling remains nascent. We introduce Jasmine, a performant JAX-based
world modeling codebase that scales from single hosts to hundreds of
accelerators with minimal code changes. Jasmine achieves an order-of-magnitude
faster reproduction of the CoinRun case study compared to prior open
implementations, enabled by performance optimizations across data loading,
training and checkpointing. The codebase guarantees fully reproducible training
and supports diverse sharding configurations. By pairing Jasmine with curated
large-scale datasets, we establish infrastructure for rigorous benchmarking
pipelines across model families and architectural ablations.

</details>


### [25] [Mixture-of-Transformers Learn Faster: A Theoretical Study on Classification Problems](https://arxiv.org/abs/2510.27004)
*Hongbo Li,Qinhang Wu,Sen Lin,Yingbin Liang,Ness B. Shroff*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mixture-of-Experts (MoE) models improve transformer efficiency but lack a
unified theoretical explanation, especially when both feed-forward and
attention layers are allowed to specialize. To this end, we study the
Mixture-of-Transformers (MoT), a tractable theoretical framework in which each
transformer block acts as an expert governed by a continuously trained gating
network. This design allows us to isolate and study the core learning dynamics
of expert specialization and attention alignment. In particular, we develop a
three-stage training algorithm with continuous training of the gating network,
and show that each transformer expert specializes in a distinct class of tasks
and that the gating network accurately routes data samples to the correct
expert. Our analysis shows how expert specialization reduces gradient conflicts
and makes each subtask strongly convex. We prove that the training drives the
expected prediction loss to near zero in $O(\log(\epsilon^{-1}))$ iteration
steps, significantly improving over the $O(\epsilon^{-1})$ rate for a single
transformer. We further validate our theoretical findings through extensive
real-data experiments, demonstrating the practical effectiveness of MoT.
Together, these results offer the first unified theoretical account of
transformer-level specialization and learning dynamics, providing practical
guidance for designing efficient large-scale models.

</details>


### [26] [Enhancing Sentiment Classification with Machine Learning and Combinatorial Fusion](https://arxiv.org/abs/2510.27014)
*Sean Patten,Pin-Yu Chen,Christina Schweikert,D. Frank Hsu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a novel approach to sentiment classification using the
application of Combinatorial Fusion Analysis (CFA) to integrate an ensemble of
diverse machine learning models, achieving state-of-the-art accuracy on the
IMDB sentiment analysis dataset of 97.072\%. CFA leverages the concept of
cognitive diversity, which utilizes rank-score characteristic functions to
quantify the dissimilarity between models and strategically combine their
predictions. This is in contrast to the common process of scaling the size of
individual models, and thus is comparatively efficient in computing resource
use. Experimental results also indicate that CFA outperforms traditional
ensemble methods by effectively computing and employing model diversity. The
approach in this paper implements the combination of a transformer-based model
of the RoBERTa architecture with traditional machine learning models, including
Random Forest, SVM, and XGBoost.

</details>


### [27] [Limits of Generalization in RLVR: Two Case Studies in Mathematical Reasoning](https://arxiv.org/abs/2510.27044)
*Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: RLVR improves evaluation metrics on two verifiable combinatorial tasks but tends to reinforce superficial heuristics rather than genuine mathematical reasoning; highlights need for benchmarks that separate true reasoning from shortcut exploitation and faithful progress measures.


<details>
  <summary>Details</summary>
Motivation: To understand whether RLVR can foster genuine mathematical reasoning in LLMs and to assess its generalization, given concerns about shortcut exploitation.

Method: Apply Reinforcement Learning with Verifiable Rewards to two fully verifiable problems (Activity Scheduling and Longest Increasing Subsequence) using curated datasets with unique optima; test multiple reward designs.

Result: RLVR yields improvements in evaluation metrics but often due to reinforcing superficial heuristics; limited generalization to genuine reasoning strategies; progress may be overstated without faithful measures.

Conclusion: Benchmarks must disentangle genuine mathematical reasoning from shortcut exploitation and provide faithful progress metrics; code is available at the linked repository to enable replication and further study.

Abstract: Mathematical reasoning is a central challenge for large language models
(LLMs), requiring not only correct answers but also faithful reasoning
processes. Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as
a promising approach for enhancing such capabilities; however, its ability to
foster genuine reasoning remains unclear. We investigate RLVR on two
combinatorial problems with fully verifiable solutions: \emph{Activity
Scheduling} and the \emph{Longest Increasing Subsequence}, using carefully
curated datasets with unique optima. Across multiple reward designs, we find
that RLVR improves evaluation metrics but often by reinforcing superficial
heuristics rather than acquiring new reasoning strategies. These findings
highlight the limits of RLVR generalization, emphasizing the importance of
benchmarks that disentangle genuine mathematical reasoning from shortcut
exploitation and provide faithful measures of progress. Code available at
https://github.com/xashru/rlvr-seq-generalization.

</details>


### [28] [Consistency Training Helps Stop Sycophancy and Jailbreaks](https://arxiv.org/abs/2510.27062)
*Alex Irpan,Alexander Matt Turner,Mark Kurzeja,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 提出一致性训练框架以提升LLM对无关提示线索的鲁棒性，提出BCT（对输出）与ACT（对内部激活）的两种实现，降低sycophancy和jailbreak易受性，强调自监督数据避免过时数据带来的风险。


<details>
  <summary>Details</summary>
Motivation: 修复提示中的无关线索导致的事实性下降、对用户信仰的盲从，以及应对越狱等不当请求。

Method: 在模型输出（BCT）和内部激活（ACT）上实现一致性训练，通过对提示数据增强（如添加前导问句、越狱文本等）使模型对同一任务保持输出/内部表示的一致性。

Result: 两种方法都降低Gemini 2.5 Flash对无关线索的敏感性；BCT在降低越狱方面表现优于ACT；两者对sycophancy的缓解效果相近；自监督数据训练避免了因使用静态数据集而带来的能力下降。

Conclusion: 将某些对齐问题视为一致性问题，BCT或ACT等方法可以简化对齐训练流程，未来工作可探索将一致性原则扩展到更广的任务与模型。

Abstract: An LLM's factuality and refusal training can be compromised by simple changes
to a prompt. Models often adopt user beliefs (sycophancy) or satisfy
inappropriate requests which are wrapped within special text (jailbreaking). We
explore \emph{consistency training}, a self-supervised paradigm that teaches a
model to be invariant to certain irrelevant cues in the prompt. Instead of
teaching the model what exact response to give on a particular prompt, we aim
to teach the model to behave identically across prompt data augmentations (like
adding leading questions or jailbreak text). We try enforcing this invariance
in two ways: over the model's external outputs (\emph{Bias-augmented
Consistency Training} (BCT) from Chua et al. [2025]) and over its internal
activations (\emph{Activation Consistency Training} (ACT), a method we
introduce). Both methods reduce Gemini 2.5 Flash's susceptibility to irrelevant
cues. Because consistency training uses responses from the model itself as
training data, it avoids issues that arise from stale training data, such as
degrading model capabilities or enforcing outdated response guidelines. While
BCT and ACT reduce sycophancy equally well, BCT does better at jailbreak
reduction. We think that BCT can simplify training pipelines by removing
reliance on static datasets. We argue that some alignment problems are better
viewed not in terms of optimal responses, but rather as consistency issues.

</details>


### [29] [Towards a Measure of Algorithm Similarity](https://arxiv.org/abs/2510.27063)
*Shairoz Sohail,Taher Ali*

Main category: cs.LG

TL;DR: EMOC 框架将算法实现嵌入可用于下游任务的特征空间，并通过 PACD 数据集实现对算法相似性、近重复和多样性的稳定评估与分类。


<details>
  <summary>Details</summary>
Motivation: 在广义问题中可判定性不足且相似性定义众多的背景下，需要一个实用、可重复的相似性评估框架，便于克隆检测、程序合成等任务。

Method: 将算法实现映射到一个包含 Evaluations、Memory、Operations、Complexity 四维特征的嵌入空间；构建并发布 PACD 数据集，包含经验证的 Python 实现，利用 EMOC 特征进行聚类、分类、近重复检测以及对大语言模型生成程序的多样性量化等下游任务。

Result: EMOC 特征可用于算法类型的聚类与分类，近重复检测，以及对 LLM 生成程序的多样性量化；并发布代码、数据与工具，提升复现实验性和未来工作可操作性。

Conclusion: 提出的 EMOC 框架在多种任务上表现出实用性与可扩展性，为跨实现对比提供了一个统一且可复用的基准，PACD 数据集为跨实现评估建立了基线。

Abstract: Given two algorithms for the same problem, can we determine whether they are
meaningfully different? In full generality, the question is uncomputable, and
empirically it is muddied by competing notions of similarity. Yet, in many
applications (such as clone detection or program synthesis) a pragmatic and
consistent similarity metric is necessary. We review existing equivalence and
similarity notions and introduce EMOC: An
Evaluation-Memory-Operations-Complexity framework that embeds algorithm
implementations into a feature space suitable for downstream tasks. We compile
PACD, a curated dataset of verified Python implementations across three
problems, and show that EMOC features support clustering and classification of
algorithm types, detection of near-duplicates, and quantification of diversity
in LLM-generated programs. Code, data, and utilities for computing EMOC
embeddings are released to facilitate reproducibility and future work on
algorithm similarity.

</details>


### [30] [MLPerf Automotive](https://arxiv.org/abs/2510.27065)
*Radoyeh Shojaei,Predrag Djurdjevic,Mostafa El-Khamy,James Goel,Kasper Mecklenburg,John Owens,Pınar Muyan-Özçelik,Tom St. John,Jinho Suh,Arjun Suresh*

Main category: cs.LG

TL;DR: MLPerf Automotive 首次提出公开的标准化基准，用于评估部署在汽车系统中的机器学习推理性能，覆盖2D/3D感知任务，提供延迟与精度指标及统一评测协议，代码开源于 GitHub。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法覆盖汽车场景的安全性和实时性等独特约束，需要一个可重复、公平的评测框架，以实现跨硬件与软件实现的一致性比较。

Method: 基于 MLCommons 与 Autonomous Vehicle Computing Consortium 的合作，设计基准框架，确定任务集（2D目标检测、2D语义分割、3D目标检测）、参考模型和提交规则，制定延迟与准确性评测指标及评测流程，并讨论数据集获取与参考实现开发中的工程挑战，代码仓库为 mlperf_automotive。

Result: 第一轮基准提交已经开展，展示了方法学的可行性，同时阐述了获取数据集与开发参考实现所面临的挑战。

Conclusion: 该基准为汽车领域的 ML 系统评测提供了统一、可复现的框架，支持在不同硬件和软件实现之间进行可比性比较，后续将扩展任务与数据、迭代改进。

Abstract: We present MLPerf Automotive, the first standardized public benchmark for
evaluating Machine Learning systems that are deployed for AI acceleration in
automotive systems. Developed through a collaborative partnership between
MLCommons and the Autonomous Vehicle Computing Consortium, this benchmark
addresses the need for standardized performance evaluation methodologies in
automotive machine learning systems. Existing benchmark suites cannot be
utilized for these systems since automotive workloads have unique constraints
including safety and real-time processing that distinguish them from the
domains that previously introduced benchmarks target. Our benchmarking
framework provides latency and accuracy metrics along with evaluation protocols
that enable consistent and reproducible performance comparisons across
different hardware platforms and software implementations. The first iteration
of the benchmark consists of automotive perception tasks in 2D object
detection, 2D semantic segmentation, and 3D object detection. We describe the
methodology behind the benchmark design including the task selection, reference
models, and submission rules. We also discuss the first round of benchmark
submissions and the challenges involved in acquiring the datasets and the
engineering efforts to develop the reference implementations. Our benchmark
code is available at https://github.com/mlcommons/mlperf_automotive.

</details>


### [31] [Towards Understanding Self-play for LLM Reasoning](https://arxiv.org/abs/2510.27072)
*Justin Yang Chae,Md Tanvirul Alam,Nidhi Rastogi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in large language model (LLM) reasoning, led by reinforcement
learning with verifiable rewards (RLVR), have inspired self-play post-training,
where models improve by generating and solving their own problems. While
self-play has shown strong in-domain and out-of-domain gains, the mechanisms
behind these improvements remain poorly understood. In this work, we analyze
the training dynamics of self-play through the lens of the Absolute Zero
Reasoner, comparing it against RLVR and supervised fine-tuning (SFT). Our study
examines parameter update sparsity, entropy dynamics of token distributions,
and alternative proposer reward functions. We further connect these dynamics to
reasoning performance using pass@k evaluations. Together, our findings clarify
how self-play differs from other post-training strategies, highlight its
inherent limitations, and point toward future directions for improving LLM math
reasoning through self-play.

</details>


### [32] [Functional embeddings enable Aggregation of multi-area SEEG recordings over subjects and sessions](https://arxiv.org/abs/2510.27090)
*Sina Javadzadeh,Rahil Soroushmojdehi,S. Alireza Seyyed Mousavi,Mehrnaz Asadi,Sumiko Abe,Terence D. Sanger*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Aggregating intracranial recordings across subjects is challenging since
electrode count, placement, and covered regions vary widely. Spatial
normalization methods like MNI coordinates offer a shared anatomical reference,
but often fail to capture true functional similarity, particularly when
localization is imprecise; even at matched anatomical coordinates, the targeted
brain region and underlying neural dynamics can differ substantially between
individuals. We propose a scalable representation-learning framework that (i)
learns a subject-agnostic functional identity for each electrode from
multi-region local field potentials using a Siamese encoder with contrastive
objectives, inducing an embedding geometry that is locality-sensitive to
region-specific neural signatures, and (ii) tokenizes these embeddings for a
transformer that models inter-regional relationships with a variable number of
channels. We evaluate this framework on a 20-subject dataset spanning basal
ganglia-thalamic regions collected during flexible rest/movement recording
sessions with heterogeneous electrode layouts. The learned functional space
supports accurate within-subject discrimination and forms clear,
region-consistent clusters; it transfers zero-shot to unseen channels. The
transformer, operating on functional tokens without subject-specific heads or
supervision, captures cross-region dependencies and enables reconstruction of
masked channels, providing a subject-agnostic backbone for downstream decoding.
Together, these results indicate a path toward large-scale, cross-subject
aggregation and pretraining for intracranial neural data where strict task
structure and uniform sensor placement are unavailable.

</details>


### [33] [QiNN-QJ: A Quantum-inspired Neural Network with Quantum Jump for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.27091)
*Yiwei Chen,Kehuan Yan,Yu Pan,Daoyi Dong*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantum theory provides non-classical principles, such as superposition and
entanglement, that inspires promising paradigms in machine learning. However,
most existing quantum-inspired fusion models rely solely on unitary or
unitary-like transformations to generate quantum entanglement. While
theoretically expressive, such approaches often suffer from training
instability and limited generalizability. In this work, we propose a
Quantum-inspired Neural Network with Quantum Jump (QiNN-QJ) for multimodal
entanglement modelling. Each modality is firstly encoded as a quantum pure
state, after which a differentiable module simulating the QJ operator
transforms the separable product state into the entangled representation. By
jointly learning Hamiltonian and Lindblad operators, QiNN-QJ generates
controllable cross-modal entanglement among modalities with dissipative
dynamics, where structured stochasticity and steady-state attractor properties
serve to stabilize training and constrain entanglement shaping. The resulting
entangled states are projected onto trainable measurement vectors to produce
predictions. In addition to achieving superior performance over the
state-of-the-art models on benchmark datasets, including CMU-MOSI, CMU-MOSEI,
and CH-SIMS, QiNN-QJ facilitates enhanced post-hoc interpretability through
von-Neumann entanglement entropy. This work establishes a principled framework
for entangled multimodal fusion and paves the way for quantum-inspired
approaches in modelling complex cross-modal correlations.

</details>


### [34] [Hierarchical Bayesian Model for Gene Deconvolution and Functional Analysis in Human Endometrium Across the Menstrual Cycle](https://arxiv.org/abs/2510.27097)
*Crystal Su,Kuai Yu,Mingyuan Shao,Daniel Bauer*

Main category: cs.LG

TL;DR: 提出一个基于概率层级贝叶斯模型的体积RNA-seq去卷积方法，使用高分辨率单细胞参考，在子宫内膜的月经周期中推断细胞类型比例和细胞内基因表达变化。结果显示周期阶段间上皮、间质和免疫细胞比例动态变化，并揭示细胞特异性差异表达，模型对参考错配和噪声具有鲁棒性，讨论临床意义及未来整合时空转录组的可能。


<details>
  <summary>Details</summary>
Motivation: 解决体层RNA-seq对多种细胞类型混杂造成的平均表达问题，需同时推断细胞类型比例和细胞类型特异的表达变化。

Method: 提出一个概率层级贝叶斯去卷积框架，利用高分辨率的单细胞参考来分解 bulk RNA-seq 数据成成分细胞类型表达谱与各类型比例，并扩展以在月经周期不同阶段推断细胞类型的表达变化。给出模型结构、先验、推断策略，结合仿真与与现有方法的比较进行验证。

Result: 在不同月经阶段观察到上皮、间质与免疫细胞比例的动态变化；识别出与内膜功能相关的细胞类型特异性差异表达（如分泌期间质细胞的蜕变标志物）。鲁棒性测试表明对参考错配和噪声具有抗性。指出将来可与空间转录组数据整合。

Conclusion: 贝叶斯去卷积框架提供对细胞组成以及细胞类型特异表达变化的原则性推断，具有潜在生殖与内膜疾病的临床意义；未来工作包括与时空转录组的整合等。

Abstract: Bulk tissue RNA sequencing of heterogeneous samples provides averaged gene
expression profiles, obscuring cell type-specific dynamics. To address this, we
present a probabilistic hierarchical Bayesian model that deconvolves bulk
RNA-seq data into constituent cell-type expression profiles and proportions,
leveraging a high-resolution single-cell reference. We apply our model to human
endometrial tissue across the menstrual cycle, a context characterized by
dramatic hormone-driven cellular composition changes. Our extended framework
provides a principled inference of cell type proportions and cell-specific gene
expression changes across cycle phases. We demonstrate the model's structure,
priors, and inference strategy in detail, and we validate its performance with
simulations and comparisons to existing methods. The results reveal dynamic
shifts in epithelial, stromal, and immune cell fractions between menstrual
phases, and identify cell-type-specific differential gene expression associated
with endometrial function (e.g., decidualization markers in stromal cells
during the secretory phase). We further conduct robustness tests and show that
our Bayesian approach is resilient to reference mismatches and noise. Finally,
we discuss the biological significance of our findings, potential clinical
implications for fertility and endometrial disorders, and future directions,
including integration of spatial transcriptomics.

</details>


### [35] [Group-Sensitive Offline Contextual Bandits](https://arxiv.org/abs/2510.27123)
*Yihong Guo,Junjie Luo,Guodong Gao,Ritu Agarwal,Anqi Liu*

Main category: cs.LG

TL;DR: 提出一种带群体层面奖励差异约束的离线策略优化框架，通过引入群体间奖励差异约束并采用双重鲁棒估计，降低群体间奖励差异同时保持总体表现，并给出收敛性保障。


<details>
  <summary>Details</summary>
Motivation: 离线上下文 bandits 学习中，最大化总体奖励可能放大不同群体之间的奖励差异，导致不公平，尤其在资源有限的场景。

Method: 在离线策略优化中将群体级奖励差异约束加入到离策略梯度优化过程，使用双重鲁棒估计来更好地估计差异，并给出收敛性保证。

Result: 在合成数据和真实数据集上，所提方法显著降低群体间奖励差异，同时保持竞争性的总体绩效。

Conclusion: 该框架可在离线策略学习中实现群体公平约束，理论上具有收敛性保证，适用于资源有限场景的公平分配。

Abstract: Offline contextual bandits allow one to learn policies from
historical/offline data without requiring online interaction. However, offline
policy optimization that maximizes overall expected rewards can unintentionally
amplify the reward disparities across groups. As a result, some groups might
benefit more than others from the learned policy, raising concerns about
fairness, especially when the resources are limited. In this paper, we study a
group-sensitive fairness constraint in offline contextual bandits, reducing
group-wise reward disparities that may arise during policy learning. We tackle
the following common-parity requirements: the reward disparity is constrained
within some user-defined threshold or the reward disparity should be minimized
during policy optimization. We propose a constrained offline policy
optimization framework by introducing group-wise reward disparity constraints
into an off-policy gradient-based optimization procedure. To improve the
estimation of the group-wise reward disparity during training, we employ a
doubly robust estimator and further provide a convergence guarantee for policy
optimization. Empirical results in synthetic and real-world datasets
demonstrate that our method effectively reduces reward disparities while
maintaining competitive overall performance.

</details>


### [36] [AI Agents in Drug Discovery](https://arxiv.org/abs/2510.27130)
*Srijit Seal,Dinh Long Huynh,Moudather Chelbi,Sara Khosravi,Ankur Kumar,Mattson Thieme,Isaac Wilks,Mark Davies,Jessica Mustali,Yannick Sun,Nick Edwards,Daniil Boiko,Andrei Tyrin,Douglas W. Selinger,Ayaan Parikh,Rahul Vijayan,Shoman Kasbekar,Dylan Reid,Andreas Bender,Ola Spjuth*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Artificial intelligence (AI) agents are emerging as transformative tools in
drug discovery, with the ability to autonomously reason, act, and learn through
complicated research workflows. Building on large language models (LLMs)
coupled with perception, computation, action, and memory tools, these agentic
AI systems could integrate diverse biomedical data, execute tasks, carry out
experiments via robotic platforms, and iteratively refine hypotheses in closed
loops. We provide a conceptual and technical overview of agentic AI
architectures, ranging from ReAct and Reflection to Supervisor and Swarm
systems, and illustrate their applications across key stages of drug discovery,
including literature synthesis, toxicity prediction, automated protocol
generation, small-molecule synthesis, drug repurposing, and end-to-end
decision-making. To our knowledge, this represents the first comprehensive work
to present real-world implementations and quantifiable impacts of agentic AI
systems deployed in operational drug discovery settings. Early implementations
demonstrate substantial gains in speed, reproducibility, and scalability,
compressing workflows that once took months into hours while maintaining
scientific traceability. We discuss the current challenges related to data
heterogeneity, system reliability, privacy, and benchmarking, and outline
future directions towards technology in support of science and translation.

</details>


### [37] [Exploring the Utilities of the Rationales from Large Language Models to Enhance Automated Essay Scoring](https://arxiv.org/abs/2510.27131)
*Hong Jiao,Hanna Choi,Haowei Hua*

Main category: cs.LG

TL;DR: 本研究比较GPT-4.1/GPT-5生成推理理由在自动作文评分中的作用与纯作文评分的差异，并评估推理理由与作文评分的集成对整体准确性的提升。


<details>
  <summary>Details</summary>
Motivation: 探索推理推断在教育评估中的有效性，及通过模型集成提升自动评分系统的可靠性和公正性。

Method: 使用2012 Kaggle ASAP数据集的 Prompt 6 作文，比较作文评分、推理理由评分，以及它们的组合模型。评估指标包括Quadratic Weighted Kappa（QWK）和F1，关注类别不平衡对0分等的影响。

Result: 总体而言，作文评分优于基于理由的评分，且理由评分在分布偏斜的0分类别上提高了F1。对作文评分与理由评分的多模型集合在特定分数和全部分数水平上提升准确性；将作文评分与两类理由评分的组合达到最佳表现，QWK为0.870，高于文献中的0.848。

Conclusion: 作文基础的评分仍优于基于推理理由的评分，但将两者结合（尤其是作文评分与两类推理理由评分的集成）可以显著提升自动评分系统的整体准确性。

Abstract: This study explored the utilities of rationales generated by GPT-4.1 and
GPT-5 in automated scoring using Prompt 6 essays from the 2012 Kaggle ASAP
data. Essay-based scoring was compared with rationale-based scoring. The study
found in general essay-based scoring performed better than rationale-based
scoring with higher Quadratic Weighted Kappa (QWK). However, rationale-based
scoring led to higher scoring accuracy in terms of F1 scores for score 0 which
had less representation due to class imbalance issues. The ensemble modeling of
essay-based scoring models increased the scoring accuracy at both specific
score levels and across all score levels. The ensemble modeling of essay-based
scoring and each of the rationale-based scoring performed about the same.
Further ensemble of essay-based scoring and both rationale-based scoring
yielded the best scoring accuracy with QWK of 0.870 compared with 0.848
reported in literature.

</details>


### [38] [FairAD: Computationally Efficient Fair Graph Clustering via Algebraic Distance](https://arxiv.org/abs/2510.27136)
*Minh Phu Vuong,Young-Ju Lee,Iván Ojeda-Ruiz,Chul-Ho Lee*

Main category: cs.LG

TL;DR: 提出 FairAD，一种高效的公平图聚类方法，通过基于代数距离构建新颖的亲和矩阵并进行图缩减来实现多组公平约束下的聚类，在六个公开数据集上的实验中实现公平性与速度的折中优势，最快可比现有方法快40倍。


<details>
  <summary>Details</summary>
Motivation: 解决将公平性约束嵌入大规模图聚类的计算挑战，确保每个簇中保护群体的比例与全局一致性，同时提升计算效率以适应大规模图数据。

Method: 在亲和矩阵上引入代数距离概念，构建一个能直接将公平性约束嵌入的新型矩阵；对该矩阵执行图缩减以获得代表性节点，从而形成k个簇的候选结构；最后通过带约束的最小化问题求解最终的公平聚类结果。

Result: 在改良的随机块模型和六个公开数据集上，FairAD在实现聚类公平性的同时显著提升计算效率，与最前沿的公平图聚类方法相比，速度提升可达约40倍。

Conclusion: FairAD 提供一个高效且可扩展的公平图聚类框架，成功在公平性与计算效率之间取得平衡。然而对代数距离与参数敏感性、以及在不同公平性定义下的鲁棒性仍需进一步评估与扩展，未来可在更广泛的数据集与多种公平性约束下进行对比与改进。

Abstract: Due to the growing concern about unsavory behaviors of machine learning
models toward certain demographic groups, the notion of 'fairness' has recently
drawn much attention from the community, thereby motivating the study of
fairness in graph clustering. Fair graph clustering aims to partition the set
of nodes in a graph into $k$ disjoint clusters such that the proportion of each
protected group within each cluster is consistent with the proportion of that
group in the entire dataset. It is, however, computationally challenging to
incorporate fairness constraints into existing graph clustering algorithms,
particularly for large graphs. To address this problem, we propose FairAD, a
computationally efficient fair graph clustering method. It first constructs a
new affinity matrix based on the notion of algebraic distance such that
fairness constraints are imposed. A graph coarsening process is then performed
on this affinity matrix to find representative nodes that correspond to $k$
clusters. Finally, a constrained minimization problem is solved to obtain the
solution of fair clustering. Experiment results on the modified stochastic
block model and six public datasets show that FairAD can achieve fair
clustering while being up to 40 times faster compared to state-of-the-art fair
graph clustering algorithms.

</details>


### [39] [Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores](https://arxiv.org/abs/2510.27145)
*Sein Kwon,Seulgi Baek,Hyunseo Yang,Youngwan Jo,Sanghyun Park*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Database Management Systems (DBMSs) are fundamental for managing large-scale
and heterogeneous data, and their performance is critically influenced by
configuration parameters. Effective tuning of these parameters is essential for
adapting to diverse workloads and maximizing throughput while minimizing
latency. Recent research has focused on automated configuration optimization
using machine learning; however, existing approaches still exhibit several key
limitations. Most tuning frameworks disregard the dependencies among
parameters, assuming that each operates independently. This simplification
prevents optimizers from leveraging relational effects across parameters,
limiting their capacity to capture performancesensitive interactions. Moreover,
to reduce the complexity of the high-dimensional search space, prior work often
selects only the top few parameters for optimization, overlooking others that
contribute meaningfully to performance. Bayesian Optimization (BO), the most
common method for automatic tuning, is also constrained by its reliance on
surrogate models, which can lead to unstable predictions and inefficient
exploration. To overcome these limitations, we propose RelTune, a novel
framework that represents parameter dependencies as a Relational Graph and
learns GNN-based latent embeddings that encode performancerelevant semantics.
RelTune further introduces Hybrid-Score-Guided Bayesian Optimization (HBO),
which combines surrogate predictions with an Affinity Score measuring proximity
to previously high-performing configurations. Experimental results on multiple
DBMSs and workloads demonstrate that RelTune achieves faster convergence and
higher optimization efficiency than conventional BO-based methods, achieving
state-of-the-art performance across all evaluated scenarios.

</details>


### [40] [Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler](https://arxiv.org/abs/2510.27172)
*Zixuan Hu,Li Shen,Zhenyi Wang,Yongxian Wei,Dacheng Tao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service
for large language models. Existing defense strategies preemptively build
robustness via attack simulation but suffer from fundamental limitations: (i)
the infeasibility of extending attack simulations beyond bounded threat models
due to the inherent difficulty of anticipating unknown attacks, and (ii)
limited adaptability to varying attack settings, as simulation fails to capture
their variability and complexity. To address these challenges, we propose
Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with
no need for attack simulation. BDS formulates harmful fine-tuning defense as a
Bayesian inference problem, learning the posterior distribution of each data
point's safety attribute, conditioned on the fine-tuning and alignment
datasets. The fine-tuning process is then constrained by weighting data with
their safety attributes sampled from the posterior, thus mitigating the
influence of harmful data. By leveraging the post hoc nature of Bayesian
inference, the posterior is conditioned on the fine-tuning dataset, enabling
BDS to tailor its defense to the specific dataset, thereby achieving adaptive
defense. Furthermore, we introduce a neural scheduler based on amortized
Bayesian learning, enabling efficient transfer to new data without retraining.
Comprehensive results across diverse attack and defense settings demonstrate
the state-of-the-art performance of our approach. Code is available at
https://github.com/Egg-Hu/Bayesian-Data-Scheduler.

</details>


### [41] [A Polynomial-time Algorithm for Online Sparse Linear Regression with Improved Regret Bound under Weaker Conditions](https://arxiv.org/abs/2510.27177)
*Junfan Li,Shizhong Liao,Zenglin Xu,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出一种基于 Dantzig 选择器的多项式时间在线稀疏线性回归算法，在仅访问每个样本的 k 个特征的条件下，在兼容性条件下显著改进了挖掘误差界，并扩展到可获得额外 k0 个特征的情形。


<details>
  <summary>Details</summary>
Motivation: 在线稀疏线性回归（OSLR）在仅能对每个样本访问有限特征的设定下被证明是 NP-hard。以往工作在数据矩阵满足线性特征独立性、兼容性条件或受限等距性等假设时才有多项式时间算法。本工作在比上述更弱的兼容性条件下，给出改进的高效算法及更优的渐进界（ regret）。

Method: 算法核心基于 Dantzig 选择器，并引入若干新技术：1）基于算法的协方差估计的采样方案；2）自适应参数调优；3）带合适初始值的分组在线牛顿步；4）对误差的 l1 范数进行诱导分析；5）对非独立随机变量协方差和渐进界进行细致分析；6）对总体 regret 进行分解。并将算法扩展至可在每轮预测后观测额外的 k0 个特征的情形。

Result: 与在兼容条件下的 Ito et al. (2017) 的结果相比，本文算法在渐进上获得显著的改进的 regret 上界；且在保留多项式时间复杂度的前提下，提供了对较弱设定下的更强收敛性。理论分析包含对 l1-范数误差的诱导分析、非独立变量协方差的细致分析以及对 regret 的分解。扩展部分给出对具有额外观测的 OSLR 的改进界，覆盖 Kale et al. (2017) 与 Ito et al. (2017) 的相关结论。

Conclusion: 该方法在较弱的兼容性条件下实现了对 OS LR 的高效求解与更紧的误差收敛，拓展了应用范围，并提供了新的分析技术（如诱导性盲法分析、非独立变量情形的协方差分析、以及对 regret 的分解）以支撑结果。

Abstract: In this paper, we study the problem of online sparse linear regression (OSLR)
where the algorithms are restricted to accessing only $k$ out of $d$ attributes
per instance for prediction, which was proved to be NP-hard. Previous work gave
polynomial-time algorithms assuming the data matrix satisfies the linear
independence of features, the compatibility condition, or the restricted
isometry property. We introduce a new polynomial-time algorithm, which
significantly improves previous regret bounds (Ito et al., 2017) under the
compatibility condition that is weaker than the other two assumptions. The
improvements benefit from a tighter convergence rate of the $\ell_1$-norm error
of our estimators. Our algorithm leverages the well-studied Dantzig Selector,
but importantly with several novel techniques, including an algorithm-dependent
sampling scheme for estimating the covariance matrix, an adaptive parameter
tuning scheme, and a batching online Newton step with careful initializations.
We also give novel and non-trivial analyses, including an induction method for
analyzing the $\ell_1$-norm error, careful analyses on the covariance of
non-independent random variables, and a decomposition on the regret. We further
extend our algorithm to OSLR with additional observations where the algorithms
can observe additional $k_0$ attributes after each prediction, and improve
previous regret bounds (Kale et al., 2017; Ito et al., 2017).

</details>


### [42] [SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference](https://arxiv.org/abs/2510.27182)
*Zongshun Zhang,Ibrahim Matta*

Main category: cs.LG

TL;DR: SERFLOW是一种混合FaaS和IaaS的按阶段资源调度框架，通过对ML推理请求在各阶段的退出概率建模，动态分配资源以降低成本并适应动态工作负载。


<details>
  <summary>Details</summary>
Motivation: 现实场景中的VM冷启动、长尾服务时间分布等因素使单一资源配置难以同时优化时延和成本，需要对不同阶段的资源进行感知与分配，并在不同资源类型之间进行高效负载均衡。

Method: 将每个ML查询建模为一个无环阶段序列，每个阶段对应一组稀疏模型参数，最终在内部或最终分类器处退出。由于阶段出口率随输入而异，提出SERFLOW：基于FaaS容器的服务器无状态执行与阶段特定的资源 provisioning，并结合基于请求进入时的自适应负载均衡在VM与无服务器函数之间分配资源，考虑各阶段的退出比例来优化成本与性能。

Result: 在动态工作负载下， SERFLOW 将云成本降低超过23%，并能有效适应请求涌入和阶段性退出分布等现实因素。

Conclusion: 阶段感知的混合资源提供策略（结合FaaS与IaaS）能够在ML推理应用中实现更优的成本-性能权衡，特别是在存在不同阶段退出分布的场景。

Abstract: Dynamic offloading of Machine Learning (ML) model partitions across different
resource orchestration services, such as Function-as-a-Service (FaaS) and
Infrastructure-as-a-Service (IaaS), can balance processing and transmission
delays while minimizing costs of adaptive inference applications. However,
prior work often overlooks real-world factors, such as Virtual Machine (VM)
cold starts, requests under long-tail service time distributions, etc. To
tackle these limitations, we model each ML query (request) as traversing an
acyclic sequence of stages, wherein each stage constitutes a contiguous block
of sparse model parameters ending in an internal or final classifier where
requests may exit. Since input-dependent exit rates vary, no single resource
configuration suits all query distributions. IaaS-based VMs become
underutilized when many requests exit early, yet rapidly scaling to handle
request bursts reaching deep layers is impractical. SERFLOW addresses this
challenge by leveraging FaaS-based serverless functions (containers) and using
stage-specific resource provisioning that accounts for the fraction of requests
exiting at each stage. By integrating this provisioning with adaptive load
balancing across VMs and serverless functions based on request ingestion,
SERFLOW reduces cloud costs by over $23\%$ while efficiently adapting to
dynamic workloads.

</details>


### [43] [MDAS-GNN: Multi-Dimensional Spatiotemporal GNN with Spatial Diffusion for Urban Traffic Risk Forecasting](https://arxiv.org/abs/2510.27197)
*Ziyuan Gao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Traffic accidents represent a critical public health challenge, claiming over
1.35 million lives annually worldwide. Traditional accident prediction models
treat road segments independently, failing to capture complex spatial
relationships and temporal dependencies in urban transportation networks. This
study develops MDAS-GNN, a Multi-Dimensional Attention-based Spatial-diffusion
Graph Neural Network integrating three core risk dimensions: traffic safety,
infrastructure, and environmental risk. The framework employs feature-specific
spatial diffusion mechanisms and multi-head temporal attention to capture
dependencies across different time horizons. Evaluated on UK Department for
Transport accident data across Central London, South Manchester, and SE
Birmingham, MDASGNN achieves superior performance compared to established
baseline methods. The model maintains consistently low prediction errors across
short, medium, and long-term periods, with particular strength in long-term
forecasting. Ablation studies confirm that integrated multi-dimensional
features outperform singlefeature approaches, reducing prediction errors by up
to 40%. This framework provides civil engineers and urban planners with
advanced predictive capabilities for transportation infrastructure design,
enabling data-driven decisions for road network optimization, infrastructure
resource improvements, and strategic safety interventions in urban development
projects.

</details>


### [44] [Feature-Function Curvature Analysis: A Geometric Framework for Explaining Differentiable Models](https://arxiv.org/abs/2510.27207)
*Hamed Najafi,Dongsheng Luo,Jason Liu*

Main category: cs.LG

TL;DR: FFCA提出特征-函数曲率分析，并扩展为动态原型分析，通过对每个特征的4维签名（影响、波动性、非线性、交互）及其随训练演化的轨迹，揭示分层学习、提升信任度，并提供容量与早期过拟合的诊断工具。


<details>
  <summary>Details</summary>
Motivation: 传统的归因方法给出单一分数，难以捕捉非线性与特征之间的交互，以及模型在训练过程中的学习机制；需要从几何角度解释学习函数并建立动态监控。

Method: 为每个特征构建4维签名：影响、波动、非线性、交互；对学习函数的几何特征进行曲率分析；将其扩展为动态原型分析，跟踪训练全过程中的签名演化；通过系统实验验证分层学习、容量不足诊断和过拟合预警。

Result: static与dynamic两部分共同提供几何化的解释框架；实证表明模型先学习简单线性效应，再学习复杂交互；动态分析可用于检测容量不足、预测过拟合的到来，并提升解释的可信度。

Conclusion: FFCA将解释从纯量化转向几何化的全过程描述，静态与动态组件共同提升对学习过程的理解，帮助构建更可信的模型。

Abstract: Explainable AI (XAI) is critical for building trust in complex machine
learning models, yet mainstream attribution methods often provide an
incomplete, static picture of a model's final state. By collapsing a feature's
role into a single score, they are confounded by non-linearity and
interactions. To address this, we introduce Feature-Function Curvature Analysis
(FFCA), a novel framework that analyzes the geometry of a model's learned
function. FFCA produces a 4-dimensional signature for each feature, quantifying
its: (1) Impact, (2) Volatility, (3) Non-linearity, and (4) Interaction.
Crucially, we extend this framework into Dynamic Archetype Analysis, which
tracks the evolution of these signatures throughout the training process. This
temporal view moves beyond explaining what a model learned to revealing how it
learns. We provide the first direct, empirical evidence of hierarchical
learning, showing that models consistently learn simple linear effects before
complex interactions. Furthermore, this dynamic analysis provides novel,
practical diagnostics for identifying insufficient model capacity and
predicting the onset of overfitting. Our comprehensive experiments demonstrate
that FFCA, through its static and dynamic components, provides the essential
geometric context that transforms model explanation from simple quantification
to a nuanced, trustworthy analysis of the entire learning process.

</details>


### [45] [Not All Instances Are Equally Valuable: Towards Influence-Weighted Dataset Distillation](https://arxiv.org/abs/2510.27253)
*Qiyan Deng,Changqian Zheng,Lianpeng Qiao,Yuping Wang,Chengliang Chai,Lei Cao*

Main category: cs.LG

TL;DR: 提出一种基于影响函数的影响加权蒸馏框架(IWD)，通过对样本在蒸馏目标上的预计影响来自适应加权，以提升蒸馏数据集质量和模型性能（提升可达7.8%）。


<details>
  <summary>Details</summary>
Motivation: 真实数据集往往包含信息量大、冗余或有害的样本；在蒸馏过程中若不考虑数据质量，可能降低模型性能。需要将数据质量纳入蒸馏的优化目标。

Method: 使用影响函数估计每个样本对蒸馏目标的影响，给样本分配自适应权重，优先保留有益数据、削弱无用或有害数据。框架具模块化设计，能无缝整合到现有的蒸馏框架中。

Result: 经验结果表明，结合IWD后，蒸馏数据集质量和模型性能有所提升，准确率提升可达7.8%。

Conclusion: IWD提供了一个基于数据质量的原则性且可模块化的蒸馏改进方法，通过影响函数对样本进行加权，在多种蒸馏框架中均可实现性能提升。

Abstract: Dataset distillation condenses large datasets into synthetic subsets,
achieving performance comparable to training on the full dataset while
substantially reducing storage and computation costs. Most existing dataset
distillation methods assume that all real instances contribute equally to the
process. In practice, real-world datasets contain both informative and
redundant or even harmful instances, and directly distilling the full dataset
without considering data quality can degrade model performance. In this work,
we present Influence-Weighted Distillation IWD, a principled framework that
leverages influence functions to explicitly account for data quality in the
distillation process. IWD assigns adaptive weights to each instance based on
its estimated impact on the distillation objective, prioritizing beneficial
data while downweighting less useful or harmful ones. Owing to its modular
design, IWD can be seamlessly integrated into diverse dataset distillation
frameworks. Our empirical results suggest that integrating IWD tends to improve
the quality of distilled datasets and enhance model performance, with accuracy
gains of up to 7.8%.

</details>


### [46] [ECVL-ROUTER: Scenario-Aware Routing for Vision-Language Models](https://arxiv.org/abs/2510.27256)
*Xin Tang,Youfang Han,Fangfei Gou,Wei Zhao,Xin Meng,Yang Yu,Jinguo Zhang,Yuanchun Shi,Yuntao Wang,Tengxiang Zhang*

Main category: cs.LG

TL;DR: 提出 ECVL-ROUTER，一种针对视觉-语言模型的场景感知路由框架，通过在大模型与小模型之间动态分配任务，提升总体效用。


<details>
  <summary>Details</summary>
Motivation: 用户在不同场景对响应速度、输出质量和能耗的偏好不同，单一云端大模型或本地小模型无法同时满足所有需求，因此需要一种能够依据需求进行智能路由的方法。

Method: 引入新的路由策略与评估指标，基于查询特征和用户需求决定路由对象；构建面向路由训练的多模态响应质量数据集；通过大量实验验证路由的有效性。

Result: 在实验中，超过80%的查询被路由到小模型，且问题解决概率仅下降不到10%。

Conclusion: 该工作展示了场景感知路由在 VLMs 中的可行性与有效性，为混合大/小模型系统提供了可扩展的框架和数据资源。

Abstract: Vision-Language Models (VLMs) excel in diverse multimodal tasks. However,
user requirements vary across scenarios, which can be categorized into fast
response, high-quality output, and low energy consumption. Relying solely on
large models deployed in the cloud for all queries often leads to high latency
and energy cost, while small models deployed on edge devices are capable of
handling simpler tasks with low latency and energy cost. To fully leverage the
strengths of both large and small models, we propose ECVL-ROUTER, the first
scenario-aware routing framework for VLMs. Our approach introduces a new
routing strategy and evaluation metrics that dynamically select the appropriate
model for each query based on user requirements, maximizing overall utility. We
also construct a multimodal response-quality dataset tailored for router
training and validate the approach through extensive experiments. Results show
that our approach successfully routes over 80\% of queries to the small model
while incurring less than 10\% drop in problem solving probability.

</details>


### [47] [ODP-Bench: Benchmarking Out-of-Distribution Performance Prediction](https://arxiv.org/abs/2510.27263)
*Han Yu,Kehan Li,Dongbai Li,Yue He,Xingxuan Zhang,Peng Cui*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recently, there has been gradually more attention paid to Out-of-Distribution
(OOD) performance prediction, whose goal is to predict the performance of
trained models on unlabeled OOD test datasets, so that we could better leverage
and deploy off-the-shelf trained models in risk-sensitive scenarios. Although
progress has been made in this area, evaluation protocols in previous
literature are inconsistent, and most works cover only a limited number of
real-world OOD datasets and types of distribution shifts. To provide convenient
and fair comparisons for various algorithms, we propose Out-of-Distribution
Performance Prediction Benchmark (ODP-Bench), a comprehensive benchmark that
includes most commonly used OOD datasets and existing practical performance
prediction algorithms. We provide our trained models as a testbench for future
researchers, thus guaranteeing the consistency of comparison and avoiding the
burden of repeating the model training process. Furthermore, we also conduct
in-depth experimental analyses to better understand their capability boundary.

</details>


### [48] [HiF-DTA: Hierarchical Feature Learning Network for Drug-Target Affinity Prediction](https://arxiv.org/abs/2510.27281)
*Minghui Li,Yuanhang Wang,Peijin Guo,Wei Wan,Shengshan Hu,Shengqing Hu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate prediction of Drug-Target Affinity (DTA) is crucial for reducing
experimental costs and accelerating early screening in computational drug
discovery. While sequence-based deep learning methods avoid reliance on costly
3D structures, they still overlook simultaneous modeling of global sequence
semantic features and local topological structural features within drugs and
proteins, and represent drugs as flat sequences without atomic-level,
substructural-level, and molecular-level multi-scale features. We propose
HiF-DTA, a hierarchical network that adopts a dual-pathway strategy to extract
both global sequence semantic and local topological features from drug and
protein sequences, and models drugs multi-scale to learn atomic, substructural,
and molecular representations fused via a multi-scale bilinear attention
module. Experiments on Davis, KIBA, and Metz datasets show HiF-DTA outperforms
state-of-the-art baselines, with ablations confirming the importance of
global-local extraction and multi-scale fusion.

</details>


### [49] [Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in Enterprise Environments](https://arxiv.org/abs/2510.27287)
*Harsh Vishwakarma,Ankush Agarwal,Ojas Patil,Chaitanya Devaguptapu,Mahesh Chandran*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Enterprise systems are crucial for enhancing productivity and decision-making
among employees and customers. Integrating LLM based systems into enterprise
systems enables intelligent automation, personalized experiences, and efficient
information retrieval, driving operational efficiency and strategic growth.
However, developing and evaluating such systems is challenging due to the
inherent complexity of enterprise environments, where data is fragmented across
multiple sources and governed by sophisticated access controls. We present
EnterpriseBench, a comprehensive benchmark that simulates enterprise settings,
featuring 500 diverse tasks across software engineering, HR, finance, and
administrative domains. Our benchmark uniquely captures key enterprise
characteristics including data source fragmentation, access control
hierarchies, and cross-functional workflows. Additionally, we provide a novel
data generation pipeline that creates internally consistent enterprise tasks
from organizational metadata. Experiments with state-of-the-art LLM agents
demonstrate that even the most capable models achieve only 41.8% task
completion, highlighting significant opportunities for improvement in
enterprise-focused AI systems.

</details>


### [50] [Temporal Cardiovascular Dynamics for Improved PPG-Based Heart Rate Estimation](https://arxiv.org/abs/2510.27297)
*Berken Utku Demirel,Christian Holz*

Main category: cs.LG

TL;DR: 提出了一种基于互信息的非线性混沌心率建模的新方法，结合深度学习，在真实场景条件下显著提升心率估计准确性，并可减少传感模态数量与后处理步骤，四数据集实证，提升高达40%。


<details>
  <summary>Details</summary>
Motivation: 心率波动具有固有的非线性、混沌性，难以在日常生活健康监测中得到稳定、准确的估计；需要从理论层面处理其非线性 temporal 复杂性，并提升与深度学习的融合效果。

Method: 通过研究心率的非线性混沌行为，利用互信息等工具提出一种新颖的心率估计增强方法，并将其与深度学习相结合，进行广泛的消融实验与对比，基于四个真实场景数据集进行验证。

Result: 在与传统方法和现有机器学习方法的对比中，所提方法在心率估计上实现显著提升，达到约40%的性能增益，同时减少对多传感模态的依赖，并消除了后处理步骤的需要。

Conclusion: 提出的方法有效地从数学层面处理非线性时序复杂性，并能提升深度学习系统的性能，证据来自四个真实数据集的实验结果。

Abstract: The oscillations of the human heart rate are inherently complex and
non-linear -- they are best described by mathematical chaos, and they present a
challenge when applied to the practical domain of cardiovascular health
monitoring in everyday life. In this work, we study the non-linear chaotic
behavior of heart rate through mutual information and introduce a novel
approach for enhancing heart rate estimation in real-life conditions. Our
proposed approach not only explains and handles the non-linear temporal
complexity from a mathematical perspective but also improves the deep learning
solutions when combined with them. We validate our proposed method on four
established datasets from real-life scenarios and compare its performance with
existing algorithms thoroughly with extensive ablation experiments. Our results
demonstrate a substantial improvement, up to 40\%, of the proposed approach in
estimating heart rate compared to traditional methods and existing
machine-learning techniques while reducing the reliance on multiple sensing
modalities and eliminating the need for post-processing steps.

</details>


### [51] [Un-Attributability: Computing Novelty From Retrieval & Semantic Similarity](https://arxiv.org/abs/2510.27313)
*Philipp Davydov,Ameya Prabhu,Matthias Bethge,Elisa Nguyen,Seong Joon Oh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding how language-model outputs relate to the pretraining corpus is
central to studying model behavior. Most training data attribution (TDA)
methods ask which training examples causally influence a given output, often
using leave-one-out tests. We invert the question: which outputs cannot be
attributed to any pretraining example? We introduce un-attributability as an
operational measure of semantic novelty: an output is novel if the pretraining
corpus contains no semantically similar context. We approximate this with a
simple two-stage retrieval pipeline: index the corpus with lightweight GIST
embeddings, retrieve the top-n candidates, then rerank with ColBERTv2. If the
nearest corpus item is less attributable than a human-generated text reference,
we consider the output of the model as novel. We evaluate on SmolLM and SmolLM2
and report three findings: (1) models draw on pretraining data across much
longer spans than previously reported; (2) some domains systematically promote
or suppress novelty; and (3) instruction tuning not only alters style but also
increases novelty. Reframing novelty assessment around un-attributability
enables efficient analysis at pretraining scale. We release ~20 TB of corpus
chunks and index artifacts to support replication and large-scale extension of
our analysis at https://huggingface.co/datasets/stai-tuebingen/faiss-smollm

</details>


### [52] [MedM2T: A MultiModal Framework for Time-Aware Modeling with Electronic Health Record and Electrocardiogram Data](https://arxiv.org/abs/2510.27321)
*Yu-Chen Kuo,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The inherent multimodality and heterogeneous temporal structures of medical
data pose significant challenges for modeling. We propose MedM2T, a time-aware
multimodal framework designed to address these complexities. MedM2T integrates:
(i) Sparse Time Series Encoder to flexibly handle irregular and sparse time
series, (ii) Hierarchical Time-Aware Fusion to capture both micro- and
macro-temporal patterns from multiple dense time series, such as ECGs, and
(iii) Bi-Modal Attention to extract cross-modal interactions, which can be
extended to any number of modalities. To mitigate granularity gaps between
modalities, MedM2T uses modality-specific pre-trained encoders and aligns
resulting features within a shared encoder. We evaluated MedM2T on MIMIC-IV and
MIMIC-IV-ECG datasets for three tasks that encompass chronic and acute disease
dynamics: 90-day cardiovascular disease (CVD) prediction, in-hospital mortality
prediction, and ICU length-of-stay (LOS) regression. MedM2T outperformed
state-of-the-art multimodal learning frameworks and existing time series
models, achieving an AUROC of 0.947 and an AUPRC of 0.706 for CVD prediction;
an AUROC of 0.901 and an AUPRC of 0.558 for mortality prediction; and Mean
Absolute Error (MAE) of 2.31 for LOS regression. These results highlight the
robustness and broad applicability of MedM2T, positioning it as a promising
tool in clinical prediction. We provide the implementation of MedM2T at
https://github.com/DHLab-TSENG/MedM2T.

</details>


### [53] [Reasoning Models Sometimes Output Illegible Chains of Thought](https://arxiv.org/abs/2510.27338)
*Arun Jose*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Language models trained via outcome-based reinforcement learning (RL) to
reason using chain-of-thought (CoT) have shown remarkable performance.
Monitoring such a model's CoT may allow us to understand its intentions and
detect potential malicious behavior. However, to be effective, this requires
that CoTs are legible and faithful. We study CoT legibility across 14 reasoning
models, finding that RL often causes reasoning to become illegible to both
humans and AI monitors, with reasoning models (except Claude) generating
illegible CoTs while returning to perfectly readable final answers. We show
that models use illegible reasoning to reach correct answers (accuracy dropping
by 53\% when forced to use only legible portions), yet find no correlation
between legibility and performance when resampling - suggesting the
relationship is more nuanced. We also find that legibility degrades on harder
questions. We discuss potential hypotheses for these results, including
steganography, training artifacts, and vestigial tokens. These results suggest
that without explicit optimization for legibility, outcome-based RL naturally
produces models with increasingly opaque reasoning processes, potentially
undermining monitoring approaches.

</details>


### [54] [FedMuon: Accelerating Federated Learning with Matrix Orthogonalization](https://arxiv.org/abs/2510.27403)
*Junkang Liu,Fanhua Shang,Junchao Zhou,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The core bottleneck of Federated Learning (FL) lies in the communication
rounds. That is, how to achieve more effective local updates is crucial for
reducing communication rounds. Existing FL methods still primarily use
element-wise local optimizers (Adam/SGD), neglecting the geometric structure of
the weight matrices. This often leads to the amplification of pathological
directions in the weights during local updates, leading deterioration in the
condition number and slow convergence. Therefore, we introduce the Muon
optimizer in local, which has matrix orthogonalization to optimize
matrix-structured parameters. Experimental results show that, in IID setting,
Local Muon significantly accelerates the convergence of FL and reduces
communication rounds compared to Local SGD and Local AdamW. However, in non-IID
setting, independent matrix orthogonalization based on the local distributions
of each client induces strong client drift. Applying Muon in non-IID FL poses
significant challenges: (1) client preconditioner leading to client drift; (2)
moment reinitialization. To address these challenges, we propose a novel
Federated Muon optimizer (FedMuon), which incorporates two key techniques: (1)
momentum aggregation, where clients use the aggregated momentum for local
initialization; (2) local-global alignment, where the local gradients are
aligned with the global update direction to significantly reduce client drift.
Theoretically, we prove that \texttt{FedMuon} achieves a linear speedup
convergence rate without the heterogeneity assumption, where $S$ is the number
of participating clients per round, $K$ is the number of local iterations, and
$R$ is the total number of communication rounds. Empirically, we validate the
effectiveness of FedMuon on language and vision models. Compared to several
baselines, FedMuon significantly reduces communication rounds and improves test
accuracy.

</details>


### [55] [Atlas-Alignment: Making Interpretability Transferable Across Language Models](https://arxiv.org/abs/2510.27413)
*Bruno Puri,Jim Berend,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Interpretability is crucial for building safe, reliable, and controllable
language models, yet existing interpretability pipelines remain costly and
difficult to scale. Interpreting a new model typically requires costly training
of model-specific sparse autoencoders, manual or semi-automated labeling of SAE
components, and their subsequent validation. We introduce Atlas-Alignment, a
framework for transferring interpretability across language models by aligning
unknown latent spaces to a Concept Atlas - a labeled, human-interpretable
latent space - using only shared inputs and lightweight representational
alignment techniques. Once aligned, this enables two key capabilities in
previously opaque models: (1) semantic feature search and retrieval, and (2)
steering generation along human-interpretable atlas concepts. Through
quantitative and qualitative evaluations, we show that simple representational
alignment methods enable robust semantic retrieval and steerable generation
without the need for labeled concept data. Atlas-Alignment thus amortizes the
cost of explainable AI and mechanistic interpretability: by investing in one
high-quality Concept Atlas, we can make many new models transparent and
controllable at minimal marginal cost.

</details>


### [56] [MVeLMA: Multimodal Vegetation Loss Modeling Architecture for Predicting Post-fire Vegetation Loss](https://arxiv.org/abs/2510.27443)
*Meenu Ravi,Shailik Sarkar,Yanshen Sun,Vaishnavi Singh,Chang-Tien Lu*

Main category: cs.LG

TL;DR: 提出一个多模态植被损失预测端到端 ML 管道 MVeLMA，用于县级火灾后植被损失预测，结合多模态特征融合、堆叠集成与不确定性建模，并生成高风险县的置信度地图。


<details>
  <summary>Details</summary>
Motivation: 理解火灾后长时间尺度的植被恢复过程需要可解释、准确的预测工具；现有工作在影响因素的模态、交互关系与可解释性方面不足，亟需一个统一的端到端框架来提升预测性能并提供不确定性信息。

Method: 提出多模态特征整合管线并采用堆叠式集成架构以捕捉不同模态及其交互，同时结合概率建模实现不确定性估计；通过训练得到县级层面的植被损失预测，并输出置信度地图。

Result: 在广泛的实验中，该模型超越了多种SOTA和基线方法，在预测火灾后植被损失方面具有更高准确性；并生成植被损失置信度地图，识别高风险县以辅助定向恢复。

Conclusion: 该方法对灾后救灾规划、生态政策制定和野生动物恢复管理具有潜在影响，未来工作可进一步扩展到更高分辨率、跨区域泛化能力及更丰富的不确定性解释。

Abstract: Understanding post-wildfire vegetation loss is critical for developing
effective ecological recovery strategies and is often challenging due to the
extended time and effort required to capture the evolving ecosystem features.
Recent works in this area have not fully explored all the contributing factors,
their modalities, and interactions with each other. Furthermore, most research
in this domain is limited by a lack of interpretability in predictive modeling,
making it less useful in real-world settings. In this work, we propose a novel
end-to-end ML pipeline called MVeLMA (\textbf{M}ultimodal \textbf{Ve}getation
\textbf{L}oss \textbf{M}odeling \textbf{A}rchitecture) to predict county-wise
vegetation loss from fire events. MVeLMA uses a multimodal feature integration
pipeline and a stacked ensemble-based architecture to capture different
modalities while also incorporating uncertainty estimation through
probabilistic modeling. Through comprehensive experiments, we show that our
model outperforms several state-of-the-art (SOTA) and baseline models in
predicting post-wildfire vegetation loss. Furthermore, we generate vegetation
loss confidence maps to identify high-risk counties, thereby helping targeted
recovery efforts. The findings of this work have the potential to inform future
disaster relief planning, ecological policy development, and wildlife recovery
management.

</details>


### [57] [Spectral Neural Graph Sparsification](https://arxiv.org/abs/2510.27474)
*Angelica Liguori,Ettore Ritacco,Pietro Sabatino,Annalisa Socievole*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Graphs are central to modeling complex systems in domains such as social
networks, molecular chemistry, and neuroscience. While Graph Neural Networks,
particularly Graph Convolutional Networks, have become standard tools for graph
learning, they remain constrained by reliance on fixed structures and
susceptibility to over-smoothing. We propose the Spectral Preservation Network,
a new framework for graph representation learning that generates reduced graphs
serving as faithful proxies of the original, enabling downstream tasks such as
community detection, influence propagation, and information diffusion at a
reduced computational cost. The Spectral Preservation Network introduces two
key components: the Joint Graph Evolution layer and the Spectral Concordance
loss. The former jointly transforms both the graph topology and the node
feature matrix, allowing the structure and attributes to evolve adaptively
across layers and overcoming the rigidity of static neighborhood aggregation.
The latter regularizes these transformations by enforcing consistency in both
the spectral properties of the graph and the feature vectors of the nodes. We
evaluate the effectiveness of Spectral Preservation Network on node-level
sparsification by analyzing well-established metrics and benchmarking against
state-of-the-art methods. The experimental results demonstrate the superior
performance and clear advantages of our approach.

</details>


### [58] [Simplex-to-Euclidean Bijections for Categorical Flow Matching](https://arxiv.org/abs/2510.27480)
*Bernardo Williams,Victor M. Yeom-Song,Marcelo Hartmann,Arto Klami*

Main category: cs.LG

TL;DR: 在Aitchinson几何下将单纯形上的分布映射到欧氏空间以进行密度建模和采样，并通过Dirichlet插值对离散观察进行去量化，从而可在欧氏空间学习并能精确重建原始离散分布；相比基于黎曼几何的方法具有简化且竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 需要在保持Aitchison几何属性的前提下，在欧氏空间进行密度建模和采样，同时实现对离散数据的去量化。

Method: 利用Aitchison几何定义的平滑双射，将开放单纯形映射到欧氏空间；采用Dirichlet插值将离散观测去量化为连续数据；在欧氏空间建模密度并通过映射实现对原始离散分布的精确恢复；与仅在单纯形上使用Riemann几何或自定义噪声过程的方法相比，该方法在欧氏空间工作同时保留Aitchison几何特性。

Result: 在合成数据和真实数据集上实现了与现有方法相当甚至竞争的性能。

Conclusion: 该方法为在保留Aitchison几何的同时在欧氏空间进行分布学习与采样提供了一条简便有效的途径，并可实现对离散分布的精确恢复。

Abstract: We propose a method for learning and sampling from probability distributions
supported on the simplex. Our approach maps the open simplex to Euclidean space
via smooth bijections, leveraging the Aitchison geometry to define the
mappings, and supports modeling categorical data by a Dirichlet interpolation
that dequantizes discrete observations into continuous ones. This enables
density modeling in Euclidean space through the bijection while still allowing
exact recovery of the original discrete distribution. Compared to previous
methods that operate on the simplex using Riemannian geometry or custom noise
processes, our approach works in Euclidean space while respecting the Aitchison
geometry, and achieves competitive performance on both synthetic and real-world
data sets.

</details>


### [59] [Thought Branches: Interpreting LLM Reasoning Requires Resampling](https://arxiv.org/abs/2510.27484)
*Uzay Macar,Paul C. Bogdan,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most work interpreting reasoning models studies only a single
chain-of-thought (CoT), yet these models define distributions over many
possible CoTs. We argue that studying a single sample is inadequate for
understanding causal influence and the underlying computation. Though fully
specifying this distribution is intractable, it can be understood by sampling.
We present case studies using resampling to investigate model decisions. First,
when a model states a reason for its action, does that reason actually cause
the action? In "agentic misalignment" scenarios, we resample specific sentences
to measure their downstream effects. Self-preservation sentences have small
causal impact, suggesting they do not meaningfully drive blackmail. Second, are
artificial edits to CoT sufficient for steering reasoning? These are common in
literature, yet take the model off-policy. Resampling and selecting a
completion with the desired property is a principled on-policy alternative. We
find off-policy interventions yield small and unstable effects compared to
resampling in decision-making tasks. Third, how do we understand the effect of
removing a reasoning step when the model may repeat it post-edit? We introduce
a resilience metric that repeatedly resamples to prevent similar content from
reappearing downstream. Critical planning statements resist removal but have
large effects when eliminated. Fourth, since CoT is sometimes "unfaithful", can
our methods teach us anything in these settings? Adapting causal mediation
analysis, we find that hints that have a causal effect on the output without
being explicitly mentioned exert a subtle and cumulative influence on the CoT
that persists even if the hint is removed. Overall, studying distributions via
resampling enables reliable causal analysis, clearer narratives of model
reasoning, and principled CoT interventions.

</details>


### [60] [FedAdamW: A Communication-Efficient Optimizer with Convergence and Generalization Guarantees for Federated Large Models](https://arxiv.org/abs/2510.27486)
*Junkang Liu,Fanhua Shang,Kewen Zhu,Hongying Liu,Yuanyuan Liu,Jin Liu*

Main category: cs.LG

TL;DR: 提出 FedAdamW，在联邦学习中通过本地纠正和解耦权重衰减降低方差、缓解客户端漂移，并实现线性加速的收敛，同时给出 PAC-Bayes 理由并在语言/视觉 Transformer 上证实。


<details>
  <summary>Details</summary>
Motivation: 解决在联邦学习场景中直接应用 AdamW 面临的方差、漂移和收敛慢等挑战。

Method: 提出 FedAdamW，包含本地纠正机制、解耦权重衰减、对第二矩估计的均值聚合并重置、在全局更新中对齐本地更新；提供理论收敛分析（无异质性假设的线性加速）以及 PAC-Bayes 泛化分析。

Result: 理论上实现了 O( sqrt( (L Δ σ_l^2) / (S K R ε^2) ) + (L Δ)/R ) 的收敛速率；经验上在语言与视觉 Transformer 上优于基线，显著减少通信轮次并提升测试准确率。

Conclusion: FedAdamW 能在联邦学习场景中稳定实现高效训练与微调，显著降低通信成本并提升泛化性能，理论与实验均支持其有效性；代码已开源。

Abstract: AdamW has become one of the most effective optimizers for training
large-scale models. We have also observed its effectiveness in the context of
federated learning (FL). However, directly applying AdamW in federated learning
settings poses significant challenges: (1) due to data heterogeneity, AdamW
often yields high variance in the second-moment estimate $\boldsymbol{v}$; (2)
the local overfitting of AdamW may cause client drift; and (3) Reinitializing
moment estimates ($\boldsymbol{v}$, $\boldsymbol{m}$) at each round slows down
convergence. To address these challenges, we propose the first
\underline{Fed}erated \underline{AdamW} algorithm, called \texttt{FedAdamW},
for training and fine-tuning various large models. \texttt{FedAdamW} aligns
local updates with the global update using both a \textbf{local correction
mechanism} and decoupled weight decay to mitigate local overfitting.
\texttt{FedAdamW} efficiently aggregates the \texttt{mean} of the second-moment
estimates to reduce their variance and reinitialize them. Theoretically, we
prove that \texttt{FedAdamW} achieves a linear speedup convergence rate of
$\mathcal{O}(\sqrt{(L \Delta \sigma_l^2)/(S K R \epsilon^2)}+(L \Delta)/R)$
without \textbf{heterogeneity assumption}, where $S$ is the number of
participating clients per round, $K$ is the number of local iterations, and $R$
is the total number of communication rounds. We also employ PAC-Bayesian
generalization analysis to explain the effectiveness of decoupled weight decay
in local training. Empirically, we validate the effectiveness of
\texttt{FedAdamW} on language and vision Transformer models. Compared to
several baselines, \texttt{FedAdamW} significantly reduces communication rounds
and improves test accuracy. The code is available in
https://github.com/junkangLiu0/FedAdamW.

</details>


### [61] [DP-FedPGN: Finding Global Flat Minima for Differentially Private Federated Learning via Penalizing Gradient Norm](https://arxiv.org/abs/2510.27504)
*Junkang Liu,Yuxuan Tian,Fanhua Shang,Yuanyuan Liu,Hongying Liu,Junchao Zhou,Daorui Ding*

Main category: cs.LG

TL;DR: 提出DP-FedPGN，通过引入全局梯度范数惩罚来寻找全局平整极小点，从而缓解DP下的性能下降并减少梯度截断误差，在ResNet和Transformer上对六个任务显示出显著改进，且提供Renyi DP隐私保证与对本地更新的敏感性分析。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，差分隐私保护会导致模型损失函数变得更加尖锐，降低泛化能力。现有以局部平整为目标的SAM在CL-DPFL中的局部平整感不一定能代表全局平整。数据异质性和DP噪声共同拉低性能，需要寻找全局平滑极小点并降低局部更新的范数以减小梯度裁剪误差，同时提供严格的隐私保证。

Method: 提出DP-FedPGN算法，在本地损失中引入全局梯度范数惩罚以寻找全局平整极小点；通过全局梯度范数惩罚同时实现更平的全局解并降低局部更新范数，从而减小梯度裁剪误差；结合Rényi差分隐私提供严格隐私保证，并对本地更新的敏感性进行分析；理论分析表明该方法可缓解DP带来的性能下降并对数据异质性的影响具有抵消作用，且实现快速收敛。

Result: 在ResNet和Transformer模型上对六个视觉和自然语言处理任务进行实验，DP-FedPGN相较现有最先进算法取得显著改进，且提供了对隐私与收敛的理论支撑，代码已公开。

Conclusion: DP-FedPGN通过全局梯度范数惩罚实现全局极小点的平滑化，降低局部更新范数与梯度裁剪误差，缓解DP带来的性能下降，抵消数据异质性影响，并实现更快的收敛与严格的隐私保护，是CL-DPFL领域的一个有效提升。

Abstract: To prevent inference attacks in Federated Learning (FL) and reduce the
leakage of sensitive information, Client-level Differentially Private Federated
Learning (CL-DPFL) is widely used. However, current CL-DPFL methods usually
result in sharper loss landscapes, which leads to a decrease in model
generalization after differential privacy protection. By using Sharpness Aware
Minimization (SAM), the current popular federated learning methods are to find
a local flat minimum value to alleviate this problem. However, the local
flatness may not reflect the global flatness in CL-DPFL. Therefore, to address
this issue and seek global flat minima of models, we propose a new CL-DPFL
algorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to
the local loss to find the global flat minimum. Moreover, by using our global
gradient norm penalty, we not only find a flatter global minimum but also
reduce the locally updated norm, which means that we further reduce the error
of gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN
mitigates the performance degradation caused by DP. Meanwhile, the proposed
DP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves
fast convergence. We also use R\'enyi DP to provide strict privacy guarantees
and provide sensitivity analysis for local updates. Finally, we conduct
effectiveness tests on both ResNet and Transformer models, and achieve
significant improvements in six visual and natural language processing tasks
compared to existing state-of-the-art algorithms. The code is available at
https://github.com/junkangLiu0/DP-FedPGN

</details>


### [62] [Leveraging Generic Time Series Foundation Models for EEG Classification](https://arxiv.org/abs/2510.27522)
*Théo Gnassounou,Yessin Moakher,Shifeng Xie,Vasilii Feofanov,Ievgen Redko*

Main category: cs.LG

TL;DR: 时间序列基础模型在跨域预训练后，能良好迁移到 EEG 任务（运动想象与睡眠分期），超越 EEGNet 与 CBraMod 基线。


<details>
  <summary>Details</summary>
Motivation: 揭示更通用的时间序列前沿模型是否能对脑信号等生物信号具备迁移性，减少对领域专用模型的依赖。

Method: 在 EEG 任务上测试一个最近提出的时间序列分类基础模型；对比两种预训练策略： (a) 来自多领域的异构真实时间序列；(b) 纯合成数据；并与 EEGNet、CBraMod 基线进行比较。

Result: 两种预训练策略均表现出色，持续优于 EEGNet 与 CBraMod。

Conclusion: 跨域、通用的时间序列基础模型在 EEG 上具有良好迁移性，表明 EEG 领域可从更广泛的时间序列研究中获益，促进脑信号分析的发展。

Abstract: Foundation models for time series are emerging as powerful general-purpose
backbones, yet their potential for domain-specific biomedical signals such as
electroencephalography (EEG) remains rather unexplored. In this work, we
investigate the applicability a recently proposed time series classification
foundation model, to a different EEG tasks such as motor imagery classification
and sleep stage prediction. We test two pretraining regimes: (a) pretraining on
heterogeneous real-world time series from multiple domains, and (b) pretraining
on purely synthetic data. We find that both variants yield strong performance,
consistently outperforming EEGNet, a widely used convolutional baseline, and
CBraMod, the most recent EEG-specific foundation model. These results suggest
that generalist time series foundation models, even when pretrained on data of
non-neural origin or on synthetic signals, can transfer effectively to EEG. Our
findings highlight the promise of leveraging cross-domain pretrained models for
brain signal analysis, suggesting that EEG may benefit from advances in the
broader time series literature.

</details>


### [63] [Active transfer learning for structural health monitoring](https://arxiv.org/abs/2510.27525)
*J. Poole,N. Dervilis,K. Worden,P. Gardner,V. Giglioni,R. S. Mills,A. J. Hughes*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data for training structural health monitoring (SHM) systems are often
expensive and/or impractical to obtain, particularly for labelled data.
Population-based SHM (PBSHM) aims to address this limitation by leveraging data
from multiple structures. However, data from different structures will follow
distinct distributions, potentially leading to large generalisation errors for
models learnt via conventional machine learning methods. To address this issue,
transfer learning -- in the form of domain adaptation (DA) -- can be used to
align the data distributions. Most previous approaches have only considered
\emph{unsupervised} DA, where no labelled target data are available; they do
not consider how to incorporate these technologies in an online framework --
updating as labels are obtained throughout the monitoring campaign. This paper
proposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DA
mappings using a limited quantity of labelled target data. In addition, this
model is integrated into an active sampling strategy to guide inspections to
select the most informative observations to label -- leading to further
reductions in the required labelled data to learn a target classifier. The
effectiveness of this methodology is evaluated on a population of experimental
bridges. Specifically, this population includes data corresponding to several
damage states, as well as, a comprehensive set of environmental conditions. It
is found that combining transfer learning and active learning can improve data
efficiency when learning classification models in label-scarce scenarios. This
result has implications for data-informed operation and maintenance of
structures, suggesting a reduction in inspections over the operational lifetime
of a structure -- and therefore a reduction in operational costs -- can be
achieved.

</details>


### [64] [TetraJet-v2: Accurate NVFP4 Training for Large Language Models with Oscillation Suppression and Outlier Control](https://arxiv.org/abs/2510.27527)
*Yuxiang Chen,Xiaoming Xu,Pengle Zhang,Michael Beyer,Martin Rapp,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: TetraJet-v2 实现端到端的4-bit 全量化训练，全面使用 NVFP4 于激活、权重与梯度，解决权重振荡与离群值问题，显著缩小与全精度训练的差距。


<details>
  <summary>Details</summary>
Motivation: LLM 训练成本极高，低精度全量化训练（FQT）具备潜力以显著降低成本。但现有4位格式在接近无损训练方面仍有挑战，需解决权重振荡和离群值等关键问题。

Method: 提出三大核心方法：1) 针对 NVFP4 的线性层实现无偏的双块量化；2) OsciReset，用以抑制权重振荡；3) OutControl，保留离群值的精度。将 NVFP4 应用于所有线性层的激活、权重和梯度，形成端到端4-bit FQT 流程。

Result: 在不同模型规模（高达370M）和数据量（高达200B token）的LLM预训练任务中，优于先前的FP4训练方法，平均将与全精度训练的性能差距缩小约51.3%。

Conclusion: 端到端4-bit FQT 在解决关键难题后具备显著的性能潜力，TetraJet-v2 为低精度LLM训练提供了可行、高效的实现方案。

Abstract: Large Language Models (LLMs) training is prohibitively expensive, driving
interest in low-precision fully-quantized training (FQT). While novel 4-bit
formats like NVFP4 offer substantial efficiency gains, achieving near-lossless
training at such low precision remains challenging. We introduce TetraJet-v2,
an end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,
and gradients in all linear layers. We identify two critical issues hindering
low-precision LLM training: weight oscillation and outliers. To address these,
we propose: 1) an unbiased double-block quantization method for NVFP4 linear
layers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)
OutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently
outperforms prior FP4 training methods on pre-training LLMs across varying
model sizes up to 370M and data sizes up to 200B tokens, reducing the
performance gap to full-precision training by an average of 51.3%.

</details>


### [65] [AstuteRAG-FQA: Task-Aware Retrieval-Augmented Generation Framework for Proprietary Data Challenges in Financial Question Answering](https://arxiv.org/abs/2510.27537)
*Mohammad Zahangir Alam,Khandoker Ashik Uz Zaman,Mahdi H. Miraz*

Main category: cs.LG

TL;DR: 提出 AstuteRAG-FQA，一种面向金融问答的自适应RAG框架，结合任务感知提示、混合检索、分层安全与合规监控，针对四类金融查询进行优化，并比较三种数据整合技术的效率与可行性。


<details>
  <summary>Details</summary>
Motivation: 在知识密集型任务中，RAG 能提升领域特异性与时效性并降低幻觉，但金融领域面临专有数据受限、检索准确性不足、监管约束与敏感数据保护等挑战，因此需要一个兼具安全性与合规性的RAG 框架。

Method: 使用混合检索策略（开放数据与专有数据），建立动态提示框架以根据查询复杂性自适应；提出四层任务分类（显式事实、隐式事实、可解释的推理、隐性因果推理），在检索与生成阶段识别挑战、数据集与优化技术；引入多层安全机制（差分隐私、数据去识别化、基于角色的访问控制）、自动合规校验系统；评估三种数据整合技术（上下文嵌入、小模型增量、定向微调）的效率与可行性。

Result: 提出了一个综合评估框架，系统比较三种数据整合技术在不同金融场景下的效率与可行性，并提供对检索-生成流程的改进路径。

Conclusion: AstuteRAG-FQA 为金融问答提供了一个具备强隐私保护、合规监控与可自适应提示的RAG 框架，特殊的四类任务分类能覆盖广泛的查询类型，未来工作可进一步扩展到更大规模的专有数据源与更严格的实时合规验证。

Abstract: Retrieval-Augmented Generation (RAG) shows significant promise in
knowledge-intensive tasks by improving domain specificity, enhancing temporal
relevance, and reducing hallucinations. However, applying RAG to finance
encounters critical challenges: restricted access to proprietary datasets,
limited retrieval accuracy, regulatory constraints, and sensitive data
interpretation. We introduce AstuteRAG-FQA, an adaptive RAG framework tailored
for Financial Question Answering (FQA), leveraging task-aware prompt
engineering to address these challenges. The framework uses a hybrid retrieval
strategy integrating both open-source and proprietary financial data while
maintaining strict security protocols and regulatory compliance. A dynamic
prompt framework adapts in real time to query complexity, improving precision
and contextual relevance. To systematically address diverse financial queries,
we propose a four-tier task classification: explicit factual, implicit factual,
interpretable rationale, and hidden rationale involving implicit causal
reasoning. For each category, we identify key challenges, datasets, and
optimization techniques within the retrieval and generation process. The
framework incorporates multi-layered security mechanisms including differential
privacy, data anonymization, and role-based access controls to protect
sensitive financial information. Additionally, AstuteRAG-FQA implements
real-time compliance monitoring through automated regulatory validation systems
that verify responses against industry standards and legal obligations. We
evaluate three data integration techniques - contextual embedding, small model
augmentation, and targeted fine-tuning - analyzing their efficiency and
feasibility across varied financial environments.

</details>


### [66] [ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling](https://arxiv.org/abs/2510.27610)
*Zhuohan Wang,Ziwei Zhu,Ziniu Li,Congliang Chen,Yizhou Han,Yufeng Lin,Zhihang Lin,Angyang Gu,Xinglin Hu,Ruoyu Sun,Tian Ding*

Main category: cs.LG

TL;DR: ORGEval 提供一个基于图的评估框架，用于评估大语言模型在将工业优化问题形式化为线性/混合整数线性规划中的能力，通过对模型进行图同构检测来判断等价性，结合对称可分解(SD)图的 Weisfeiler-Lehman (WL) 测试以保证正确的等价性检测，并构建 Bench4Opt 数据集进行基准测试，结果表明该方法在鲁棒性和速度方面优于基于求解器的方法，且某些模型在直接提示下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 工业应用中的优化建模需要大量人工工作和领域知识，评估 LLMs 的性能缺乏稳健度量。现有求解器方法存在不一致性、不可行性和高成本等问题，因此需要一种结构化、对数值扰动鲁棒的评估框架。

Method: 将优化模型表示为图，将等价性检测化为图同构问题；在 SD 图条件下给出 WL 测试的充分条件，确保其正确识别同构；结合 SD 检测算法，构建一个定制化的 WL 变体用于评估模型格式化的等价性；忽略数值层面的具体实例配置以提升鲁棒性；并构建 Bench4Opt 数据集，基于 ORGEval 对主流/前沿 LLM 进行基准。

Result: 方法能在随机参数配置下达到 100% 一致性结果，并在运行时间上显著优于求解器方法，尤其在困难问题上表现突出；DeepSeek-V3 和 Claude-Opus-4 在直接提示下达到最高准确率，甚至超过了领先的推理模型。

Conclusion: 提供一个鲁棒、可扩展的评估框架，用于对比分析 LLMs 在优化建模中的能力，并通过 Bench4Opt 数据集推动领域基准；尽管大多数模型仍然面临建模挑战，但特定模型在直接提示下表现优于一些复杂推理模型。

Abstract: Formulating optimization problems for industrial applications demands
significant manual effort and domain expertise. While Large Language Models
(LLMs) show promise in automating this process, evaluating their performance
remains difficult due to the absence of robust metrics. Existing solver-based
approaches often face inconsistency, infeasibility issues, and high
computational costs. To address these issues, we propose ORGEval, a
graph-theoretic evaluation framework for assessing LLMs' capabilities in
formulating linear and mixed-integer linear programs. ORGEval represents
optimization models as graphs, reducing equivalence detection to graph
isomorphism testing. We identify and prove a sufficient condition, when the
tested graphs are symmetric decomposable (SD), under which the
Weisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.
Building on this, ORGEval integrates a tailored variant of the WL-test with an
SD detection algorithm to evaluate model equivalence. By focusing on structural
equivalence rather than instance-level configurations, ORGEval is robust to
numerical variations. Experimental results show that our method can
successfully detect model equivalence and produce 100\% consistent results
across random parameter configurations, while significantly outperforming
solver-based methods in runtime, especially on difficult problems. Leveraging
ORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs
on optimization modeling. Our results reveal that although optimization
modeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4
achieve the highest accuracies under direct prompting, outperforming even
leading reasoning models.

</details>


### [67] [Panprediction: Optimal Predictions for Any Downstream Task and Loss](https://arxiv.org/abs/2510.27638)
*Sivaraman Balakrishnan,Nika Haghtalab,Daniel Hsu,Brian Lee,Eric Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Supervised learning is classically formulated as training a model to minimize
a fixed loss function over a fixed distribution, or task. However, an emerging
paradigm instead views model training as extracting enough information from
data so that the model can be used to minimize many losses on many downstream
tasks. We formalize a mathematical framework for this paradigm, which we call
panprediction, and study its statistical complexity. Formally, panprediction
generalizes omniprediction and sits upstream from multi-group learning, which
respectively focus on predictions that generalize to many downstream losses or
many downstream tasks, but not both. Concretely, we design algorithms that
learn deterministic and randomized panpredictors with
$\tilde{O}(1/\varepsilon^3)$ and $\tilde{O}(1/\varepsilon^2)$ samples,
respectively. Our results demonstrate that under mild assumptions,
simultaneously minimizing infinitely many losses on infinitely many tasks can
be as statistically easy as minimizing one loss on one task. Along the way, we
improve the best known sample complexity guarantee of deterministic
omniprediction by a factor of $1/\varepsilon$, and match all other known sample
complexity guarantees of omniprediction and multi-group learning. Our key
technical ingredient is a nearly lossless reduction from panprediction to a
statistically efficient notion of calibration, called step calibration.

</details>


### [68] [Imbalanced Classification through the Lens of Spurious Correlations](https://arxiv.org/abs/2510.27650)
*Jakob Hackstein,Sidney Bender*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Class imbalance poses a fundamental challenge in machine learning, frequently
leading to unreliable classification performance. While prior methods focus on
data- or loss-reweighting schemes, we view imbalance as a data condition that
amplifies Clever Hans (CH) effects by underspecification of minority classes.
In a counterfactual explanations-based approach, we propose to leverage
Explainable AI to jointly identify and eliminate CH effects emerging under
imbalance. Our method achieves competitive classification performance on three
datasets and demonstrates how CH effects emerge under imbalance, a perspective
largely overlooked by existing approaches.

</details>


### [69] [Information-Theoretic Greedy Layer-wise Training for Traffic Sign Recognition](https://arxiv.org/abs/2510.27651)
*Shuyan Lyu,Zhanzimo Wu,Junliang Du*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern deep neural networks (DNNs) are typically trained with a global
cross-entropy loss in a supervised end-to-end manner: neurons need to store
their outgoing weights; training alternates between a forward pass
(computation) and a top-down backward pass (learning) which is biologically
implausible. Alternatively, greedy layer-wise training eliminates the need for
cross-entropy loss and backpropagation. By avoiding the computation of
intermediate gradients and the storage of intermediate outputs, it reduces
memory usage and helps mitigate issues such as vanishing or exploding
gradients. However, most existing layer-wise training approaches have been
evaluated only on relatively small datasets with simple deep architectures. In
this paper, we first systematically analyze the training dynamics of popular
convolutional neural networks (CNNs) trained by stochastic gradient descent
(SGD) through an information-theoretic lens. Our findings reveal that networks
converge layer-by-layer from bottom to top and that the flow of information
adheres to a Markov information bottleneck principle. Building on these
observations, we propose a novel layer-wise training approach based on the
recently developed deterministic information bottleneck (DIB) and the
matrix-based R\'enyi's $\alpha$-order entropy functional. Specifically, each
layer is trained jointly with an auxiliary classifier that connects directly to
the output layer, enabling the learning of minimal sufficient task-relevant
representations. We empirically validate the effectiveness of our training
procedure on CIFAR-10 and CIFAR-100 using modern deep CNNs and further
demonstrate its applicability to a practical task involving traffic sign
recognition. Our approach not only outperforms existing layer-wise training
baselines but also achieves performance comparable to SGD.

</details>


### [70] [Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems](https://arxiv.org/abs/2510.27659)
*Alireza Saleh Abadi,Leen-Kiat Soh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the rapidly evolving field of multi-agent reinforcement learning (MARL),
understanding the dynamics of open systems is crucial. Openness in MARL refers
to the dynam-ic nature of agent populations, tasks, and agent types with-in a
system. Specifically, there are three types of openness as reported in (Eck et
al. 2023) [2]: agent openness, where agents can enter or leave the system at
any time; task openness, where new tasks emerge, and existing ones evolve or
disappear; and type openness, where the capabil-ities and behaviors of agents
change over time. This report provides a conceptual and empirical review,
focusing on the interplay between openness and the credit assignment problem
(CAP). CAP involves determining the contribution of individual agents to the
overall system performance, a task that becomes increasingly complex in open
environ-ments. Traditional credit assignment (CA) methods often assume static
agent populations, fixed and pre-defined tasks, and stationary types, making
them inadequate for open systems. We first conduct a conceptual analysis,
in-troducing new sub-categories of openness to detail how events like agent
turnover or task cancellation break the assumptions of environmental
stationarity and fixed team composition that underpin existing CAP methods. We
then present an empirical study using representative temporal and structural
algorithms in an open environment. The results demonstrate that openness
directly causes credit misattribution, evidenced by unstable loss functions and
significant performance degradation.

</details>
