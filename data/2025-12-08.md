<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 52]
- [stat.ML](#stat.ML) [Total: 6]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators](https://arxiv.org/abs/2512.05297)
*Xianglong Hou,Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: Continuous Flow Operator (CFO) learns the PDE right-hand side via flow matching on temporal splines, enabling time-continuous, time-resilient surrogates with data-efficient training.


<details>
  <summary>Details</summary>
Motivation: Autoregressive neural operators accumulate error and rely on uniform time steps; need a time-continuous surrogate that can handle irregular sampling and arbitrary temporal queries without backprop through solvers.

Method: Fit temporal splines to trajectory data and estimate time derivatives at knots with finite differences to construct velocity paths; train a neural operator via flow matching to predict these velocity fields; inference uses numerical ODE integration, bypassing backprop through solvers.

Result: CFO achieves superior long-horizon stability and data efficiency across Lorenz, 1D Burgers, 2D diffusion-reaction, and 2D shallow water benchmarks; with only 25% irregularly subsampled time points, it outperforms autoregressive models trained on full data (up to 87% relative error reduction); competitive efficiency despite requiring integration, and unique capabilities for reverse-time inference and arbitrary temporal querying.

Conclusion: CFO provides a time-resolution invariant PDE surrogate that avoids backprop through ODE solvers, improves data efficiency and long-horizon stability, and enables flexible temporal querying; potential future work includes exploring scalability, different spline choices, and extending to more complex PDEs.

Abstract: Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.

</details>


### [2] [Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models](https://arxiv.org/abs/2512.05216)
*Rajna Fani,Rafi Al Attrach,David Restrepo,Yugang Jia,Leo Anthony Celi,Peter Schüffler*

Main category: cs.LG

TL;DR: 提出CV-Masking，一种基于特征波动性的自适应掩码策略用于MAE在EHR上的预训练，结合值掩码目标，提高重建与下游任务性能并加速收敛。


<details>
  <summary>Details</summary>
Motivation: 实验室检测具有显著的波动性差异，均匀随机掩码忽略了不同特征的可预测性差异，导致对高波动特征的建模不足和潜在临床信息的丢失。

Method: 对每个特征基于内在变异性（如变异系数）自适应设定掩码概率，辅以值域相关的掩码目标（value-only masking），在MAE框架下进行预训练，数据集为大规模实验室检测面板。

Result: 相较随机和基于方差的掩码策略，CV-Masking在重建、下游预测及收敛速度上实现系统性提升，产生更鲁棒且具临床意义的EHR表征。

Conclusion: CV-Masking为EHR的MAE预训练提供了一种考虑特征波动性的自适应掩码方案，具有潜在的临床应用前景与推广价值。

Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.

</details>


### [3] [Rethinking Tokenization for Clinical Time Series: When Less is More](https://arxiv.org/abs/2512.05217)
*Rafi Al Attrach,Rajna Fani,David Restrepo,Yugang Jia,Peter Schüffler*

Main category: cs.LG

TL;DR: 对临床时间序列模型的标记化策略进行系统评估，结果取决于具体任务；冻结的预训练编码器在性能和参数效率上优于可训练的版本，整体上简单高效的标记化策略往往能达到较好表现，但最优策略依任务而异。


<details>
  <summary>Details</summary>
Motivation: 在临床时间序列建模中，标记化策略与模型性能之间的关系尚不清晰，亟需通过公平对比来揭示时间编码、数值特征以及编码器大小等因素对不同预测任务的影响。

Method: 以MIMIC-IV数据集，覆盖四个临床预测任务，对多种标记化策略进行受控消融实验；比较显式时间编码、仅编码值特征、仅代码序列信号、可冻结的预训练编码器与可训练版本，以及不同编码器规模等变量，评估其对下游任务的影响。

Result: 显式时间编码对下游任务没有一致的显著收益；值特征的重要性因任务而异，对死亡率预测有影响但对再入院预测影响不明显；代码序列本身就能提供可预测信号；冻结的预训练代码编码器在参数数量显著减少的情况下性能优于可训练版本；更大的编码器在冻结嵌入时带来一致性提升，减轻计算负担。

Conclusion: 通过受控评估实现对标记化策略的公平比较；在多数情况下，简单、参数高效的方法能达到强表现，但最优标记化策略仍取决于具体任务；未来工作可聚焦于对任务定制化的标记化选择与大规模冻结嵌入的进一步优化。

Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.

</details>


### [4] [Variance Matters: Improving Domain Adaptation via Stratified Sampling](https://arxiv.org/abs/2512.05226)
*Andrea Napoli,Paul White*

Main category: cs.LG

TL;DR: 提出 VaRDASS，通过分层抽样实现对 UDA 中的方差降维，在相关对齐和 MMD 两种距离上给出分层目标、误差界与最优性证明，并给出一个类 k-means 的优化算法，实验显示在三个数据集上提升了误差估计准确性和目标域性能。


<details>
  <summary>Details</summary>
Motivation: 在无监督领域自适应中，域差异估计在随机场景下方差高，削弱了理论收益，需要对估计过程进行方差约简以提升稳定性和性能。

Method: 提出同类首个针对 UDA 的随机方差降低方法 VaRDASS，通过分层采样对相关对齐和 MMD 等距离进行分层目标设计；推导期望和最坏情形误差界，证明在某些假设下 MMD 的目标函数在方差最小化上是理论最优；给出一个实用的类似 k-means 的优化算法并给出分析。

Result: 理论方面给出误差界与最优性证明；实验在三个域移位数据集上显示减少了估计方差并提升了目标域性能。

Conclusion: VaRDASS 作为首个专门的 UDA 方差降维技术，能显著提升域自适应中的距离估计稳定性和目标域效果，并提供可实现的优化方法。

Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.

</details>


### [5] [Bridging quantum and classical computing for partial differential equations through multifidelity machine learning](https://arxiv.org/abs/2512.05241)
*Bruno Jacob,Amanda A. Howard,Panos Stinis*

Main category: cs.LG

TL;DR: A multifidelity learning framework that corrects coarse quantum PDE solutions using sparse classical high-fidelity data, enabling accurate predictions and temporal extrapolation on near-term quantum devices.


<details>
  <summary>Details</summary>
Motivation: Near-term quantum hardware imposes severe limits (few qubits, shallow circuits), making high-accuracy PDE solvers impractical. A method that leverages abundant coarse quantum outputs alongside sparse high-fidelity data can unlock practical utility.

Method: Train a low-fidelity surrogate from abundant quantum solver outputs. Learn correction mappings with a multifidelity neural network that combines linear and nonlinear transformations. Use sparse classical high-fidelity data to calibrate corrections. Demonstrated on nonlinear PDEs (viscous Burgers, incompressible Navier–Stokes via quantum lattice Boltzmann methods) to correct coarse quantum predictions and enable temporal extrapolation beyond the training window.

Result: The framework successfully corrects coarse quantum predictions, achieves temporal extrapolation beyond the classical training window, and delivers accuracy competitive with classical methods while reducing reliance on expensive high-fidelity simulations.

Conclusion: This approach provides a practical pathway to extract value from current quantum devices for computational physics, guiding algorithm development and deployment by linking hardware-limited quantum simulations with real-world scientific tasks.

Abstract: Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.

</details>


### [6] [When unlearning is free: leveraging low influence points to reduce computational costs](https://arxiv.org/abs/2512.05254)
*Anat Kleiman,Robert Fisher,Ben Deaner,Udi Wieder*

Main category: cs.LG

TL;DR: 通过对影响函数的比较分析，识别对模型输出影响极小的数据子集，在进行“忘记/清除”前先剔除这部分数据，从而显著降低 unlearning 的计算成本（约50%）。


<details>
  <summary>Details</summary>
Motivation: 随着数据隐私在机器学习中的关注度增加，模型需要“遗忘”某些数据点。传统的忘记方法对忘记集中的所有点一视同仁，成本高且低效。若能区分对模型学习影响微弱的样本，便可更高效地执行数据忘记。

Method: 在语言和视觉任务中比较影响函数，识别对模型输出影响微小的训练数据子集；提出一个高效的忘记框架，在未实际执行忘记之前先对数据集进行裁剪，从而缩小待忘记数据集规模以降低计算开销。

Result: 在真实世界的案例中，该方法实现了约50%的计算成本下降，同时保持忘记效果的有效性。

Conclusion: 选择性地进行数据忘记（只删除对模型影响显著的数据点）是可行且高效的，可以在保证隐私合规的前提下提升 unlearning 的实际应用效率。

Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.

</details>


### [7] [Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition](https://arxiv.org/abs/2512.05323)
*Adam Lizerbram,Shane Stevenson,Iman Khadir,Matthew Tu,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: FCNv2 对初始输入噪声表现出一定鲁棒性，但在高噪声下强度估计持续偏低；随机初始化后输出迅速趋于平滑，表明模型倾向于稳定输出；所用方法可广泛应用于其他数据驱动天气模型。


<details>
  <summary>Details</summary>
Motivation: 评估基于AI的天气预报模型在输入不确定性下的输出可靠性，尤其是对飓风等极端天气事件的预测。

Method: 进行两组实验：1）用高斯噪声扰动 ERA5 初始场（以飓风 Florence，2018-09-13至16日为对象），观察轨迹和强度预测；2）用完全随机初始条件启动 FCNv2，观察对荒诞输入的响应。

Result: 在低到中等噪声下，模型能较好保留飓风特征；高噪声下仍保留大致轨迹和结构，但位置精度下降；无论何级噪声，模型均低估强度和持续性；完全随机初始条件下，预测在数步后变得平滑连贯，显示输出趋于稳定。

Conclusion: 方法简单、便于移植到其他数据驱动天气预测模型；结果提示对强度的偏低估计及对稳健性评估的价值，建议结合多源信息改进强度预测。

Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.

</details>


### [8] [Text Rationalization for Robust Causal Effect Estimation](https://arxiv.org/abs/2512.05373)
*Lijinghua Zhang,Hengrui Cai*

Main category: cs.LG

TL;DR: 提出 Confounding-Aware Token Rationalization (CATR) 框架，通过残差独立性诊断筛选稀疏的 token 子集，以在保持混杂信息的前提下减小文本维度，从而缓解观测层面的正性假设违背、提高因果估计的稳定性与可解释性；在合成数据及 MIMIC-III 实验中，CATR 表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 高维文本能编码丰富的上下文信息，但会在因果推断中带来正性假设的违背、权重不稳定及估计方差膨胀等问题，亟需在不丢失混杂信息的前提下降低维度与噪声。

Method: 提出 Confounding-Aware Token Rationalization (CATR)，通过一个残差独立性诊断识别一个稀疏的必要 token 子集，以保留足以实现无混淆（unconfoundedness）的混杂信息；丢弃无关文本，缓解观测层面的正性违背并稳定后续因果估计。

Result: 在合成数据及真实世界的 MIMIC-III 数据集上，CATR 相较于现有基线方法，获得更准确、稳定且容易解释的因果效应估计。

Conclusion: CATR 通过聚焦必要文本信号来减小文本高维带来的正性假设问题与估计方差，提升因果推断的鲁棒性与可解释性。

Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.

</details>


### [9] [Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces](https://arxiv.org/abs/2512.05291)
*Na Li,Hangguan Shan,Wei Ni,Wenjie Zhang,Xinyu Li*

Main category: cs.LG

TL;DR: 提出一个基于 RKHS 和 SHAP 的可解释强化学习算法 RSA2C，将状态属性贡献引入 Actor- critic 的学习过程，通过Mahalanobis加权的核方法实现对状态维度的异质性权重分配，从而提升效率、稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的可解释强化学习方法往往对状态特征赋予同等权重，忽略不同维度对奖励的异质影响，导致训练效率和解释性受限。

Method: RSA2C 将 Actor 放在向量值 RKHS 中，使用带 Mahalanobis 权重的算子值核；Value Critic 与 Advantage Critic 置于标量 RKHS；采用稀疏字典，Value Critic 拥有独立字典，Actor 与 Advantage Critic 共享字典；通过 RKHS–SHAP 计算状态属性贡献（在流形内外的核平均嵌入），并将其转化为 Mahalanobis 门控权重，调制 Actor 梯度和 Advantage Critic 目标；给出在状态扰动下的全局非渐近收敛界，含扰动误差项（稳定性）和收敛误差项（效率）。

Result: 在三个标准连续控制环境上，RA S2C 展现出更好的效率、稳定性和可解释性。

Conclusion: RSA2C 将状态属性引入 Actor–Critic 学习，结合 RKHS 表示和 SHAP 解释，提供理论稳定性和实证优越性的综合方法。

Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.

</details>


### [10] [Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability](https://arxiv.org/abs/2512.05638)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 提出了 Modular Jets 概念，用来评估回归/分类流水线中模块分解的可辨识性。通过估计局部线性响应映射（jets）来描述模块对输入的结构化扰动的反应，区分“mirage”与“identifiable”两种情形；在两模块线性回归中证明 jets 的可识别性定理，并给出 MoJet 算法用于经验 jets 的估计与 Mirage 诊断。


<details>
  <summary>Details</summary>
Motivation: 传统评估仅关注预测风险，未回答数据与评价设计是否能唯一确定模型的内部分解。引入 empirical jets 与 mirage 概念，旨在判定在给定数据和评估设计下，模块化分解是否具备唯一性。

Method: 引入局部线性响应映射的经验 jets，描述模块对输入的结构化扰动的局部敏感性。提出 Mirages 的概念，即多种不同的模块分解在 jets 上不可区分；在两模块线性回归管线下给出 jet-identifiability 定理，指出在适度的秩条件和获得模块级 jets 的前提下，内部因子化唯一确定。基于此提出 MoJet 算法，用于经验 jets 的估计与 Mirage 诊断，并通过线性和深度回归以及管线分类的实验进行演示。

Result: 理论方面：在两模块线性回归管线中证明 jet-identifiability（喷射可辨识性）定理，即在满足轻度秩条件且可获取模块级 jets 的情况下，内部因子分解唯一确定；而单靠风险评估可能存在大量 Mirage 分解，实施同样的输入-输出映射。方法方面：提出 MoJet 算法用于经验 jet 的估计与 Mirage 诊断。实验方面：展现在线性回归、深度回归以及管线分类任务上的应用示例。

Conclusion: 该工作表明，在特定条件下，内部模块化分解可以被唯一确定，从而提升对模型内部结构的可解释性；但仅以预测风险进行评估可能无法揭示真正的分解结构，容易产生 Mirage。MoJet 为实证估计和诊断提供了工具，适用于理解和设计更具辨识性的模块化管线。

Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.

</details>


### [11] [NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process](https://arxiv.org/abs/2512.05893)
*Neha Gupta,Aditya Maheshwari*

Main category: cs.LG

TL;DR: 提出一个基于RNN/LSTM的FPP参数估计框架，利用互事件间隔序列估计μ和β，对比MOM显著降低误差，在真实高频数据中也有良好表现。


<details>
  <summary>Details</summary>
Motivation: Fractional Poisson 过程具有记忆和长程依赖，但参数估计通常依赖传统方法如矩估计，存在局限性。采用LSTM能捕捉时间相关性并自适应估计μ与β。

Method: 使用LSTM从互事件间隔序列中回归估计参数μ>0和β∈(0,1)；在合成数据上与矩估计(MOM)方法比较，显示MSE下降约55.3%；在真实高频数据集（ Montgomery County, PA 的紧急呼叫记录 与 AAPL 股票交易数据）上评估其跟踪每日模式和参数变化的能力。

Result: 在仿真数据上实现了对μ和β的准确估计，MSE较MOM降低约55.3%；在两组真实高频数据上，LSTM能够有效捕捉日内模式和参数变动，表现稳定。

Conclusion: 基于LSTM的FPP参数估计方法对具有记忆性的到达过程参数估计有效性高，在真实数据中也具备实用性，能够处理复杂时间依赖。

Abstract: In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $μ>0$ and $β\in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.

</details>


### [12] [On the Bayes Inconsistency of Disagreement Discrepancy Surrogates](https://arxiv.org/abs/2512.05931)
*Neil G. Marchant,Andrew C. Cullen,Feng Liu,Sarah M. Erfani*

Main category: cs.LG

TL;DR: Disagreement discrepancy optimization uses non-differentiable 0-1 loss; existing surrogates lack Bayes consistency. The paper derives bounds on the optimality gap, introduces a Bayes-consistent disagreement loss paired with cross-entropy, and shows improved empirical accuracy and robustness under distribution shift and adversarial conditions.


<details>
  <summary>Details</summary>
Motivation: Distribution shift undermines real-world NN reliability. Disagreement discrepancy is a promising metric for bounding error under shifts, detecting harmful shifts, and training robust models; ensuring the surrogate optimization aligns with the true objective is crucial.

Method: 1) Prove that existing surrogates for disagreement discrepancy are not Bayes consistent. 2) Derive upper and lower bounds on the optimality gap of such surrogates. 3) Propose a novel disagreement loss that, with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. 4) Empirically compare against existing approaches across benchmarks, including adversarial settings.

Result: We identify non Bayes-consistency in current surrogates and provide theoretical bounds on the optimality gap. The proposed disagreement loss, when paired with cross-entropy, offers a Bayes-consistent surrogate for disagreement discrepancy. Empirical results show more accurate and robust estimates of disagreement discrepancy than prior methods, particularly under challenging adversarial conditions.

Conclusion: The work exposes limitations of existing surrogates for disagreement discrepancy, provides a rigorous path to a consistent surrogate, and demonstrates practical gains in accuracy and robustness under distribution shift and adversarial scenarios.

Abstract: Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models changes under a shifting distribution. The process of maximizing this measure has seen applications in bounding error under shifts, testing for harmful shifts, and training more robust models. However, this optimization involves the non-differentiable zero-one loss, necessitating the use of practical surrogate losses. We prove that existing surrogates for disagreement discrepancy are not Bayes consistent, revealing a fundamental flaw: maximizing these surrogates can fail to maximize the true disagreement discrepancy. To address this, we introduce new theoretical results providing both upper and lower bounds on the optimality gap for such surrogates. Guided by this theory, we propose a novel disagreement loss that, when paired with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. Empirical evaluations across diverse benchmarks demonstrate that our method provides more accurate and robust estimates of disagreement discrepancy than existing approaches, particularly under challenging adversarial conditions.

</details>


### [13] [Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay](https://arxiv.org/abs/2512.05320)
*Mehmet Efe Lorasdagi,Dogan Can Cicek,Furkan Burak Mutlu,Suleyman Serdar Kozat*

Main category: cs.LG

TL;DR: DPER 将 Actor 与 Critic 的经验回放批次解耦，实现独立采样以提供更合适的学习信号；在持续控制任务中与 TD3/DDPG 等算法结合，优于常规经验回放与优先经验回放。


<details>
  <summary>Details</summary>
Motivation: 在 Actor-Critic 架构中，Actor 与 Critic 的学习目标和更新动力学不同，使用相同的回放批次可能不是最优。通过分离批次，可以为两者提供更合适的学习信号。

Method: 提出 Decoupled Prioritized Experience Replay (DPER)，允许对 Actor 与 Critic 独立地采样经验回放批次，并可与离策略深度强化学习算法结合。将 DPER 与最先进的 Twin Delayed DDPG（TD3/TD-DPG）算法集成，在标准持续控制基准上进行评估。

Result: DPER 在 OpenAI Gym MuJoCo 的多项任务上，优于传统的经验回放（vanilla）和标准的优先经验回放（PER）。

Conclusion: 将经验回放解耦以更好地服务 Actor 与 Critic 的学习信号，可提升训练动力学与最终策略质量，DPER 为广义的 Actor-Critic 离策略 RL 算法提供可推广的改进机制。

Abstract: Background: Deep Deterministic Policy Gradient-based reinforcement learning algorithms utilize Actor-Critic architectures, where both networks are typically trained using identical batches of replayed transitions. However, the learning objectives and update dynamics of the Actor and Critic differ, raising concerns about whether uniform transition usage is optimal.
  Objectives: We aim to improve the performance of deep deterministic policy gradient algorithms by decoupling the transition batches used to train the Actor and the Critic. Our goal is to design an experience replay mechanism that provides appropriate learning signals to each component by using separate, tailored batches.
  Methods: We introduce Decoupled Prioritized Experience Replay (DPER), a novel approach that allows independent sampling of transition batches for the Actor and the Critic. DPER can be integrated into any off-policy deep reinforcement learning algorithm that operates in continuous control domains. We combine DPER with the state-of-the-art Twin Delayed DDPG algorithm and evaluate its performance across standard continuous control benchmarks.
  Results: DPER outperforms conventional experience replay strategies such as vanilla experience replay and prioritized experience replay in multiple MuJoCo tasks from the OpenAI Gym suite.
  Conclusions: Our findings show that decoupling experience replay for Actor and Critic networks can enhance training dynamics and final policy quality. DPER offers a generalizable mechanism that enhances performance for a wide class of actor-critic off-policy reinforcement learning algorithms.

</details>


### [14] [Non-Convex Federated Optimization under Cost-Aware Client Selection](https://arxiv.org/abs/2512.05327)
*Xiaowen Jiang,Anton Rodomanov,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 在联邦优化中引入成本化模型并给出基于RG-SAGA的最优复杂度算法，兼具较低通信与本地计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有优化比较指标往往忽略不同客户端选取策略带来的通信成本差异，缺乏一个量化通信与本地计算的统一框架。该工作提出一个简单自然的联邦优化模型，将通信成本和本地计算成本显式化并与不同客户端选取策略绑定，便于横向比较不同方法的实际成本。

Method: 提出基于不完全复合梯度法的算法框架，在每次迭代中通过精心构造的梯度估计器执行子问题近似求解。梯度估计器基于SAGA，并推导出新的方差界，表明SAGA能够利用函数间相似性；提出“递归梯度”技术，作为提升条件无偏梯度估计器误差界的通用方法（对SAGA和SVRG均有适用性）。将递归梯度应用于SAGA得到RG-SAGA，其误差界相比原SAGA得到改进。

Result: 在该模型框架下，所提算法实现了非凸联邦优化中的最佳已知通信与局部计算复杂度组合。RG-SAGA相较原始SAGA具有更好的估计误差界，提升了梯度估计的稳定性与效率。

Conclusion: 该成本敏感的联邦优化模型为比较不同客户端选取策略提供了量化工具，并且基于RG-SAGA的算法在非凸情形下达到更优的通信与本地计算平衡，具备潜在扩展性到其他两阶段/变体梯度估计器，推动联邦优化在实际场景中的应用前景。

Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.

</details>


### [15] [PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering](https://arxiv.org/abs/2512.05336)
*Durga Prasad Maram,Kalpa Gunaratna,Vijay Srinivasan,Haris Jeelani,Srinivas Chappidi*

Main category: cs.LG

TL;DR: PATHFINDER通过蒙特卡洛树搜索生成训练路径、用子答案回忆和LLM评审筛除错误/冗长轨迹、并重构子查询来应对检索失败，从而提升多跳问答的训练数据质量与模型性能。


<details>
  <summary>Details</summary>
Motivation: 训练型方法在多跳问答中易受LLM幻觉与错误推理路径影响，导致性能下降，需要改进训练数据质量并鲁棒地处理检索失败情况。

Method: 1) 采用蒙特卡洛树搜索(MCTS)生成训练路径轨迹；2) 通过子答案回忆与LLM作为判定者的验证来筛除错误与冗长的轨迹，提升训练数据质量；3) 重新表述子查询以应对检索失败的情况。

Result: PATHFINDER在公开基准数据集上的多跳问答任务上提升了模型性能。

Conclusion: 通过改进训练路径的生成与筛选，并对检索失败情况进行子查询重表述，PATHFINDER能有效提升多跳问答的训练效果与最终表现。

Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.

</details>


### [16] [Interaction Tensor Shap](https://arxiv.org/abs/2512.05338)
*Hiroki Hasegawa,Yukihiko Okada*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.

</details>


### [17] [Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models](https://arxiv.org/abs/2512.05339)
*Mahesh Kumar Nandwana,Youngwan Lim,Joseph Liu,Alex Yang,Varun Notibala,Nishchaie Khanna*

Main category: cs.LG

TL;DR: 提出 Roblox Guard 1.0，一款基于 Llama-3.1-8B-Instruct 的指令微调 LLM，聚焦输入输出层面的综合审核与管控，通过多模型管控流水线提升安全性，并发布 RobloxGuard-Eval 基准以评估防护框架的效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型在输入输出安全控制方面的不足，提升跨输入输出场景的鲁棒安全防护与评估能力。

Method: 采用多模型管控流水线、指令微调、结合合成数据与开源安全数据集、引入链式推理(CoT)与输入反演以增强上下文理解和决策能力，基于 Llama-3.1-8B-Instruct 进行微调.

Result: 在未遇见的安全分类（out-of-domain）上具备较强泛化能力，对域外安全基准表现良好；并发布 RobloxGuard-Eval 基准，提供可扩展的安全分类体系以评估 LLM 的防护与管控效果。

Conclusion: 提出一种新型的输入输出全方位管控的安全框架，结合可扩展评估基准，提升 LLM 系统的安全性和可评估性。

Abstract: Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.

</details>


### [18] [When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation](https://arxiv.org/abs/2512.05341)
*Yiwen Liang,Qiufeng Li,Shikai Wang,Weidong Cao*

Main category: cs.LG

TL;DR: 提出一种面向硬件代码生成的LLM“遗忘”框架，结合语法保护的遗忘策略和细粒度的 Floor-aware 选择性损失，在保持 RTL 代码合规性与功能性的前提下，显著提升可遗忘性和训练效率。


<details>
  <summary>Details</summary>
Motivation: 缓解大语言模型在硬件设计场景中对专有 IP、带污染基准及不安全编码模式的 memorization 风险，并需要在不显著损害生成能力的前提下实现有针对性的知识删除。

Method: 提出一个两段式框架： (i) 语法保持的遗忘策略，确保在“忘记”过程中不破坏硬件代码的结构语义；(ii) 细粒度、考虑底层布局（floor）的选择性损失，能够精准、高效地移除有问题的知识。该框架通过二者结合实现对目标知识的高效遗忘，同时保持代码生成能力。

Result: 在广泛实验中，框架支持的遗忘集合（forget sets）大小可达原有方法的3倍以上，通常仅需单轮训练就可完成，且在语法正确性与 RTL 级功能完整性方面表现良好。

Conclusion: 为LLM辅助硬件设计提供更可靠的“忘记”能力，降低潜在风险，同时尽量保持生成能力，推动可信赖的硬件设计流程。

Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.

</details>


### [19] [China Regional 3km Downscaling Based on Residual Corrective Diffusion Model](https://arxiv.org/abs/2512.05377)
*Honglu Sun,Hao Jing,Zhixiang Dai,Sa Xiao,Wei Xue,Jian Sun,Qifeng Lu*

Main category: cs.LG

TL;DR: Diffusion-based downscaling (CorrDiff) extend to a 20x larger region and multi-level variables to produce 3 km forecasts from 25 km CMA forecasts, outperform CMA-MESO on MAE and capture fine-scale radar details.


<details>
  <summary>Details</summary>
Motivation: Efficiently generate high-resolution weather forecasts; improve downscaling performance; handle larger regional scope and high-level variables; leverage generative diffusion to capture fine-scale structure and uncertainty.

Method: Extend CorrDiff diffusion-based downscaling framework. Increase region size (~20x), include high-level variables (six pressure levels) as targets, add a global residual connection, apply to CMA-GFS 25 km forecasts and SFF, generate 3 km forecasts for China, baseline CMA-MESO, evaluate MAE and radar reflectivity realism.

Result: CorrDiff downscaled forecasts generally achieve lower MAE than CMA-MESO for target variables; generates realistic fine-scale details in radar reflectivity compared to deterministic regression models; demonstrates effectiveness of diffusion-based downscaling in this setting.

Conclusion: Diffusion-based downscaling with CorrDiff is effective for high-resolution numerical weather prediction, offering accuracy gains over traditional baselines and producing realistic fine-scale structures; promising for broader regional and multivariate downscaling.

Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.

</details>


### [20] [Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets](https://arxiv.org/abs/2512.05386)
*Jakub Kopko,David Graber,Saltuk Mustafa Eyrilmez,Stanislav Mazurenko,David Bednar,Jiri Sedlar,Josef Sivic*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.

</details>


### [21] [Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction](https://arxiv.org/abs/2512.05402)
*Sithumi Wickramasinghe,Bikramjit Das,Dorien Herremans*

Main category: cs.LG

TL;DR: Proposes MineROI-Net, a Transformer-based model for time-series classification of ASIC hardware acquisition profitability (ROI >= 1, 0 < ROI < 1, ROI <= 0) within one year. Achieves 83.7% accuracy and 83.1% macro F1 on 20 miners (2015–2024); high precision for profitable/unprofitable periods; open-source tool to guide investment timing in mining hardware.


<details>
  <summary>Details</summary>
Motivation: Mining is now capital-intensive with volatile markets and rapid technology turnover. There is little guidance on when to acquire new ASIC hardware and no prior computational framework to assist the decision.

Method: MineROI-Net uses a Transformer-based architecture to capture multi-scale temporal patterns in profitability signals. Evaluated against LSTM-based and TSLANet baselines on data from 20 ASIC miners released between 2015 and 2024.

Result: 84% accuracy (83.7%), 83.1% macro F1; 93.6% precision for unprofitable periods and 98.5% for profitable periods; robust across diverse market regimes.

Conclusion: MineROI-Net provides a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive operations; the model is open-source at the provided GitHub repository.

Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.

</details>


### [22] [RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design](https://arxiv.org/abs/2512.05403)
*Gyusam Chang,Jeongyoon Yoon,Shin han yi,JaeHyeok Lee,Sujin Jang,Sangpil Kim*

Main category: cs.LG

TL;DR: RevoNAD is a reflective evolutionary system that bridges LLM-guided design with feedback-aligned neural architecture search through multi-round consensus, adaptive exploration, and Pareto-based selection, achieving state-of-the-art results across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: LLM-driven neural architecture design suffers from non-differentiable token-level design loops, mode collapse into redundant or infeasible structures, and weak grounding of constructive reasoning; a feedback-driven, grounded framework is needed to reliably guide architectural search.

Method: 1) Multi-round Multi-expert Consensus transfers diverse design rules from LLMs into actionable architectural clues; 2) Adaptive Reflective Exploration modulates exploration vs. exploitation based on reward variance, exploring under uncertainty and refining under stability; 3) Pareto-guided Evolutionary Selection jointly optimizes accuracy, efficiency, latency, confidence, and structural diversity to promote robust architectures.

Result: Empirically achieves state-of-the-art performance on CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape. Ablation and transfer studies validate effectiveness and deployability of RevoNAD in reliable neural architecture design.

Conclusion: RevoNAD effectively integrates LLM-based reasoning with feedback-aligned evolutionary search, yielding reliable, diverse, and high-performing architectures suitable for practical deployment.

Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.

</details>


### [23] [Sepsis Prediction Using Graph Convolutional Networks over Patient-Feature-Value Triplets](https://arxiv.org/abs/2512.05416)
*Bozhi Dan,Di Wu,Ji Xu,Xiang Liu,Yiziting Zhu,Xin Shu,Yujie Li,Bin Yi*

Main category: cs.LG

TL;DR: Triplet-GCN 将 EHR 数据表示为患者-特征-数值的三元组，在双向的 EHR 图上传播信息，提升 ICU 败血性疾病的早期风险预测，优于强基线的表格模型。


<details>
  <summary>Details</summary>
Motivation: EHR 数据的稀疏性、异构性和复杂性显著降低了对败血症的及时检测。需要一种端到端、能够在图结构中有效融合患者与多维特征信息的表示学习方法，以改进预测性能并提升早期警报的实用性。

Method: 提出单分支 Triplet-GCN：将每次就诊表示为患者-特征-数值三元组，构建患者-特征双域 EHR 图，先通过 GCN 进行信息传播再接一个轻量级 MLP 进行预测。对数值变量进行中位数填充与标准化，对二元特征采用效应编码，对罕见分类属性采用众数填充并引入低维嵌入。以摘要统计量初始化患者节点，同时在边上保留测量值以保留“谁测量了什么以及测量强度”。在包含3家三级医院、N=648 的中国多中心回顾性队列上（70/30 训练/测试），Triplet-GCN 在与 KNN、SVM、XGBoost、随机森林等强基线的比较中，在判别力和平衡指标上均具显著优势，具备更有利的灵敏度-特异度权衡以及早期告警的实用性。

Result: 在回顾性多中心中国队列中，Triplet-GCN 相较于强基线在分类性能与平衡指标上持续优越，显示以三元组表示和图传播的信息整合能得到更具信息量的患者表示。

Conclusion: 将 EHR 编码为患者-特征-数值三元组并在患者-特征图上进行传播，能产生更具判别力的患者表示，为可部署的败血症风险分层提供一个简单、端到端的蓝本。

Abstract: In the intensive care setting, sepsis continues to be a major contributor to patient illness and death; however, its timely detection is hindered by the complex, sparse, and heterogeneous nature of electronic health record (EHR) data. We propose Triplet-GCN, a single-branch graph convolutional model that represents each encounter as patient--feature--value triplets, constructs a bipartite EHR graph, and learns patient embeddings via a Graph Convolutional Network (GCN) followed by a lightweight multilayer perceptron (MLP). The pipeline applies type-specific preprocessing -- median imputation and standardization for numeric variables, effect coding for binary features, and mode imputation with low-dimensional embeddings for rare categorical attributes -- and initializes patient nodes with summary statistics, while retaining measurement values on edges to preserve "who measured what and by how much". In a retrospective, multi-center Chinese cohort (N = 648; 70/30 train--test split) drawn from three tertiary hospitals, Triplet-GCN consistently outperforms strong tabular baselines (KNN, SVM, XGBoost, Random Forest) across discrimination and balanced error metrics, yielding a more favorable sensitivity--specificity trade-off and improved overall utility for early warning. These findings indicate that encoding EHR as triplets and propagating information over a patient--feature graph produce more informative patient representations than feature-independent models, offering a simple, end-to-end blueprint for deployable sepsis risk stratification.

</details>


### [24] [TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning](https://arxiv.org/abs/2512.05419)
*Jonathan Adam Rico,Nagarajan Raghavan,Senthilnath Jayavelu*

Main category: cs.LG

TL;DR: TS-Hint提出了一种面向时间序列的基础模型，结合链式推理的注意力提示，解决静态特征丢失时间动态性和数据不足的问题，在极少数据场景下通过少样本学习直接从多变量时间序列中学习以预测半导体制造中的材料去除率（MRR）。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法将时间序列转化为静态特征，无法保留时间动态信息；且对大量数据依赖强。需要在有限数据条件下捕捉时间动态并直接利用多变量时间序列。

Method: 提出TS-Hint框架——Time Series Foundation Model（TSFM），并结合链式思维推理，在训练过程中基于注意力机制数据和显著性数据提供注意力提示。模型能够直接从多变量时间序列特征学习，辅以少样本学习实现强化学习或推理过程中的推理提示。

Result: 实验结果表明，在数据有限的设置下通过少样本学习表现出有效性，模型能够直接从多变量时间序列特征学习MRR相关信息。

Conclusion: TS-Hint证明了通过时间序列基础模型结合注意力提示的链式推理，可以在有限数据条件下有效捕捉时间动态并直接利用多变量时间序列进行MRR预测，具有较好的数据效率和泛化潜力。

Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.

</details>


### [25] [IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?](https://arxiv.org/abs/2512.05442)
*Hua Wang,Jinghao Lu,Fan Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.

</details>


### [26] [How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data](https://arxiv.org/abs/2512.05469)
*Zubair Ahmed Mohammad*

Main category: cs.LG

TL;DR: Ensembles reduce variance to achieve high accuracy with controlled overfitting on tabular data; effectiveness depends on data structure and noise.


<details>
  <summary>Details</summary>
Motivation: Understand when ensemble methods improve generalization in real-world tabular tasks and quantify factors that influence variance vs bias.

Method: Repeated stratified cross-validation on four tabular datasets (Breast Cancer, Heart Disease, Pima Diabetes, Credit Card Fraud). Compare linear models, a single decision tree, and nine ensemble methods. Use statistical significance tests and compute simple dataset complexity indicators (linearity score, Fisher ratio, noise estimate) to explain when ensembles work well.

Result: Ensembles can reach high accuracy with small generalization gaps by reducing variance via averaging or boosting. On near-linear/clean data, linear models generalize well and ensembles add little. On nonlinear data, tree-based ensembles improve test accuracy by 5–7 points with gaps under 3%. On noisy/imbalanced data, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. Dataset complexity indicators help predict when variance control will be effective.

Conclusion: The study offers practical guidance for model selection in real-world tabular applications and shows how dataset structure and complexity influence the gain from ensembles.

Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.

</details>


### [27] [PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning](https://arxiv.org/abs/2512.05475)
*Saumya Biswas,Jiten Oswal*

Main category: cs.LG

TL;DR: 在分子几何的层级结构下，比较 Geometric Quantum Machine Learning（G-QML）在 LiH 与 NH3 两个数据集上的对称性影响，发现图嵌入的置换对称性在泛化性与可训练性方面最具优势，且为几何学习的通用模型。


<details>
  <summary>Details</summary>
Motivation: 探究对称性约束和图嵌入在几何学习中的作用，评估不同对称性架构对准确性和泛化性的影响，以确定在几何分子数据上的最佳建模策略。

Method: 比较无对称性、旋转对称性、置换对称性，以及图嵌入的置换对称性等几何对称性约束下的量子机器学习模型；以一个经典等变基线模型比较性能；在 LiH（线性）与 NH3（三角锥形）两种几何下评估准确性和泛化性，并分析特征的图嵌入对可训练性的贡献。

Result: 结果显示：图嵌入的置换对称性显著提升了模型的可训练性与泛化性；置换对称嵌入被认为是最具一般化能力的量子机器学习模型；无对称性或仅有旋转对称性的模型在泛化性方面相对较弱；基线经典等变模型提供了性能参照。

Conclusion: 在几何学习任务中，优先采用图嵌入的置换对称性量子模型可获得最强的普适性和泛化性，且图嵌入有助于提升训练效率，建议在相关几何数据集上作为首选策略。

Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.

</details>


### [28] [Credal and Interval Deep Evidential Classifications](https://arxiv.org/abs/2512.05526)
*Michele Caprio,Shireen K. Manchingal,Fabio Cuzzolin*

Main category: cs.LG

TL;DR: 提出CDEC与IDEC两种基于证据理论的深度学习分类方法，用证 credal 集和区间预测来量化不确定性，能在不确定性超阈值时选择拒绝分类；在MNIST/CIFAR及其 OoD 变体上实现竞争性准确性、优越的 OoD 检测和良好校准的预测区间，且小规模集成即可得到稳定的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 在决策、风险评估和模型可靠性中，量化不确定性（包括可减少的知识不确定性与不可减少的天生不确定性）对于AI系统至关重要。现有UQ方法存在过拟合、标注偏差、校准差等局限，亟需具备明确理论基础且可扩展的证据深度学习框架来同时处理 epistemic 与 aleatoric 不确定性，并具备在高不确定性时拒绝分类的能力。

Method: 提出两种新方法：Credal Deep Evidential Classification (CDEC) 与 Interval Deep Evidential Classification (IDEC)。分别使用闭合且凸的概率集合（证 Credal 集）和 evidential 预测分布的区间来表征预测不确定性。通过标准反向传播训练，损失函数基于证据理论，能够在可接受的不确定性下给出标签集合及其概率保证；在不确定性超过阈值时，自动拒绝或标记不确定性来源（ epistemic/aleatoric）。与以往方法相比，克服过拟合、提供更强的不确定性分解与更稳健的预测区间。

Result: 在 MNIST、CIFAR-10 与 CIFAR-100 及其自然 OoD 变体（F-MNIST、K-MNIST、SVHN、Intel、TinyImageNet）上，CDEC 与 IDEC 展现出具有竞争力的预测准确性、在 epistemic 与总不确定性方面的先进 OoD 检测，以及在分布偏移下可靠扩展的不确定性区间；对集成规模的消融显示 CDEC 即使在小型集成下也能获得稳定的不确定性估计。

Conclusion: CDEC 与 IDEC 将证据理论引入深度学习的分类不确定性量化，提供可解释、可扩展的框架，能在需要时拒绝分类并给出可靠的不确定性分解与预测区间，显著改善分布偏移下的鲁棒性与实时决策的可信度。

Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.

</details>


### [29] [IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection](https://arxiv.org/abs/2512.05531)
*Yang Xu,Yixiao Ma,Kaifeng Zhang,Zuliang Yang,Kai Ming Ting*

Main category: cs.LG

TL;DR: 提出 IDK-S：一个增量更新的流式异常检测的分布式核方法，在内核均值嵌入框架下通过动态表示实现高精度和实时性，并可逐步增量更新，表现优于离线方法与现有流式方法。


<details>
  <summary>Details</summary>
Motivation: 数据流中的异常检测需要在分布漂移下保持高准确性，同时实现近似实时性；现有方法要么缺乏增量更新能力，要么在高成本下无法重新训练；因此需要一个可增量更新、数据依赖的核方法。

Method: 提出 IDK-S：在内核均值嵌入框架中引入一个增量式的分布核，继承 Isolation Distributional Kernel 的数据自适应核优点，并采用轻量级增量更新机制，避免完整再训练，同时保持与完整模型等价的统计性质。

Result: 在十三个基准数据集上，该方法在检测准确性方面优于现有最先进方法，同时在计算效率方面显著更快，许多情况下快一个数量级。

Conclusion: IDK-S 提供了一种高效、准确、可增量更新的流数据异常检测解决方案，适用于需要实时处理的场景，并且实现对分布漂移的鲁棒性。

Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.

</details>


### [30] [SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection](https://arxiv.org/abs/2512.05540)
*Yang Xu,Hang Zhang,Yixiao Ma,Ye Zhu,Kai Ming Ting*

Main category: cs.LG

TL;DR: 提出一种高效的多视角一致邻域表示方法 SCoNE，用于大规模多视图异常检测。


<details>
  <summary>Details</summary>
Motivation: 多视角异常检测中，需在不同视图之间对局部邻域进行一致表示。但现有方法在独立构建邻域后再融合，容易受不同视图密度差异影响，且计算成本高，难以在大规模数据上应用。

Method: 提出 SCoNE（Spherical Consistent Neighborhoods Ensemble）。核心是直接使用多视图实例来表示一致邻域，不用中间表示；邻域具有数据自适应的大小特征（在稀疏区域邻域大，在密集区域邻域小），据此实现无需学习的局部一致性，从而实现线性时间复杂度 O(N)；通过集合集成获得鲁棒的一致邻域表示。

Result: 实验表明 SCoNE 在检测准确性方面优于现有方法，且在大数据集上运行速度比现有方法快若干数量级。

Conclusion: SCoNE 提供了一种高效且有效的多视图一致邻域表示框架，适用于大规模多视图异常检测。未来工作可能包括对不同密度分布下鲁棒性的进一步分析，以及与深度学习等方法的结合。

Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.

</details>


### [31] [Improving Local Fidelity Through Sampling and Modeling Nonlinearity](https://arxiv.org/abs/2512.05556)
*Sanjeev Shrestha,Rahul Dubey,Hui Liu*

Main category: cs.LG

TL;DR: 提出一种基于MARS的本地非线性解释方法，并结合N-ball采样替代LIME的重加权，以提升局部解释的保真度。对3个UCI数据集和多分类器/核宽度进行评估，平均RMSE降低37%，明显提升局部保真性。


<details>
  <summary>Details</summary>
Motivation: LIME等局部解释方法假设局部边界线性，忽略非线性关系，导致解释不准确；需要更高保真度的局部解释以适用于高风险场景。

Method: 在LIME框架下引入MARS来拟合局部非线性边界，并使用N-ball采样直接从目标分布采样，替代LIME中的样本再加权过程，从而提高解释的保真度。

Result: 在3个UCI数据集、不同分类器和核宽度设置下，与 baselines 相比，提出的方法在保真度方面表现更佳，平均RMSE下降约37%。

Conclusion: 将MARS和N-ball采样结合的局部解释框架显著提升了局部保真性，给出比现有基线更可信的模型解释。

Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.

</details>


### [32] [Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning](https://arxiv.org/abs/2512.05591)
*Zhenpeng Su,Leiyu Pan,Minxuan Lv,Tiehua Mei,Zijia Lin,Yuntao Li,Wenping Hu,Ruiming Tang,Kun Gai,Guorui Zhou*

Main category: cs.LG

TL;DR: 提出 Entropy Ratio Clipping (ERC)，以当前策略与上一轮策略之间的熵比作为全局指标来约束策略更新的分布变化，从而稳定 off-policy 强化学习中的更新，并将 ERC 集成到 DAPO 与 GPPO，实验在多项基准上显示性能提升。


<details>
  <summary>Details</summary>
Motivation: 离线/离策略 RL 的分布漂移导致策略超出信赖域，进而引发训练不稳定，尤其表现为策略熵波动和梯度不稳定。尽管 PPO-Clip 通过重要性裁剪缓解，但仍未对全局行动分布的变化进行约束。

Method: 提出以当前策略和前一策略的熵比作为全局度量，构建 Entropy Ratio Clipping，实施双向约束熵比的变化；将 ERC 融入 DAPO 与 GPPO 的学习流程。

Result: 在多项基准上实验，ERC 显示出性能提升，证明在全局分布层面对策略更新的稳定性具有显著作用。

Conclusion: ERC 提供一种稳定策略更新的新全局约束，补充 PPO-Clip 在未采样动作概率变化方面的不足，且在 DAPO/GPPO 上表现出一致的性能提升。

Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.

</details>


### [33] [Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales](https://arxiv.org/abs/2512.05620)
*Shikai Qiu,Zixi Chen,Hoang Phan,Qi Lei,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 在矩阵级预条件优化器（Shampoo、SOAP、Muon）上，通过类似μP的超参数传递进行尺度化，发现合适的学习率和权重衰减随模型宽度/深度的缩放对比 AdamW 的相对提升；但存在有限宽度误差，需要屏蔽（blocking）和显式谱归一化来缓解。


<details>
  <summary>Details</summary>
Motivation: 理解大规模训练中预条件优化器的有效性及跨宽度/深度的超参数传递，解决这些优化器在可重复性和可比性方面的质疑。

Method: 对 Shampoo、SOAP、Muon 等优化器进行广泛实验，研究学习率与权重衰减随宽度/深度的缩放规律；考察 blocking、grafting、谱归一化等因素的影响；在 190M–1.4B 的 Llama 架构上进行训练对比。

Result: μP 缩放的学习率传递有助于迁移，但有限宽度偏差导致学习率漂移；通过 blocking 与显式谱归一化可缓解；独立于宽度的权重衰减（≈1/width）缩放接近最优；在这些缩放规则下，Muon 与 Shampoo 相对 AdamW 的加速约为 1.4× 与 1.3×，错误缩放下加速消失。

Conclusion: 强调在跨规模比较优化器时，需研究最优超参数传递；正确的缩放策略下，矩阵级预条件优化器可带来显著加速；应在实际的调参预算下进行公平评估。

Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $μ$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $μ$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.

</details>


### [34] [Bounded Graph Clustering with Graph Neural Networks](https://arxiv.org/abs/2512.05623)
*Kibidi Neocosmos,Diego Baptista,Nicole Ludwig*

Main category: cs.LG

TL;DR: 提出一种GNN中可控的聚类数框架，允许通过指定聚类数的范围来训练并获得期望数量的簇，甚至在给定精确簇数时也能可靠返回。


<details>
  <summary>Details</summary>
Motivation: 在社区检测中，簇数往往难以事先确定，且穷举搜索不可行；现有GNN方法很难控制输出的簇数，与经典算法相比存在差距。

Method: 设计一个灵活且有原则的方法，在训练中对簇数量设定可行的范围并强制满足；如果需要精确的簇数，可以直接指定并在输出中可靠返回。

Result: 框架实现对簇数的约束，能够在训练中维持指定范围，并在要求精确簇数时给出可靠的簇数结果。

Conclusion: 为GNN在社区检测中的簇数控制提供统一、可观测的机制，提升模型的可控性和适用性。

Abstract: In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.

</details>


### [35] [Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs](https://arxiv.org/abs/2512.05648)
*Igor Shilov,Alex Cloud,Aryo Pradipta Gema,Jacob Goldman-Wetzler,Nina Panickssery,Henry Sleight,Erik Jones,Cem Anil*

Main category: cs.LG

TL;DR: SGTM 是一种以选择性梯度屏蔽实现的未学习方法，在存在标签噪声时对目标知识的忘记比数据过滤和早期 Gradient Routing 更鲁棒，并对抗性微调也更为困难，显示出良好的保留/忘记权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型具备双重用途风险，数据过滤成本高且模型规模增大降低了对标签错误的容忍度。需要在预训练阶段引入对目标知识的受控记忆/遗忘机制，提升对噪声的鲁棒性。

Method: 提出 Selective GradienT Masking (SGTM)：通过对选定的梯度进行零屏蔽，使目标域示例仅更新其专用参数子集，达到对知识的选择性保留与移除。通过两项任务评估：在双语合成数据上移除一种语言的知识；在英文维基百科语料上移除生物学知识。与数据过滤及此前的 Gradient Routing 相比，SGTM 在带标签噪声时具有更好的保留/忘记权衡，且对抗性微调具更强鲁棒性。

Result: 在两项任务中，SGTM 在标签噪声存在时实现更好的忘记与保留平衡，优于数据过滤与早期 Gradient Routing；对抗性微调下，需七倍以上的微调步数才能达到忘记集合的基线性能，显示更强鲁棒性。

Conclusion: SGTM 作为对现有安全 mitigations 的有前景的预训练阶段补充，特别是在标签噪声不可避免的场景中具有显著潜力。

Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.

</details>


### [36] [Feasibility of AI-Assisted Programming for End-User Development](https://arxiv.org/abs/2512.05666)
*Irene Weber*

Main category: cs.LG

TL;DR: AI-assisted end-user coding is a feasible paradigm for end-user development that could complement or potentially replace low-code/no-code platforms; a case study showed non-programmers could build a basic web app with AI assistants in reasonable time and with positive attitudes toward the approach.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional end-user development via LCNC (e.g., vendor lock-in, limited flexibility) by leveraging generative AI copilots to enable end users to generate and refine code directly from natural language prompts.

Method: A case study where non-programmers interacted with AI assistants to develop a basic web application, examining task completion time, success rate, and user perceptions.

Result: Most participants completed the task within a reasonable time frame and expressed support for AI-assisted end-user coding as a viable approach, indicating promise for practical adoption.

Conclusion: AI-assisted end-user coding shows feasibility as a development paradigm for end users and could complement or replace LCNC in the future; further research, practice guidelines, and teaching implications discussed.

Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.

</details>


### [37] [Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks](https://arxiv.org/abs/2512.05680)
*Alexander Mattick,George Yammine,Georgios Kontes,Setareh Maghsudi,Christopher Mutschler*

Main category: cs.LG

TL;DR: 提出一种基于部分可观测马尔可夫决策过程（POMDP）的在线波束选择框架，将环境建模为码本本身，通过对未观测的最优波束的信念状态进行条件化的候选波束选择来定位移动中的最优波束；在大码本场景下能适应新路径与环境变化，显著优于监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 在大规模天线阵列的模拟/现实中，传统的模拟/代码本射束成形需要大量码本并且易受反射、遮挡影响，现有工作多采用监督学习预测下一个最佳波束，难以适应动态环境与未见路径，因此需要一种可在线自适应的波束管理方法。

Method: 将环境建模为码本本身，将波束选取问题建模为POMDP，在每个时步基于对未观测到的最优波束的信念状态以及已探测的波束，选择一个候选波束，从而进行在线搜索以定位移动的最优波束。

Result: 与现有工作相比，误差显著降低，能够处理新出现的轨迹和环境变化，效果达到数量级级的提升。

Conclusion: 将POMDP引入波束管理的在线搜索框架可实现对动态环境的鲁棒性，尤其在大码本场景下，优于基于监督学习的预测方法。

Abstract: Beamforming-capable antenna arrays with many elements enable higher data rates in next generation 5G and 6G networks. In current practice, analog beamforming uses a codebook of pre-configured beams with each of them radiating towards a specific direction, and a beam management function continuously selects \textit{optimal} beams for moving user equipments (UEs). However, large codebooks and effects caused by reflections or blockages of beams make an optimal beam selection challenging. In contrast to previous work and standardization efforts that opt for supervised learning to train classifiers to predict the next best beam based on previously selected beams we formulate the problem as a partially observable Markov decision process (POMDP) and model the environment as the codebook itself. At each time step, we select a candidate beam conditioned on the belief state of the unobservable optimal beam and previously probed beams. This frames the beam selection problem as an online search procedure that locates the moving optimal beam. In contrast to previous work, our method handles new or unforeseen trajectories and changes in the physical environment, and outperforms previous work by orders of magnitude.

</details>


### [38] [Learnability Window in Gated Recurrent Neural Networks](https://arxiv.org/abs/2512.05790)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: 提出一个框架，将门控RNN的梯度传播中的门控结构与可学习窗口H_N联系起来，揭示有效学习率μ_{t,ℓ}如何决定梯度传输的规模和各向异性，并在重尾梯度噪声下给出N(ℓ)和H_N的解析推导及对门谱与噪声的预测。


<details>
  <summary>Details</summary>
Motivation: 与以往仅关注Jacobian乘积数值稳定性的分析不同，作者认为学习能力由门控引入的有效学习率控制，需从一阶展开入手建立从门控Jacobian到样本复杂度的联系。

Method: 对门控诱导的Jacobian乘积在反向传播过程中的一阶近似进行分析，提出在每个时滞ℓ及每个神经元层面的μ_{t,ℓ}作为有效学习率；在重尾α稳定噪声下推导最小样本量N(ℓ)与f(ℓ)=||μ_{t,ℓ}||_1的关系；给出H_N的显式公式以及f(ℓ)的对数、幂次、指数衰减的收敛规律。

Result: 给出可实际计算的H_N公式及f(ℓ)衰减的三种等级（对数、幂次、指数）规律；表明更宽的门谱或更异质的谱会导致f(ℓ)衰减更慢、学习窗口更大；噪声的重尾特性则压缩H_N。

Conclusion: 将门控时序结构、梯度噪声与样本复杂度联系起来，确立有效学习率作为决定何时以及多久能够学习长程依赖的核心量。

Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $μ_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($α$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-α}$, where $f(\ell)=\|μ_{t,\ell}\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory predicts that broader or more heterogeneous gate spectra produce slower decay of $f(\ell)$ and hence larger learnability windows, whereas heavier-tailed noise compresses $\mathcal{H}_N$ by slowing statistical concentration. By linking gate-induced time-scale structure, gradient noise, and sample complexity, the framework identifies the effective learning rates as the fundamental quantities that govern when -- and for how long -- gated recurrent networks can learn long-range temporal dependencies.

</details>


### [39] [Mechanistic Interpretability of Antibody Language Models Using SAEs](https://arxiv.org/abs/2512.05794)
*Rebonto Haque,Oliver M. Turnbull,Anisha Parsan,Nithin Parsan,John J. Yang,Charlotte M. Deane*

Main category: cs.LG

TL;DR: TopK SAEs mappings reveal concept-latent features but do not guarantee causal control over generation; Ordered SAEs establish a hierarchical, steerable feature structure at the cost of more complex activation patterns.


<details>
  <summary>Details</summary>
Motivation: 提升对域特定蛋白质语言模型的机械可解释性，通过分析稀疏自编码器（SAEs）在自回归抗体语言模型p-IgGen中的潜在特征来理解与引导生成。

Method: 比较两种SAE变体（TopK SAEs与Ordered SAEs）在p-IgGen上的应用，评估特征概念相关性及其对生成的因果控制，以及有序结构对可引导性的影响，分析激活模式的复杂性与可解释性。

Result: TopK SAEs能够揭示具有生物学意义的潜在特征，但高特征概念相关性并不保证对生成过程的因果控制。相比之下，Ordered SAEs强制层级结构，能更可靠地识别可引导的特征，但伴随更复杂且不可解释性更高的激活模式。

Conclusion: 在将潜在特征映射到概念方面，TopK SAEs足以；若需要更精确的生成引导，Ordered SAEs更为可取，但需接受更大的解释复杂性。

Abstract: Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.

</details>


### [40] [Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization](https://arxiv.org/abs/2512.05825)
*Shuhei Watanabe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.

</details>


### [41] [NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation](https://arxiv.org/abs/2512.05844)
*Daniel Rose,Roxane Axel Jacob,Johannes Kirchmair,Thierry Langer*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.

</details>


### [42] [Sparse Attention Post-Training for Mechanistic Interpretability](https://arxiv.org/abs/2512.05865)
*Florent Draye,Anson Lei,Ingmar Posner,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.

</details>


### [43] [Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks](https://arxiv.org/abs/2512.05868)
*Brian Ezinwoke,Oliver Rhodes*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.

</details>


### [44] [Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning](https://arxiv.org/abs/2512.05870)
*Daniel Miliate,Ashlie Martini*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.

</details>


### [45] [Neural Coherence : Find higher performance to out-of-distribution tasks from few samples](https://arxiv.org/abs/2512.05880)
*Simon Guiroy,Mats Richter,Sarath Chandar,Christopher Pal*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.

</details>


### [46] [DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints](https://arxiv.org/abs/2512.05881)
*Rahul Golder,Bimol Nath Roy,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.

</details>


### [47] [LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction](https://arxiv.org/abs/2512.05915)
*Marius F. R. Juston,Ramavarapu S. Sreenivas,Dustin Nottage,Ahmet Soylemezoglu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\%-13\% accuracy gains over SLL Layers on 121 UCI data sets.

</details>


### [48] [KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity](https://arxiv.org/abs/2512.05916)
*Damien Lesens,Beheshteh T. Rakhshan,Guillaume Rabusseau*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.

</details>


### [49] [Developing synthetic microdata through machine learning for firm-level business surveys](https://arxiv.org/abs/2512.05948)
*Jorge Cisneros Paz,Timothy Wojan,Matthew Williams,Jennifer Ozawa,Robert Chew,Kimberly Janda,Timothy Navarro,Michael Floyd,Christine Task,Damon Streat*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.

</details>


### [50] [Impugan: Learning Conditional Generative Models for Robust Data Imputation](https://arxiv.org/abs/2512.05950)
*Zalish Mahmud,Anantaa Kotal,Aritran Piplai*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025

</details>


### [51] [MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution](https://arxiv.org/abs/2512.05958)
*Sara Patel,Mingxun Zhou,Giulia Fanti*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.

</details>


### [52] [Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity](https://arxiv.org/abs/2512.05962)
*Germán Kruszewski,Pierre Erbacher,Jos Rozen,Marc Dymetman*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [53] [One-Step Diffusion Samplers via Self-Distillation and Deterministic Flow](https://arxiv.org/abs/2512.05251)
*Pascal Jutras-Dube,Jiaru Zhang,Ziran Wang,Ruqi Zhang*

Main category: stat.ML

TL;DR: One-step diffusion samplers compress many small steps into a single large step by learning a step-conditioned ODE, with deterministic-flow ELBO estimation and volume-consistency regularization, achieving competitive sampling and robust ELBOs in few steps.


<details>
  <summary>Details</summary>
Motivation: Sampling from unnormalized target distributions is computationally expensive with existing iterative samplers; reducing the number of steps while maintaining sample quality and reliable evidence estimates is highly desirable.

Method: Train a step-conditioned ODE via a state-space consistency loss so one large step imitates the trajectory of many small steps. Develop a deterministic-flow (DF) importance weight for ELBO estimation without a backward kernel, and introduce volume-consistency regularization to align accumulated volume change across step resolutions.

Result: Achieves competitive sample quality on synthetic and Bayesian benchmarks with orders-of-magnitude fewer network evaluations; ELBO estimates remain robust even in few-step regimes.

Conclusion: The proposed one-step diffusion sampler offers efficient sampling and stable evidence estimation in one or only a few steps.

Abstract: Sampling from unnormalized target distributions is a fundamental yet challenging task in machine learning and statistics. Existing sampling algorithms typically require many iterative steps to produce high-quality samples, leading to high computational costs. We introduce one-step diffusion samplers which learn a step-conditioned ODE so that one large step reproduces the trajectory of many small ones via a state-space consistency loss. We further show that standard ELBO estimates in diffusion samplers degrade in the few-step regime because common discrete integrators yield mismatched forward/backward transition kernels. Motivated by this analysis, we derive a deterministic-flow (DF) importance weight for ELBO estimation without a backward kernel. To calibrate DF, we introduce a volume-consistency regularization that aligns the accumulated volume change along the flow across step resolutions. Our proposed sampler therefore achieves both sampling and stable evidence estimate in only one or few steps. Across challenging synthetic and Bayesian benchmarks, it achieves competitive sample quality with orders-of-magnitude fewer network evaluations while maintaining robust ELBO estimates.

</details>


### [54] [Symmetric Linear Dynamical Systems are Learnable from Few Observations](https://arxiv.org/abs/2512.05337)
*Minh Vu,Andrey Y. Lokhov,Marc Vuffray*

Main category: stat.ML

TL;DR: 基于矩量法的新估计器：在单轨迹T=O(log N)下对对称动态矩阵实现逐元素最大误差很小的恢复，且对稀疏/密集矩阵均适用，且无需正则化。


<details>
  <summary>Details</summary>
Motivation: 在仅有单一时间轨迹、有限观测条件下学习N维随机线性动态系统参数，并实现高精度重建，尤其对结构发现具有意义。

Method: 提出基于矩量法的估计器，用来估计对称动态矩阵的参数；不依赖问题特定的正则化，适用于全观测与部分观测场景；分析在观测数量仅为T=O(log N)时的误差界限。

Result: 证明在T=O(log N)观测下实现对称动态矩阵的逐元素最大误差很小；误差与矩阵稀疏性无关；对全观测和部分观测均有效。

Conclusion: 该方法为结构发现等应用提供一种无需正则化的矩量法估计器，即使在极少观测下也能稳定地恢复对称动态矩阵。

Abstract: We consider the problem of learning the parameters of a $N$-dimensional stochastic linear dynamics under both full and partial observations from a single trajectory of time $T$. We introduce and analyze a new estimator that achieves a small maximum element-wise error on the recovery of symmetric dynamic matrices using only $T=\mathcal{O}(\log N)$ observations, irrespective of whether the matrix is sparse or dense. This estimator is based on the method of moments and does not rely on problem-specific regularization. This is especially important for applications such as structure discovery.

</details>


### [55] [Do We Really Even Need Data? A Modern Look at Drawing Inference with Predicted Data](https://arxiv.org/abs/2512.05456)
*Stephen Salerno,Kentaro Hoffman,Awan Afiaz,Anna Neufeld,Tyler H. McCormick,Jeffrey T. Leek*

Main category: stat.ML

TL;DR: 使用预测数据进行推断并非预测准确就一定可靠；核心问题在于偏差与方差的处理欠缺，导致对自变量与结果的关系的错误推断。本论文提供一个理论框架并回顾IPD方法，强调与经典统计的联系，以及在实际研究中实现透明、统计学上有原则的使用。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能与机器学习工具日益普及，数据收集成本上升、响应率下降等挑战迫使研究者用预测数据替代真实观测。需要明确在预测数据下进行推断的可行性、鲁棒性与合理性，以避免误导性结论。

Method: 理论分析将使用预测数据的推断问题归纳为两类核心统计问题：偏差（预测系统性地改变效应估计或扭曲变量关系）与方差（忽略预测模型不确定性及数据本身的变异性）。并对最近的IPD（使用预测数据进行推断）方法进行综述，展示其与经典统计理论的联系。

Result: 给出一个系统的IPD推断框架，说明高预测准确度不等同于有效推断；将失败归因于偏差与方差两方面，并整理现有方法来缓解这些问题；讨论若干未解问题及未来研究方向，强调在科学研究中实现透明且统计学上有原则的使用预测数据。

Conclusion: 强调以统计学原则和透明性为核心的IPD应用，呼吁未来工作在方法论层面解决偏差与方差问题，并提出对预测数据使用的实际指南与研究方向。

Abstract: As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g., rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as substitutes for missing or unobserved data. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to drawing inference with predicted data (IPD) and show that high predictive accuracy does not guarantee valid downstream inference. We show that all such failures reduce to statistical notions of (i) bias, when predictions systematically shift the estimand or distort relationships among variables, and (ii) variance, when uncertainty from the prediction model and the intrinsic variability of the true data are ignored. We then review recent methods for conducting IPD and discuss how this framework is deeply rooted in classical statistical theory. We then comment on some open questions and interesting avenues for future work in this area, and end with some comments on how to use predicted data in scientific studies that is both transparent and statistically principled.

</details>


### [56] [Design-marginal calibration of Gaussian process predictive distributions: Bayesian and conformal approaches](https://arxiv.org/abs/2512.05611)
*Aurélien Pion,Emmanuel Vazquez*

Main category: stat.ML

TL;DR: 提出两种GP插值设定下的预测分布校准方法：cps-gp和bcr-gp，通过设计-边际校准实现可靠的不确定性表达，并在基准函数上与Jackknife+及全 conformal GP比较。


<details>
  <summary>Details</summary>
Motivation: 在高斯过程插值中，预测分布的校准性往往不足，需从设计分布的角度实现边际覆盖和概率校准，支持序贯设计和鲁棒不确定性表达。

Method: cps-gp：将 conformal预测系统改造为GP插值场景，利用标准化的留一残差，得到逐步的预测分布且具有限样本边际校准。bcr-gp：保留GP后验均值，用广义正态模型拟合通过交叉验证得到的标准化残差，结合基于贝叶斯的选择规则（基于方差的后验上分位数或基于KS的跨后验校验）来控制分散与尾部行为并得到光滑的预测分布。

Result: 在基准函数的数值实验中，比较对象包括 cps-gp、bcr-gp、GP的 Jackknife+、以及完整的 conformal GP；使用覆盖率、KS、积分绝对误差等校准指标，以及通过尺度化的连续等级分数（CRPS）衡量准确性/锋利度。结果显示两种方法分别在边际校准和分布平滑方面具有优势，且贝叶斯选择规则能在保守性与尾部控制之间取得权衡。

Conclusion: 所提方法为GP插值情境下的预测分布提供可控的校准机制；cps-gp具有限样本边际校准的保证，bcr-gp提供光滑且可控尾部的预测分布，基于贝叶斯选择的策略可根据序贯设计需求在保守性与校准之间进行权衡，整体提升不确定性量化的可靠性与实用性。

Abstract: We study the calibration of Gaussian process (GP) predictive distributions in the interpolation setting from a design-marginal perspective. Conditioning on the data and averaging over a design measure μ, we formalize μ-coverage for central intervals and μ-probabilistic calibration through randomized probability integral transforms. We introduce two methods. cps-gp adapts conformal predictive systems to GP interpolation using standardized leave-one-out residuals, yielding stepwise predictive distributions with finite-sample marginal calibration. bcr-gp retains the GP posterior mean and replaces the Gaussian residual by a generalized normal model fitted to cross-validated standardized residuals. A Bayesian selection rule-based either on a posterior upper quantile of the variance for conservative prediction or on a cross-posterior Kolmogorov-Smirnov criterion for probabilistic calibration-controls dispersion and tail behavior while producing smooth predictive distributions suitable for sequential design. Numerical experiments on benchmark functions compare cps-gp, bcr-gp, Jackknife+ for GPs, and the full conformal Gaussian process, using calibration metrics (coverage, Kolmogorov-Smirnov, integral absolute error) and accuracy or sharpness through the scaled continuous ranked probability score.

</details>


### [57] [BalLOT: Balanced $k$-means clustering with optimal transport](https://arxiv.org/abs/2512.05926)
*Wenyan Luo,Dustin G. Mixon*

Main category: stat.ML

TL;DR: BalLOT is an optimal-transport–based alternating minimization method for balanced k-means clustering. It shows that, for generic data, the transport couplings are integral at each step, provides landscape guarantees for exact and partial recovery of planted clusters under the stochastic ball model, and proposes initialization schemes that enable one-step recovery. Numerical experiments support its fast and effective performance.


<details>
  <summary>Details</summary>
Motivation: Balanced k-means clustering is a fundamental problem where balance constraints complicate standard k-means. An optimal-transport viewpoint offers a natural way to couple data points to clusters, enabling principled and potentially faster algorithms with provable guarantees.

Method: Introduce BalLOT, an optimal-transport-based alternating minimization framework for balanced k-means. Prove that, for generic data, the coupling matrices are integral at every step. Conduct landscape analysis to derive theoretical guarantees for exact and partial recovery of planted clusters under the stochastic ball model. Propose initialization schemes that achieve one-step recovery of planted clusters.

Result: BalLOT delivers a fast and effective solution to balanced k-means. Numerical experiments illustrate strong empirical performance. Theoretical contributions include integrality of the transport couplings for generic data, and recovery guarantees (exact and partial) under the stochastic ball model, along with initialization schemes enabling one-step recovery.

Conclusion: BalLOT provides a fast, theoretically grounded approach to balanced k-means clustering. It endows the alternating minimization with optimal transport-based couplings, guarantees integrality for generic data, and offers recovery guarantees for planted clusters under the stochastic ball model, aided by initialization strategies that enable one-step recovery.

Abstract: We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.

</details>


### [58] [Consequences of Kernel Regularity for Bandit Optimization](https://arxiv.org/abs/2512.05957)
*Madison Lee,Tara Javidi*

Main category: stat.ML

TL;DR: 本工作通过谱分析统一了核正则性与带时优化中的算法性能之间的关系，揭示各类常用核（如Matérn、平方指数、有理二次、γ-指数、分段多项式、Dirichlet核）在傅里叶谱衰减上的共同决定作用，并给出对应的渐近 regret 及信息增益界限，同时将全局高斯过程方法与局部多项式估计的混合方法LP-GP-UCB置于同一框架下进行评估。


<details>
  <summary>Details</summary>
Motivation: 揭示核正则性与局部平滑性两种视角在谱层面的内在联系，建立一个统一的分析框架，以便在不同核族之间转化分析工具并给出明确的渐近性能界限。

Method: 对多类常见核的傅里叶谱进行严格刻画，分析谱衰减率对信息增益及渐近 regret 的影响；利用谱衰减来建立Hölder/Besov等函数空间嵌入，从而将全局核方法与局部平滑方法联系起来；在LP-GP-UCB框架下评估混合方法的渐近最优性。

Result: 给出各核族的明确衰减率与相应的信息增益上界，进而推导出逐步的regret界和对数/幂次级的渐近阶，若干情形给出新颖结果；LP-GP-UCB在多核族下可达到渐进最优性，虽不对所有情形严格优于专门方法，但在多核族场景下表现出较强的适用性。

Conclusion: 谱层面的统一分析框架成功地把核方法与局部自适应方法纳入同一范式，便于在不同核族之间迁移分析工具，并为未来的核设计与混合策略提供理论支撑。

Abstract: In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Matérn, square-exponential, rational-quadratic, $γ$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish Hölder space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.

</details>
