<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 52]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Hierarchical Sparse Plus Low Rank Compression of LLM](https://arxiv.org/abs/2601.07839)
*Pawan Kumar,Aditi Gupta*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern large language models (LLMs) place extraordinary pressure on memory and compute budgets, making principled compression indispensable for both deployment and continued training. We present Hierarchical Sparse Plus Low-Rank (HSS) compression, a two-stage scheme that (i) removes the largest-magnitude weights into a sparse matrix S and (ii) applies a recursive Hierarchically Sparse Separable (HSS) low-rank factorisation to the dense residual matrix. A recursive rank-reducing strategy and a reverse Cuthill-Mckee (RCM) permutation are introduced to align high weights towards the diagonal with the block-diagonal hierarchy, maximising off-diagonal compressibility (because they are touched only once). HSS is hardware-friendly: its matrix-vector multiply reduces to one sparse and a sequence of thin-matrix multiplications and can be trained end-to-end with standard optimisers.
  Experiments on LLaMA-7B show that targeting only the self-attention projections (1.6 B parameters of Q, K, and V matrices out of a total 7B parameters) suffices to yield large memory savings while retaining comparable state-of-the-art perplexity scores on test samples of the WikiText dataset. For example, with a 30\% sparsity budget and an outer rank of 512, sHSS-RCM achieves a perplexity of 1.64, outperforming dense baselines and classical sparse-plus-SVD variants, while also achieving significant memory savings.

</details>


### [2] [Affect and Effect: Limitations of regularisation-based continual learning in EEG-based emotion classification](https://arxiv.org/abs/2601.07858)
*Nina Peire,Yupei Li,Björn Schuller*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generalisation to unseen subjects in EEG-based emotion classification remains a challenge due to high inter-and intra-subject variability. Continual learning (CL) poses a promising solution by learning from a sequence of tasks while mitigating catastrophic forgetting. Regularisation-based CL approaches, such as Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI), and Memory Aware Synapses (MAS), are commonly used as baselines in EEG-based CL studies, yet their suitability for this problem remains underexplored. This study theoretically and empirically finds that regularisation-based CL methods show limited performance for EEG-based emotion classification on the DREAMER and SEED datasets. We identify a fundamental misalignment in the stability-plasticity trade-off, where regularisation-based methods prioritise mitigating catastrophic forgetting (backward transfer) over adapting to new subjects (forward transfer). We investigate this limitation under subject-incremental sequences and observe that: (1) the heuristics for estimating parameter importance become less reliable under noisy data and covariate shift, (2) gradients on parameters deemed important by these heuristics often interfere with gradient updates required for new subjects, moving optimisation away from the minimum, (3) importance values accumulated across tasks over-constrain the model, and (4) performance is sensitive to subject order. Forward transfer showed no statistically significant improvement over sequential fine-tuning (p > 0.05 across approaches and datasets). The high variability of EEG signals means past subjects provide limited value to future subjects. Regularisation-based continual learning approaches are therefore limited for robust generalisation to unseen subjects in EEG-based emotion classification.

</details>


### [3] [HOSC: A Periodic Activation with Saturation Control for High-Fidelity Implicit Neural Representations](https://arxiv.org/abs/2601.07870)
*Michal Jan Wlodarczyk,Danzel Serrano,Przemyslaw Musialski*

Main category: cs.LG

TL;DR: 提出一种带饱和控制的周期性激活 HOSC，其形式为 tanh(β sin(ω0 x))，通过 βω0 给出显式的 Lipschitz 上界，可以在保留周期载波的同时调控梯度幅度，并在多个领域的 INRs 表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 在隐式神经表示中，周期性激活（如正弦）可保留高频信息，但容易导致梯度不稳定且对多尺度行为的控制不足，需要一种可控、易调参的周期性激活来提升 INR 的表现。

Method: 提出 HOSC 激活：HOSC(x) = tanh(β sin(ω0 x))，引入显式参数 β，理论上 Lipschitz 上界为 β ω0；通过数学分析与标准化训练协议对图像、音频、视频、NeRF、SDF 等任务进行广泛实验，并与 SIREN、FINER 等方法进行对比。

Result: 实验表明 HOSC 在多模态任务上具有显著或竞争性的性能，并给出域特定的超参数选择指南；代码公开在项目页面。

Conclusion: HOSC 是一种实用的周期性激活，适用于 INR 应用，且提供明确的超参数设计原则以控制梯度和多尺度表达。

Abstract: Periodic activations such as sine preserve high-frequency information in implicit neural representations (INRs) through their oscillatory structure, but often suffer from gradient instability and limited control over multi-scale behavior. We introduce the Hyperbolic Oscillator with Saturation Control (HOSC) activation, $\text{HOSC}(x) = \tanh\bigl(β\sin(ω_0 x)\bigr)$, which exposes an explicit parameter $β$ that controls the Lipschitz bound of the activation by $βω_0$. This provides a direct mechanism to tune gradient magnitudes while retaining a periodic carrier. We provide a mathematical analysis and conduct a comprehensive empirical study across images, audio, video, NeRFs, and SDFs using standardized training protocols. Comparative analysis against SIREN, FINER, and related methods shows where HOSC provides substantial benefits and where it achieves competitive parity. Results establish HOSC as a practical periodic activation for INR applications, with domain-specific guidance on hyperparameter selection. For code visit the project page https://hosc-nn.github.io/ .

</details>


### [4] [Multiplicative Orthogonal Sequential Editing for Language Models](https://arxiv.org/abs/2601.07873)
*Hao-Xiang Xu,Jun-Yu Ma,Ziqi Peng,Yuhao Sun,Zhen-Hua Ling,Jia-Chen Gu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Knowledge editing aims to efficiently modify the internal knowledge of large language models (LLMs) without compromising their other capabilities. The prevailing editing paradigm, which appends an update matrix to the original parameter matrix, has been shown by some studies to damage key numerical stability indicators (such as condition number and norm), thereby reducing editing performance and general abilities, especially in sequential editing scenario. Although subsequent methods have made some improvements, they remain within the additive framework and have not fundamentally addressed this limitation. To solve this problem, we analyze it from both statistical and mathematical perspectives and conclude that multiplying the original matrix by an orthogonal matrix does not change the numerical stability of the matrix. Inspired by this, different from the previous additive editing paradigm, a multiplicative editing paradigm termed Multiplicative Orthogonal Sequential Editing (MOSE) is proposed. Specifically, we first derive the matrix update in the multiplicative form, the new knowledge is then incorporated into an orthogonal matrix, which is multiplied by the original parameter matrix. In this way, the numerical stability of the edited matrix is unchanged, thereby maintaining editing performance and general abilities. We compared MOSE with several current knowledge editing methods, systematically evaluating their impact on both editing performance and the general abilities across three different LLMs. Experimental results show that MOSE effectively limits deviations in the edited parameter matrix and maintains its numerical stability. Compared to current methods, MOSE achieves a 12.08% improvement in sequential editing performance, while retaining 95.73% of general abilities across downstream tasks. The code is available at https://github.com/famoustourist/MOSE.

</details>


### [5] [NOVAK: Unified adaptive optimizer for deep neural networks](https://arxiv.org/abs/2601.07876)
*Sergii Kavun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work introduces NOVAK, a modular gradient-based optimization algorithm that integrates adaptive moment estimation, rectified learning-rate scheduling, decoupled weight regularization, multiple variants of Nesterov momentum, and lookahead synchronization into a unified, performance-oriented framework. NOVAK adopts a dual-mode architecture consisting of a streamlined fast path designed for production. The optimizer employs custom CUDA kernels that deliver substantial speedups (3-5 for critical operations) while preserving numerical stability under standard stochastic-optimization assumptions. We provide fully developed mathematical formulations for rectified adaptive learning rates, a memory-efficient lookahead mechanism that reduces overhead from O(2p) to O(p + p/k), and the synergistic coupling of complementary optimization components. Theoretical analysis establishes convergence guarantees and elucidates the stability and variance-reduction properties of the method. Extensive empirical evaluation on CIFAR-10, CIFAR-100, ImageNet, and ImageNette demonstrates NOVAK superiority over 14 contemporary optimizers, including Adam, AdamW, RAdam, Lion, and Adan. Across architectures such as ResNet-50, VGG-16, and ViT, NOVAK consistently achieves state-of-the-art accuracy, and exceptional robustness, attaining very high accuracy on VGG-16/ImageNette demonstrating superior architectural robustness compared to contemporary optimizers. The results highlight that NOVAKs architectural contributions (particularly rectification, decoupled decay, and hybrid momentum) are crucial for reliable training of deep plain networks lacking skip connections, addressing a long-standing limitation of existing adaptive optimization methods.

</details>


### [6] [E^2-LLM: Bridging Neural Signals and Interpretable Affective Analysis](https://arxiv.org/abs/2601.07877)
*Fei Ma,Han Lin,Yifan Xie,Hongwei Ren,Xiaoyu Shen,Wenbo Ding,Qi Tian*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Emotion recognition from electroencephalography (EEG) signals remains challenging due to high inter-subject variability, limited labeled data, and the lack of interpretable reasoning in existing approaches. While recent multimodal large language models (MLLMs) have advanced emotion analysis, they have not been adapted to handle the unique spatiotemporal characteristics of neural signals. We present E^2-LLM (EEG-to-Emotion Large Language Model), the first MLLM framework for interpretable emotion analysis from EEG. E^2-LLM integrates a pretrained EEG encoder with Qwen-based LLMs through learnable projection layers, employing a multi-stage training pipeline that encompasses emotion-discriminative pretraining, cross-modal alignment, and instruction tuning with chain-of-thought reasoning. We design a comprehensive evaluation protocol covering basic emotion prediction, multi-task reasoning, and zero-shot scenario understanding. Experiments on the dataset across seven emotion categories demonstrate that E^2-LLM achieves excellent performance on emotion classification, with larger variants showing enhanced reliability and superior zero-shot generalization to complex reasoning scenarios. Our work establishes a new paradigm combining physiological signals with LLM reasoning capabilities, showing that model scaling improves both recognition accuracy and interpretable emotional understanding in affective computing.

</details>


### [7] [Sliced-Wasserstein Distribution Alignment Loss Improves the Ultra-Low-Bit Quantization of Large Language Models](https://arxiv.org/abs/2601.07878)
*Deyu Cao,Yixin Yin,Samin Aref*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The benefits of most large language models come with steep and often hidden economic and environmental costs due to their resource usage inefficiency during deployment. Model quantization improves energy and memory efficiency through representing model parameters by lower-precision values. However, compression below 4-bits often distorts activation distributions and degrades performance. We address this challenge by introducing a sliced Wasserstein loss function for distribution-aware calibration in ultra-low-bit post-training quantization. The proposed loss aligns the output distributions of full-precision and quantized models under random linear projections, complementing standard mean-squared error loss without adding any computational overhead during inference. Our proposed loss function can be incorporated with any post-training quantization framework that has a retraining component. We demonstrate the performance gains of our proposed model by incorporating it with two frontier methods known as OmniQuant and TesseraQ. Compared to these two baselines, the proposed loss consistently improves both perplexity and downstream task accuracy across multiple ultra-low-bit settings. Our proposed loss function recovers 4.12-20.37% of the OmniQuant's lost accuracy on the language model LLaMA-2-7B, 0.93-7.65% on OPT-6.7B, and 2.26-6.20% on LLaMA-2-13B. TesseraQ's accuracy degradation is recovered by 3.63-7.63% in relative terms when augmented by our proposed loss function. Taken together, these results demonstrate that distributional alignment provides a simple yet effective performance boost that can push the limits of frontier quantization methods. Our method is available on GitHub to facilitate future progress in ultra-low-bit quantization.

</details>


### [8] [KVzap: Fast, Adaptive, and Faithful KV Cache Pruning](https://arxiv.org/abs/2601.07891)
*Simon Jegou,Maximilian Jeblick*

Main category: cs.LG

TL;DR: KVzap是一种快速、输入自适应的KV缓存近似方法，用于变换器模型的推理中KV缓存的压缩。它在prefilling和解码阶段均可工作，能实现2–4×的KV缓存压缩，同时几乎不损失准确性，在多种模型和任务上达到KVpress排行榜的最优表现。


<details>
  <summary>Details</summary>
Motivation: 随着上下文长度增加，Transformer的KV缓存成为推理的瓶颈。尽管已有多种KV缓存剪枝方法，但在主流推理引擎中的速度与准确性权衡使其难以落地。需要一种快速、对输入自适应的压缩方法，兼容prefill和解码两阶段推理。

Method: 提出KVzap，一种快速、输入自适应的KVzip近似方法，能够应用于prefill和解码阶段。通过对KV缓存进行高效近似和自适应剪枝，在保持不可观测的准确性损失的前提下实现2–4×的缓存压缩。该方法在多个模型（Qwen3-8B、Llama-3.1-8B-Instruct、Qwen3-32B）和长上下文/推理任务上进行评估。

Result: 在上述模型与任务上，KVzap实现了2–4×的KV缓存压缩，且几乎无准确性损失，同时在KVpress排行榜上达到了State-of-the-Art的表现。

Conclusion: 代码和模型已开源，可在GitHub（NVIDIA/kvpress）获取，具备实际工程落地潜力。

Abstract: Growing context lengths in transformer-based language models have made the key-value (KV) cache a critical inference bottleneck. While many KV cache pruning methods have been proposed, they have not yet been adopted in major inference engines due to speed--accuracy trade-offs. We introduce KVzap, a fast, input-adaptive approximation of KVzip that works in both prefilling and decoding. On Qwen3-8B, Llama-3.1-8B-Instruct, and Qwen3-32B across long-context and reasoning tasks, KVzap achieves $2$--$4\times$ KV cache compression with negligible accuracy loss and achieves state-of-the-art performance on the KVpress leaderboard. Code and models are available at https://github.com/NVIDIA/kvpress.

</details>


### [9] [Sherry: Hardware-Efficient 1.25-Bit Ternary Quantization via Fine-grained Sparsification](https://arxiv.org/abs/2601.07892)
*Hong Huang,Decheng Wu,Qiangqiang Hu,Guanghua Yu,Jinhai Yang,Jianchen Zhu,Xue Liu,Dapeng Wu*

Main category: cs.LG

TL;DR: Sherry提出一种硬件友好的三值量化框架，通过3:4细粒度稀疏和五比特打包实现1.25-bit等效宽度，提升对齐与存储效率；同时引入Arenas以解决稀疏训练中的表示崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署大规模语言模型面临高内存与计算成本的问题。现有三值量化要么使用2-bit对齐打包导致比特浪费，要么使用1.67-bit的非规则打包降低推理速度，因此需要一种硬件友好且高效的量化方案。

Method: 提出Sherry框架：采用3:4细粒度稀疏结构，将四个权重打包成五位(bit)以实现近似1.25-bit的对齐宽度，从而恢复对齐的幂-of-two效率。此外，提出Arenas（退火残差突触机制）以在稀疏三值训练中维持表示多样性，避免权重陷阱导致的表示崩溃。在LLaMA-3.2上进行多基准评估。

Result: 在五项基准上，Sherry与最先进的三值量化方法保持相当的性能，同时显著降低模型尺寸；在1B模型、搭载Intel i7-14700HX CPU时，达到了与SOTA基线同等准确率，且实现了25%的比特节省与10%的推理速度提升。

Conclusion: Sherry提供一种与硬件对齐的高效三值量化解决方案，适合资源受限的边缘设备部署；Arenas通过在训练阶段维持表示多样性，解决稀疏训练中的 representational collapse。代码公开，便于复现与进一步优化。

Abstract: The deployment of Large Language Models (LLMs) on resource-constrained edge devices is increasingly hindered by prohibitive memory and computational requirements. While ternary quantization offers a compelling solution by reducing weights to {-1, 0, +1}, current implementations suffer from a fundamental misalignment with commodity hardware. Most existing methods must choose between 2-bit aligned packing, which incurs significant bit wastage, or 1.67-bit irregular packing, which degrades inference speed. To resolve this tension, we propose Sherry, a hardware-efficient ternary quantization framework. Sherry introduces a 3:4 fine-grained sparsity that achieves a regularized 1.25-bit width by packing blocks of four weights into five bits, restoring power-of-two alignment. Furthermore, we identify weight trapping issue in sparse ternary training, which leads to representational collapse. To address this, Sherry introduces Arenas, an annealing residual synapse mechanism that maintains representational diversity during training. Empirical evaluations on LLaMA-3.2 across five benchmarks demonstrate that Sherry matches state-of-the-art ternary performance while significantly reducing model size. Notably, on an Intel i7-14700HX CPU, our 1B model achieves zero accuracy loss compared to SOTA baselines while providing 25% bit savings and 10% speed up. The code is available at https://github.com/Tencent/AngelSlim .

</details>


### [10] [Revealing the Attention Floating Mechanism in Masked Diffusion Models](https://arxiv.org/abs/2601.07894)
*Xin Dai,Pengcheng Huang,Zhenghao Liu,Shuo Wang,Yukun Yan,Chaojun Xiao,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.LG

TL;DR: Masked diffusion models (MDMs) exhibit dynamic, floating attention anchors (Attention Floating), unlike autoregressive models (ARMs) which converge to a fixed sink. Shallow layers use floating tokens to build a global structure, while deep layers focus on semantic content, creating a shallow-structure aware, deep-content focused attention mechanism. This explains the strong in-context learning and higher knowledge-task performance of MDMs, potentially doubling ARM performance. Code is available at the provided URL.


<details>
  <summary>Details</summary>
Motivation: To understand and explain why MDMs can outperform ARMs on knowledge-intensive tasks by dissecting their internal attention mechanics, which remain under-explored.

Method: Analyze attention dynamics across denoising steps and layers in masked diffusion models; identify Attention Floating phenomenon; characterize the distribution of attention anchors over time; associate shallow vs deep layer roles with global structure vs semantic content; relate findings to in-context learning performance.

Result: Attention anchors in MDMs are dynamic and dispersed, shifting across denoising steps and layers (Attention Floating). Shallow layers leverage floating tokens to build a global structural framework, while deeper layers allocate more capacity to semantic content. This mechanism provides a plausible explanation for stronger in-context learning and improved performance on knowledge-intensive tasks, reportedly doubling ARMs’ performance.

Conclusion: The observed Attention Floating pattern and the shallow-structure/deep-content attention strategy offer a mechanistic explanation for MDMs’ strong in-context learning and enhanced knowledge-task performance, suggesting new directions for designing diffusion-based models and attention mechanisms.

Abstract: Masked diffusion models (MDMs), which leverage bidirectional attention and a denoising process, are narrowing the performance gap with autoregressive models (ARMs). However, their internal attention mechanisms remain under-explored. This paper investigates the attention behaviors in MDMs, revealing the phenomenon of Attention Floating. Unlike ARMs, where attention converges to a fixed sink, MDMs exhibit dynamic, dispersed attention anchors that shift across denoising steps and layers. Further analysis reveals its Shallow Structure-Aware, Deep Content-Focused attention mechanism: shallow layers utilize floating tokens to build a global structural framework, while deeper layers allocate more capability toward capturing semantic content. Empirically, this distinctive attention pattern provides a mechanistic explanation for the strong in-context learning capabilities of MDMs, allowing them to double the performance compared to ARMs in knowledge-intensive tasks. All codes and datasets are available at https://github.com/NEUIR/Attention-Floating.

</details>


### [11] [Large Language Models and Algorithm Execution: Application to an Arithmetic Function](https://arxiv.org/abs/2601.07898)
*Farah Ben Slama,Frédéric Armetta*

Main category: cs.LG

TL;DR: 提出 LLM-DAL（Large Language Model - Decompositional Algorithmic Learning）训练框架，通过对推理分解的有监督训练，显著提升 LLM 在算法推理和泛化方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型在统计学习和泛化方面表现突出，但在内化处理的数据以及自主执行算法方面存在局限性，需要通过更结构化的推理能力来提升其算法执行能力。

Method: 提出 LLM-DAL 训练模型，聚焦于推理分解的有监督训练，设计一种引导学习过程的方法，使模型在解决复杂算法推理时能逐步分解问题并执行算法步骤。

Result: 证明在采用专门设计来引导学习过程的训练方法后，LLMs 在执行复杂算法推断和泛化方面的能力显著提升。

Conclusion: 表明通过对推理分解的有监督训练，可以显著提升 LLM 的算法执行能力，LLM-DAL 为实现该目标提供了一个可操作的框架。

Abstract: Large Language Models (LLMs) have recently developed new advanced functionalities. Their effectiveness relies on statistical learning and generalization capabilities. However, they face limitations in internalizing the data they process and struggle, for instance, to autonomously execute algorithms. In this paper, we investigate the possibility of extending these models' capabilities to algorithm execution through specialized supervised training focused on reasoning decomposition. We introduce a training model called LLM-DAL (Large Language Model - Decompositional Algorithmic Learning), through which we demonstrate that LLMs' ability to perform complex algorithmic inferences and generalize can be significantly improved when the training method is properly designed to guide the model in its learning process.

</details>


### [12] [Enhancing Large Language Models for Time-Series Forecasting via Vector-Injected In-Context Learning](https://arxiv.org/abs/2601.07903)
*Jianqi Zhang,Jingyao Wang,Wenwen Qiang,Fanjiang Xu,Changwen Zheng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The World Wide Web needs reliable predictive capabilities to respond to changes in user behavior and usage patterns. Time series forecasting (TSF) is a key means to achieve this goal. In recent years, the large language models (LLMs) for TSF (LLM4TSF) have achieved good performance. However, there is a significant difference between pretraining corpora and time series data, making it hard to guarantee forecasting quality when directly applying LLMs to TSF; fine-tuning LLMs can mitigate this issue, but often incurs substantial computational overhead. Thus, LLM4TSF faces a dual challenge of prediction performance and compute overhead. To address this, we aim to explore a method for improving the forecasting performance of LLM4TSF while freezing all LLM parameters to reduce computational overhead. Inspired by in-context learning (ICL), we propose LVICL. LVICL uses our vector-injected ICL to inject example information into a frozen LLM, eliciting its in-context learning ability and thereby enhancing its performance on the example-related task (i.e., TSF). Specifically, we first use the LLM together with a learnable context vector adapter to extract a context vector from multiple examples adaptively. This vector contains compressed, example-related information. Subsequently, during the forward pass, we inject this vector into every layer of the LLM to improve forecasting performance. Compared with conventional ICL that adds examples into the prompt, our vector-injected ICL does not increase prompt length; moreover, adaptively deriving a context vector from examples suppresses components harmful to forecasting, thereby improving model performance. Extensive experiments demonstrate the effectiveness of our approach.

</details>


### [13] [Coupled Diffusion-Encoder Models for Reconstruction of Flow Fields](https://arxiv.org/abs/2601.07946)
*AmirPouya Hemmasian,Amir Barati Farimani*

Main category: cs.LG

TL;DR: DiffCoder 将扩散模型作为生成解码器，与卷积 ResNet 编码器耦合，对流场进行强压缩下的统计规律重建；在Kolmogorov流数据上，相比 VAE，在谱特性和分布结构保持方面表现更好，尤其在信息瓶颈强时。


<details>
  <summary>Details</summary>
Motivation: 传统自编码器（如 VAE）在强压缩下往往无法保留流场的高阶统计结构和光谱性质，导致统计分布失真。需要一种能在保持重建像素误差的同时，恢复流场分布和谱特性的解码机制。

Method: 将卷积 ResNet 编码器与一个条件化的扩散模型耦合，端到端训练。编码器将流场压缩为潜在向量，扩散模型在给定该潜在向量的条件下学习对重建的生成先验，从而在点wise 损失之外优化分布和谱特性

Result: 在多模型规模和不同压缩比的设置下，基于 DiffCoder 的谱精度显著优于 VAE，且在强压缩时 VAEs 发生较大退化。两者的相对 L2 重建误差相近，但 DiffCoder 更好地保持了流场的分布结构。当压缩在中等水平时，足够大的 VAEs 仍具竞争力，表明扩散先验在信息瓶颈严重时最具优势。

Conclusion: 扩散模型作为解码的生成先验，能够提供统计一致且紧凑的流场表示，是实现复杂流场统计特性的有效路径，尤其在信息瓶颈较严重时效果更突出。

Abstract: Data-driven flow-field reconstruction typically relies on autoencoder architectures that compress high-dimensional states into low-dimensional latent representations. However, classical approaches such as variational autoencoders (VAEs) often struggle to preserve the higher-order statistical structure of fluid flows when subjected to strong compression. We propose DiffCoder, a coupled framework that integrates a probabilistic diffusion model with a conventional convolutional ResNet encoder and trains both components end-to-end. The encoder compresses the flow field into a latent representation, while the diffusion model learns a generative prior over reconstructions conditioned on the compressed state. This design allows DiffCoder to recover distributional and spectral properties that are not strictly required for minimizing pointwise reconstruction loss but are critical for faithfully representing statistical properties of the flow field. We evaluate DiffCoder and VAE baselines across multiple model sizes and compression ratios on a challenging dataset of Kolmogorov flow fields. Under aggressive compression, DiffCoder significantly improves the spectral accuracy while VAEs exhibit substantial degradation. Although both methods show comparable relative L2 reconstruction error, DiffCoder better preserves the underlying distributional structure of the flow. At moderate compression levels, sufficiently large VAEs remain competitive, suggesting that diffusion-based priors provide the greatest benefit when information bottlenecks are severe. These results demonstrate that the generative decoding by diffusion offers a promising path toward compact, statistically consistent representations of complex flow fields.

</details>


### [14] [Reinforcement Learning Methods for Neighborhood Selection in Local Search](https://arxiv.org/abs/2601.07948)
*Yannick Molinghen,Augustin Delecluse,Renaud De Landtsheer,Stefano Michelini*

Main category: cs.LG

TL;DR: 通过对多种RL方法在局部搜索中的邻域选择进行评估，比较带上限置信区间的多臂赌博机、ε-greedy、PPO、Double DQN等，在TSP、PDPTW、Car Sequencing三个问题上，发现ε-greedy表现较为稳健，DRL方法因计算开销较大而在较长运行时间下才具备竞争力。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在组合优化中显示出潜力，但将其用于局部搜索中的邻域选择的研究仍相对不足。本研究旨在系统评估不同RL策略在多问题场景下的有效性、鲁棒性与奖励设计的重要性。

Method: 评估一系列RL邻域选择策略：多臂赌博机（UCB、ε-greedy）和深度RL（PPO、Double DQN），并将它们与多种基线在三个问题上进行比较；强调搜索中特殊的成本波动（由于约束违规惩罚）需要精心设计的奖励函数以提供稳定且具有信息性的学习信号；进行大规模实验，比较性能与运行时间。

Result: 结果显示算法性能高度依赖问题类型，但ε-greedy在多数场景中保持了较好表现。深度RL方法的计算开销较大，使其只有在运行时间较长的设定下才具竞争力。

Conclusion: 对局部搜索而言，强化学习具有潜力，但其效果高度依赖具体问题和奖励设计；简单的bandit方法往往更稳健高效，深度RL需要权衡奖励结构与计算成本以实现实用性。

Abstract: Reinforcement learning has recently gained traction as a means to improve combinatorial optimization methods, yet its effectiveness within local search metaheuristics specifically remains comparatively underexamined. In this study, we evaluate a range of reinforcement learning-based neighborhood selection strategies -- multi-armed bandits (upper confidence bound, $ε$-greedy) and deep reinforcement learning methods (proximal policy optimization, double deep $Q$-network) -- and compare them against multiple baselines across three different problems: the traveling salesman problem, the pickup and delivery problem with time windows, and the car sequencing problem. We show how search-specific characteristics, particularly large variations in cost due to constraint violation penalties, necessitate carefully designed reward functions to provide stable and informative learning signals. Our extensive experiments reveal that algorithm performance varies substantially across problems, although that $ε$-greedy consistently ranks among the best performers. In contrast, the computational overhead of deep reinforcement learning approaches only makes them competitive with a substantially longer runtime. These findings highlight both the promise and the practical limitations of deep reinforcement learning in local search.

</details>


### [15] [InfGraND: An Influence-Guided GNN-to-MLP Knowledge Distillation](https://arxiv.org/abs/2601.08033)
*Amir Eskandari,Aman Anand,Elyas Rashno,Farhana Zulkernine*

Main category: cs.LG

TL;DR: InfGraND 提出了一种基于结构影响力的知识蒸馏框架，将 GNN 的知识以节点重要性为优先级传递给 MLP，同时通过一次性多跳邻域特征预计算让 MLP 获得结构感知，实现在传导和归纳场景下的优越 KD 性能，同时降低推理时的开销。


<details>
  <summary>Details</summary>
Motivation: GNN 在图数据分析中效果突出，但聚合/更新操作带来低延迟推理和资源约束场景的挑战。虽然简单的 MLP 更高效，但监督学习的直接训练往往性能不足。因此需要一种高效且结构感知的知识蒸馏方法，将 GNN 的结构信息转移给更轻量的 MLP。

Method: 提出 InfGraND，通过识别并优先学习对图结构影响最大的节点来引导 GNN 到 MLP 的蒸馏过程，使学生在最关键的图结构区域获得知识；并通过一次性多跳邻域特征的预计算将结构信息嵌入到 MLP 输入中，避免推理时的额外开销。

Result: 在传导性（transductive）和归纳性（inductive）设置下，对七个同质图基准数据集的评估显示，InfGraND 在现有的 GNN→MLP 知识蒸馏方法上持续优越，证明了其在现实需要低延迟的应用中的可行性。

Conclusion: InfGraND 通过将结构影响力融入蒸馏流程并以一次性特征预计算实现结构感知的轻量级 MLP，使得在延迟敏感场景中也能获得更好的知识迁移效果，具有较好的实用性与推广潜力。

Abstract: Graph Neural Networks (GNNs) are the go-to model for graph data analysis. However, GNNs rely on two key operations - aggregation and update, which can pose challenges for low-latency inference tasks or resource-constrained scenarios. Simple Multi-Layer Perceptrons (MLPs) offer a computationally efficient alternative. Yet, training an MLP in a supervised setting often leads to suboptimal performance. Knowledge Distillation (KD) from a GNN teacher to an MLP student has emerged to bridge this gap. However, most KD methods either transfer knowledge uniformly across all nodes or rely on graph-agnostic indicators such as prediction uncertainty. We argue this overlooks a more fundamental, graph-centric inquiry: "How important is a node to the structure of the graph?" We introduce a framework, InfGraND, an Influence-guided Graph KNowledge Distillation from GNN to MLP that addresses this by identifying and prioritizing structurally influential nodes to guide the distillation process, ensuring that the MLP learns from the most critical parts of the graph. Additionally, InfGraND embeds structural awareness in MLPs through one-time multi-hop neighborhood feature pre-computation, which enriches the student MLP's input and thus avoids inference-time overhead. Our rigorous evaluation in transductive and inductive settings across seven homophilic graph benchmark datasets shows InfGraND consistently outperforms prior GNN to MLP KD methods, demonstrating its practicality for numerous latency-critical applications in real-world settings.

</details>


### [16] [Riemannian Zeroth-Order Gradient Estimation with Structure-Preserving Metrics for Geodesically Incomplete Manifolds](https://arxiv.org/abs/2601.08039)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we study Riemannian zeroth-order optimization in settings where the underlying Riemannian metric $g$ is geodesically incomplete, and the goal is to approximate stationary points with respect to this incomplete metric. To address this challenge, we construct structure-preserving metrics that are geodesically complete while ensuring that every stationary point under the new metric remains stationary under the original one. Building on this foundation, we revisit the classical symmetric two-point zeroth-order estimator and analyze its mean-squared error from a purely intrinsic perspective, depending only on the manifold's geometry rather than any ambient embedding. Leveraging this intrinsic analysis, we establish convergence guarantees for stochastic gradient descent with this intrinsic estimator. Under additional suitable conditions, an $ε$-stationary point under the constructed metric $g'$ also corresponds to an $ε$-stationary point under the original metric $g$, thereby matching the best-known complexity in the geodesically complete setting. Empirical studies on synthetic problems confirm our theoretical findings, and experiments on a practical mesh optimization task demonstrate that our framework maintains stable convergence even in the absence of geodesic completeness.

</details>


### [17] [LUT-Compiled Kolmogorov-Arnold Networks for Lightweight DoS Detection on IoT Edge Devices](https://arxiv.org/abs/2601.08044)
*Oleksandr Kuznetsov*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Denial-of-Service (DoS) attacks pose a critical threat to Internet of Things (IoT) ecosystems, yet deploying effective intrusion detection on resource-constrained edge devices remains challenging. Kolmogorov-Arnold Networks (KANs) offer a compact alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate spline functions on edges rather than fixed activations on nodes, achieving competitive accuracy with fewer parameters. However, runtime B-spline evaluation introduces significant computational overhead unsuitable for latency-critical IoT applications. We propose a lookup table (LUT) compilation pipeline that replaces expensive spline computations with precomputed quantized tables and linear interpolation, dramatically reducing inference latency while preserving detection quality. Our lightweight KAN model (50K parameters, 0.19~MB) achieves 99.0\% accuracy on the CICIDS2017 DoS dataset. After LUT compilation with resolution $L=8$, the model maintains 98.96\% accuracy (F1 degradation $<0.0004$) while achieving $\mathbf{68\times}$ speedup at batch size 256 and over $\mathbf{5000\times}$ speedup at batch size 1, with only $2\times$ memory overhead. We provide comprehensive evaluation across LUT resolutions, quantization schemes, and out-of-bounds policies, establishing clear Pareto frontiers for accuracy-latency-memory trade-offs. Our results demonstrate that LUT-compiled KANs enable real-time DoS detection on CPU-only IoT gateways with deterministic inference latency and minimal resource footprint.

</details>


### [18] [Q-realign: Piggybacking Realignment on Quantization for Safe and Efficient LLM Deployment](https://arxiv.org/abs/2601.08089)
*Qitao Tan,Xiaoying Song,Ningxi Cheng,Ninghao Liu,Xiaoming Zhai,Lingzi Hong,Yanzhi Wang,Zhen Xiang,Geng Yuan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Public large language models (LLMs) are typically safety-aligned during pretraining, yet task-specific fine-tuning required for deployment often erodes this alignment and introduces safety risks. Existing defenses either embed safety recovery into fine-tuning or rely on fine-tuning-derived priors for post-hoc correction, leaving safety recovery tightly coupled with training and incurring high computational overhead and a complex workflow. To address these challenges, we propose \texttt{Q-realign}, a post-hoc defense method based on post-training quantization, guided by an analysis of representational structure. By reframing quantization as a dual-objective procedure for compression and safety, \texttt{Q-realign} decouples safety alignment from fine-tuning and naturally piggybacks into modern deployment pipelines. Experiments across multiple models and datasets demonstrate that our method substantially reduces unsafe behaviors while preserving task performance, with significant reductions in memory usage and GPU hours. Notably, our approach can recover the safety alignment of a fine-tuned 7B LLM on a single RTX 4090 within 40 minutes. Overall, our work provides a practical, turnkey solution for safety-aware deployment.

</details>


### [19] [Local-Global Feature Fusion for Subject-Independent EEG Emotion Recognition](https://arxiv.org/abs/2601.08094)
*Zheng Zhou,Isabella McEvoy,Camilo E. Valderrama*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Subject-independent EEG emotion recognition is challenged by pronounced inter-subject variability and the difficulty of learning robust representations from short, noisy recordings. To address this, we propose a fusion framework that integrates (i) local, channel-wise descriptors and (ii) global, trial-level descriptors, improving cross-subject generalization on the SEED-VII dataset. Local representations are formed per channel by concatenating differential entropy with graph-theoretic features, while global representations summarize time-domain, spectral, and complexity characteristics at the trial level. These representations are fused in a dual-branch transformer with attention-based fusion and domain-adversarial regularization, with samples filtered by an intensity threshold. Experiments under a leave-one-subject-out protocol demonstrate that the proposed method consistently outperforms single-view and classical baselines, achieving approximately 40% mean accuracy in 7-class subject-independent emotion recognition. The code has been released at https://github.com/Danielz-z/LGF-EEG-Emotion.

</details>


### [20] [STO-RL: Offline RL under Sparse Rewards via LLM-Guided Subgoal Temporal Order](https://arxiv.org/abs/2601.08107)
*Chengyang Gu,Yuxin Pan,Hui Xiong,Yize Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Offline reinforcement learning (RL) enables policy learning from pre-collected datasets, avoiding costly and risky online interactions, but it often struggles with long-horizon tasks involving sparse rewards. Existing goal-conditioned and hierarchical offline RL methods decompose such tasks and generate intermediate rewards to mitigate limitations of traditional offline RL, but usually overlook temporal dependencies among subgoals and rely on imprecise reward shaping, leading to suboptimal policies. To address these issues, we propose STO-RL (Offline RL using LLM-Guided Subgoal Temporal Order), an offline RL framework that leverages large language models (LLMs) to generate temporally ordered subgoal sequences and corresponding state-to-subgoal-stage mappings. Using this temporal structure, STO-RL applies potential-based reward shaping to transform sparse terminal rewards into dense, temporally consistent signals, promoting subgoal progress while avoiding suboptimal solutions. The resulting augmented dataset with shaped rewards enables efficient offline training of high-performing policies. Evaluations on four discrete and continuous sparse-reward benchmarks demonstrate that STO-RL consistently outperforms state-of-the-art offline goal-conditioned and hierarchical RL baselines, achieving faster convergence, higher success rates, and shorter trajectories. Ablation studies further confirm STO-RL's robustness to imperfect or noisy LLM-generated subgoal sequences, demonstrating that LLM-guided subgoal temporal structures combined with theoretically grounded reward shaping provide a practical and scalable solution for long-horizon offline RL.

</details>


### [21] [Intra-tree Column Subsampling Hinders XGBoost Learning of Ratio-like Interactions](https://arxiv.org/abs/2601.08121)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 在 XGBoost 的树内列子采样中，若数据中存在类比比值的结构，子采样会削弱模型对这类信号的联合利用，导致性能下降；若事先 engineered ratio 特征存在，影响显著减弱。


<details>
  <summary>Details</summary>
Motivation: 探究树内列采样对融合信号（如比值/比率形式的信号）的影响，特别是在存在污染项（nuisance factor）时，模型是否需要通过协同分裂来“合成”该信号。

Method: 使用两种具有抵消结构的合成数据生成过程；在每棵树内对列进行子采样，变动 colsample_bylevel 和 colsample_bynode，取 s ∈ {0.4,0.6,0.8,0.9}，重点关注较小的子采样（s≥0.8 为主）。设有对照特征集，其中包含工程化的比值特征以消除合成需求。评估测试集 PR-AUC；同时引入基于路径的共用度量以追踪信号利用的变化。

Result: 在原始特征（仅原始特征）设置下，树内列采样减少了测试集的 PR-AUC；当两个参数均为 0.4 时，相对下降可达 54%；当包含工程化的比值特征时，性能下降几乎消失。路径共用度量在出现性能下降的单元格也同步下降。

Conclusion: 如果信号中存在比值/比率结构，需避免树内子采样，或在模型中包含目标比值特征以便于模型“合成”该信号。

Abstract: Many applied problems contain signal that becomes clear only after combining multiple raw measurements. Ratios and rates are common examples. In gradient boosted trees, this combination is not an explicit operation: the model must synthesize it through coordinated splits on the component features. We study whether intra-tree column subsampling in XGBoost makes that synthesis harder. We use two synthetic data generating processes with cancellation-style structure. In both, two primitive features share a strong nuisance factor, while the target depends on a smaller differential factor. A log ratio cancels the nuisance and isolates the signal. We vary colsample_bylevel and colsample_bynode over s in {0.4, 0.6, 0.8, 0.9}, emphasizing mild subsampling (s >= 0.8). A control feature set includes the engineered ratio, removing the need for synthesis. Across both processes, intra-tree column subsampling reduces test PR-AUC in the primitives-only setting. In the main process the relative decrease reaches 54 percent when both parameters are set to 0.4. The effect largely disappears when the engineered ratio is present. A path-based co-usage metric drops in the same cells where performance deteriorates. Practically, if ratio-like structure is plausible, either avoid intra-tree subsampling or include the intended ratio features.

</details>


### [22] [Generalization Analysis and Method for Domain Generalization for a Family of Recurrent Neural Networks](https://arxiv.org/abs/2601.08122)
*Atefeh Termehchi,Ekram Hossain,Isaac Woungang*

Main category: cs.LG

TL;DR: 利用Koopman算子对RNN状态演化近似线性化，并通过谱分析评估域移位对泛化误差的影响；在此基础上提出域泛化方法以提升OOD鲁棒性，且在实用时序任务中得到验证。


<details>
  <summary>Details</summary>
Motivation: 深度学习在可解释性与泛化性方面存在局限，尤其在安全关键场景；时序数据存在时间相关性，常规泛化分析往往假设独立同分布不成立；需分析RNN的可解释性并提升对分布改变的鲁棒性。

Method: 把RNN的状态演化建模为未知的离散时间非线性闭环反馈系统；使用Koopman算子将非线性动力学线性化以实现可解释性；通过谱分析量化域移位对泛化误差的最坏影响；在此分析基础上提出域泛化方法以降低OOD泛化误差并提升鲁棒性。

Result: 在实际的时序模式学习任务上验证，所提方法能降低OOD泛化误差并提升对分布漂移的鲁棒性。

Conclusion: 基于Koopman线性化与谱分析的框架为RNN的可解释性与域泛化提供一种系统性与原理性的方法论，并能提升对分布漂移的鲁棒性。

Abstract: Deep learning (DL) has driven broad advances across scientific and engineering domains. Despite its success, DL models often exhibit limited interpretability and generalization, which can undermine trust, especially in safety-critical deployments. As a result, there is growing interest in (i) analyzing interpretability and generalization and (ii) developing models that perform robustly under data distributions different from those seen during training (i.e. domain generalization). However, the theoretical analysis of DL remains incomplete. For example, many generalization analyses assume independent samples, which is violated in sequential data with temporal correlations. Motivated by these limitations, this paper proposes a method to analyze interpretability and out-of-domain (OOD) generalization for a family of recurrent neural networks (RNNs). Specifically, the evolution of a trained RNN's states is modeled as an unknown, discrete-time, nonlinear closed-loop feedback system. Using Koopman operator theory, these nonlinear dynamics are approximated with a linear operator, enabling interpretability. Spectral analysis is then used to quantify the worst-case impact of domain shifts on the generalization error. Building on this analysis, a domain generalization method is proposed that reduces the OOD generalization error and improves the robustness to distribution shifts. Finally, the proposed analysis and domain generalization approach are validated on practical temporal pattern-learning tasks.

</details>


### [23] [Reverse Flow Matching: A Unified Framework for Online Reinforcement Learning with Diffusion and Flow Policies](https://arxiv.org/abs/2601.08136)
*Zeyang Li,Sunbochen Tang,Navid Azizan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diffusion and flow policies are gaining prominence in online reinforcement learning (RL) due to their expressive power, yet training them efficiently remains a critical challenge. A fundamental difficulty in online RL is the lack of direct samples from the target distribution; instead, the target is an unnormalized Boltzmann distribution defined by the Q-function. To address this, two seemingly distinct families of methods have been proposed for diffusion policies: a noise-expectation family, which utilizes a weighted average of noise as the training target, and a gradient-expectation family, which employs a weighted average of Q-function gradients. Yet, it remains unclear how these objectives relate formally or if they can be synthesized into a more general formulation. In this paper, we propose a unified framework, reverse flow matching (RFM), which rigorously addresses the problem of training diffusion and flow models without direct target samples. By adopting a reverse inferential perspective, we formulate the training target as a posterior mean estimation problem given an intermediate noisy sample. Crucially, we introduce Langevin Stein operators to construct zero-mean control variates, deriving a general class of estimators that effectively reduce importance sampling variance. We show that existing noise-expectation and gradient-expectation methods are two specific instances within this broader class. This unified view yields two key advancements: it extends the capability of targeting Boltzmann distributions from diffusion to flow policies, and enables the principled combination of Q-value and Q-gradient information to derive an optimal, minimum-variance estimator, thereby improving training efficiency and stability. We instantiate RFM to train a flow policy in online RL, and demonstrate improved performance on continuous-control benchmarks compared to diffusion policy baselines.

</details>


### [24] [Dynamic Graph Structure Learning via Resistance Curvature Flow](https://arxiv.org/abs/2601.08149)
*Chaoqun Fei,Huanjiang Liu,Tinglve Zhou,Yangyang Li,Tianyong Hao*

Main category: cs.LG

TL;DR: 提出 Resistance Curvature Flow (RCF)，以有效电阻替代昂贵的曲率优化，显著加速 OCF 的计算并保持相近的几何优化能力，进而通过 DGSL-RCF 提升深度度量学习、流形学习与图结构学习中的表示质量与下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决 GRL 领域中基于欧氏距离的静态图构建难以捕捉数据流形曲率的问题，同时克服 Ollivier-Ricci 曲率流中依赖最优传输(Wasserstein)所带来的高计算成本，使大规模数据与深度学习场景的曲率优化成为可能。

Method: 以电路物理中的有效电阻为灵感，将昂贵的曲率优化转化为高效的矩阵运算，通过曲率梯度重分配边权以消除拓扑噪声、增强局部簇结构；给出理论基础与动力学原理，并设计 DGSL-RCF 作为具体实现，兼容深度学习框架。

Result: 实验结果表明，DGSL-RCF 在深度度量学习、流形学习和图结构学习等任务中显著提升表示质量与下游任务性能；相比 OCF，计算获得超过 100×的加速，且保持与 OCF 相当的几何优化能力。

Conclusion: RCF 提供一个可扩展、易于与深度学习结合的曲率驱动图优化框架，适用于大规模 GRL 应用，提升表示学习效果并降低计算成本。

Abstract: Geometric Representation Learning (GRL) aims to approximate the non-Euclidean topology of high-dimensional data through discrete graph structures, grounded in the manifold hypothesis. However, traditional static graph construction methods based on Euclidean distance often fail to capture the intrinsic curvature characteristics of the data manifold. Although Ollivier-Ricci Curvature Flow (OCF) has proven to be a powerful tool for dynamic topological optimization, its core reliance on Optimal Transport (Wasserstein distance) leads to prohibitive computational complexity, severely limiting its application in large-scale datasets and deep learning frameworks. To break this bottleneck, this paper proposes a novel geometric evolution framework: Resistance Curvature Flow (RCF). Leveraging the concept of effective resistance from circuit physics, RCF transforms expensive curvature optimization into efficient matrix operations. This approach achieves over 100x computational acceleration while maintaining geometric optimization capabilities comparable to OCF. We provide an in-depth exploration of the theoretical foundations and dynamical principles of RCF, elucidating how it guides the redistribution of edge weights via curvature gradients to eliminate topological noise and strengthen local cluster structures. Furthermore, we provide a mechanistic explanation of RCF's role in manifold enhancement and noise suppression, as well as its compatibility with deep learning models. We design a graph optimization algorithm, DGSL-RCF, based on this framework. Experimental results across deep metric learning, manifold learning, and graph structure learning demonstrate that DGSL-RCF significantly improves representation quality and downstream task performance.

</details>


### [25] [TabPFN Through The Looking Glass: An interpretability study of TabPFN and its internal representations](https://arxiv.org/abs/2601.08181)
*Aviral Gupta,Armaan Sethi,Dhruv Kumar*

Main category: cs.LG

TL;DR: Tabular foundational models encode meaningful, structured information in their hidden representations; probing reveals signals of both intermediate and final quantities across layers, shedding light on the model's internal computations and refinement process.


<details>
  <summary>Details</summary>
Motivation: To understand the interpretability of tabular foundational models by examining what information their hidden representations store and how it evolves through layers.

Method: Conduct a set of probing experiments that test for the presence of linear regression coefficients, intermediate values from complex expressions, and the final output in early layers to infer the computations performed inside the model.

Result: Evidence that hidden representations contain concrete, interpretable information corresponding to intermediate and final quantities involved in the prediction, indicating that information is refined across layers.

Conclusion: These findings advance understanding of the internal mechanics of tabular foundational models and support efforts toward more transparent and trustworthy decision processes.

Abstract: Tabular foundational models are pre-trained models designed for a wide range of tabular data tasks. They have shown strong performance across domains, yet their internal representations and learned concepts remain poorly understood. This lack of interpretability makes it important to study how these models process and transform input features. In this work, we analyze the information encoded inside the model's hidden representations and examine how these representations evolve across layers. We run a set of probing experiments that test for the presence of linear regression coefficients, intermediate values from complex expressions, and the final answer in early layers. These experiments allow us to reason about the computations the model performs internally. Our results provide evidence that meaningful and structured information is stored inside the representations of tabular foundational models. We observe clear signals that correspond to both intermediate and final quantities involved in the model's prediction process. This gives insight into how the model refines its inputs and how the final output emerges. Our findings contribute to a deeper understanding of the internal mechanics of tabular foundational models. They show that these models encode concrete and interpretable information, which moves us closer to making their decision processes more transparent and trustworthy.

</details>


### [26] [Scalable Multiagent Reinforcement Learning with Collective Influence Estimation](https://arxiv.org/abs/2601.08210)
*Zhenglong Luo,Zhiyong Chen,Aoxiang Liu,Ke Pan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multiagent reinforcement learning (MARL) has attracted considerable attention due to its potential in addressing complex cooperative tasks. However, existing MARL approaches often rely on frequent exchanges of action or state information among agents to achieve effective coordination, which is difficult to satisfy in practical robotic systems. A common solution is to introduce estimator networks to model the behaviors of other agents and predict their actions; nevertheless, such designs cause the size and computational cost of the estimator networks to grow rapidly with the number of agents, thereby limiting scalability in large-scale systems.
  To address these challenges, this paper proposes a multiagent learning framework augmented with a Collective Influence Estimation Network (CIEN). By explicitly modeling the collective influence of other agents on the task object, each agent can infer critical interaction information solely from its local observations and the task object's states, enabling efficient collaboration without explicit action information exchange. The proposed framework effectively avoids network expansion as the team size increases; moreover, new agents can be incorporated without modifying the network structures of existing agents, demonstrating strong scalability. Experimental results on multiagent cooperative tasks based on the Soft Actor-Critic (SAC) algorithm show that the proposed method achieves stable and efficient coordination under communication-limited environments. Furthermore, policies trained with collective influence modeling are deployed on a real robotic platform, where experimental results indicate significantly improved robustness and deployment feasibility, along with reduced dependence on communication infrastructure.

</details>


### [27] [GADPN: Graph Adaptive Denoising and Perturbation Networks via Singular Value Decomposition](https://arxiv.org/abs/2601.08230)
*Hao Deng,Bo Liu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Graph Neural Networks (GNNs) excel on graph-structured data, their performance is fundamentally limited by the quality of the observed graph, which often contains noise, missing links, or structural properties misaligned with GNNs' underlying assumptions. To address this, graph structure learning aims to infer a more optimal topology. Existing methods, however, often incur high computational costs due to complex generative models and iterative joint optimization, limiting their practical utility. In this paper, we propose GADPN, a simple yet effective graph structure learning framework that adaptively refines graph topology via low-rank denoising and generalized structural perturbation. Our approach makes two key contributions: (1) we introduce Bayesian optimization to adaptively determine the optimal denoising strength, tailoring the process to each graph's homophily level; and (2) we extend the structural perturbation method to arbitrary graphs via Singular Value Decomposition (SVD), overcoming its original limitation to symmetric structures. Extensive experiments on benchmark datasets demonstrate that GADPN achieves state-of-the-art performance while significantly improving efficiency. It shows particularly strong gains on challenging disassortative graphs, validating its ability to robustly learn enhanced graph structures across diverse network types.

</details>


### [28] [Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making](https://arxiv.org/abs/2601.08247)
*Liu He*

Main category: cs.LG

TL;DR: Introducing cognitive biases into reinforcement learning for financial trading yields inconclusive or negative results, highlighting challenges in modeling human-like bias and its impact on performance.


<details>
  <summary>Details</summary>
Motivation: Traditional RL assumes rational agents in finance; incorporating biases aims to produce more realistic trading behavior and potentially improve risk-adjusted returns.

Method: Embed biases (e.g., overconfidence and loss aversion) into reward signals and decision-making processes; evaluate in simulated and real-world trading environments.

Result: Results were inconclusive or negative; the study provides practical insights and cautionary lessons for building robust financial AI systems.

Conclusion: Modeling human biases in reinforcement learning is challenging and yields mixed outcomes; further work is needed to design robust methods and understand when biases may be beneficial.

Abstract: Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.

</details>


### [29] [Hyperbolic Heterogeneous Graph Transformer](https://arxiv.org/abs/2601.08251)
*Jongmin Park,Seunghoon Han,Hyewon Lee,Won-Yong Shin,Sungsu Lim*

Main category: cs.LG

TL;DR: 提出在双曲空间中学习异构图的新框架HypHGT，通过 transformer 架构和关系特定的双曲注意力，在保持异构信息的同时实现线性复杂度，能够同时捕捉局部与全局依赖，并在节点分类任务上超越SOTA，且训练时间和内存显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有的基于双曲的异构图GNN多依赖切向空间操作，易产生映射畸变；大多模型仍以局部邻域为主，难以捕捉全局层级结构和跨类型的长程依赖，需要一个能在整个双曲空间内高效建模异构结构的框架。

Method: 提出HypHGT，在双曲空间内完全学习异构图表示，利用 transformer 架构同时建模局部和全局依赖；提出关系特异的双曲注意力机制，具有线性时间复杂度，确保高效计算并保留跨关系的信息。该设计实现对异构图的复杂结构属性与语义信息的有效捕获。

Result: 在节点分类任务上，HypHGT 的表现持续优于现有最先进方法，且显著降低训练时间和内存使用。

Conclusion: 该方法有效捕获异构图的复杂结构与语义信息，能够在双曲空间中高效学习表示，兼顾局部与全局关系。

Abstract: In heterogeneous graphs, we can observe complex structures such as tree-like or hierarchical structures. Recently, the hyperbolic space has been widely adopted in many studies to effectively learn these complex structures. Although these methods have demonstrated the advantages of the hyperbolic space in learning heterogeneous graphs, most existing methods still have several challenges. They rely heavily on tangent-space operations, which often lead to mapping distortions during frequent transitions. Moreover, their message-passing architectures mainly focus on local neighborhood information, making it difficult to capture global hierarchical structures and long-range dependencies between different types of nodes. To address these limitations, we propose Hyperbolic Heterogeneous Graph Transformer (HypHGT), which effectively and efficiently learns heterogeneous graph representations entirely within the hyperbolic space. Unlike previous message-passing based hyperbolic heterogeneous GNNs, HypHGT naturally captures both local and global dependencies through transformer-based architecture. Furthermore, the proposed relation-specific hyperbolic attention mechanism in HypHGT, which operates with linear time complexity, enables efficient computation while preserving the heterogeneous information across different relation types. This design allows HypHGT to effectively capture the complex structural properties and semantic information inherent in heterogeneous graphs. We conduct comprehensive experiments to evaluate the effectiveness and efficiency of HypHGT, and the results demonstrate that it consistently outperforms state-of-the-art methods in node classification task, with significantly reduced training time and memory usage.

</details>


### [30] [On Evaluation of Unsupervised Feature Selection for Pattern Classification](https://arxiv.org/abs/2601.08257)
*Gyu-Il Kim,Dae-Won Kim,Jaesung Lee*

Main category: cs.LG

TL;DR: Using a multi-label classification framework to evaluate unsupervised feature selection reveals that single-label based rankings are biased; on 21 datasets, method rankings differ from traditional single-label results, suggesting multi-label evaluation for fair comparisons.


<details>
  <summary>Details</summary>
Motivation: Single-label evaluation on features chosen from multi-label data can be arbitrary depending on which label is selected, leading to inconsistent and potentially biased method rankings.

Method: Assess representative unsupervised feature selection methods on 21 multi-label datasets under a multi-label classification setting, and compare the resulting rankings to those obtained under single-label evaluation.

Result: Rankings of methods differ markedly between multi-label and single-label evaluations; multi-label evaluation can change which methods appear superior, indicating that single-label accuracy is not a reliable discriminative measure in unsupervised feature selection.

Conclusion: Adopt multi-label evaluation frameworks to fairly and reliably compare unsupervised feature selection methods in multi-label data contexts.

Abstract: Unsupervised feature selection aims to identify a compact subset of features that captures the intrinsic structure of data without supervised label. Most existing studies evaluate the performance of methods using the single-label dataset that can be instantiated by selecting a label from multi-label data while maintaining the original features. Because the chosen label can vary arbitrarily depending on the experimental setting, the superiority among compared methods can be changed with regard to which label happens to be selected. Thus, evaluating unsupervised feature selection methods based solely on single-label accuracy is unreasonable for assessing their true discriminative ability. This study revisits this evaluation paradigm by adopting a multi-label classification framework. Experiments on 21 multi-label datasets using several representative methods demonstrate that performance rankings differ markedly from those reported under single-label settings, suggesting the possibility of multi-label evaluation settings for fair and reliable comparison of unsupervised feature selection methods.

</details>


### [31] [Demystifying the Slash Pattern in Attention: The Role of RoPE](https://arxiv.org/abs/2601.08297)
*Yuan Cheng,Fengzhuo Zhang,Yunlong Hou,Cunxiao Du,Chao Du,Tianyu Pang,Aixin Sun,Zhuoran Yang*

Main category: cs.LG

TL;DR: 论文揭示大语言模型中 Slash-Dominant Heads (SDHs) 的出现机制：在 Queries/Keys 近似 rank-one 且 RoPE 以中高频为主时，会产生沿 Δ 子对角线的注意力模式，理论与实验均证明其可在梯度下降训练中出现并能泛化到分布外提示。


<details>
  <summary>Details</summary>
Motivation: 理解注意力矩阵中跨 token 信息传递的根本原因，揭示 SDHs 的内在性、鲁棒性及对训练与提示阶段的影响，从而提升对大模型注意力结构的理解与潜在改进。

Method: 1) 对开源 LLMs 进行实证分析，观察 SDHs 的普遍性与跨分布鲁棒性；2) 分析 Queries、Keys 与 RoPE 的相互作用，提出两条关键条件；3) 建立简化 Transformer + RoPE 的理论模型，在训练动力学下证明满足条件时会出现 SDHs；4) 讨论 SDHs 的泛化性。

Result: 发现两条特征条件：1) Queries 与 Keys 近似 rank-one；2) RoPE 的频谱以中高频分量为主。这些条件使得 token 之间的交互主要由 RoPE 的中高频成分驱动，导致 SDHs 的出现。理论分析证明，在这些条件下，使用梯度下降训练的浅层 Transformer 也会产生 SDHs，且 SDHs 能泛化到 out-of-distribution 的提示。

Conclusion: SDHs 是模型内在特性的一部分，能跨任务与分布保持稳定，论文通过实验与理论并行给出充分条件及机制，深化对注意力模式的理解，并为进一步研究对齐、鲁棒性与解释性提供线索。

Abstract: Large Language Models (LLMs) often exhibit slash attention patterns, where attention scores concentrate along the $Δ$-th sub-diagonal for some offset $Δ$. These patterns play a key role in passing information across tokens. But why do they emerge? In this paper, we demystify the emergence of these Slash-Dominant Heads (SDHs) from both empirical and theoretical perspectives. First, by analyzing open-source LLMs, we find that SDHs are intrinsic to models and generalize to out-of-distribution prompts. To explain the intrinsic emergence, we analyze the queries, keys, and Rotary Position Embedding (RoPE), which jointly determine attention scores. Our empirical analysis reveals two characteristic conditions of SDHs: (1) Queries and keys are almost rank-one, and (2) RoPE is dominated by medium- and high-frequency components. Under these conditions, queries and keys are nearly identical across tokens, and interactions between medium- and high-frequency components of RoPE give rise to SDHs. Beyond empirical evidence, we theoretically show that these conditions are sufficient to ensure the emergence of SDHs by formalizing them as our modeling assumptions. Particularly, we analyze the training dynamics of a shallow Transformer equipped with RoPE under these conditions, and prove that models trained via gradient descent exhibit SDHs. The SDHs generalize to out-of-distribution prompts.

</details>


### [32] [ORBIT: On-policy Exploration-Exploitation for Controllable Multi-Budget Reasoning](https://arxiv.org/abs/2601.08310)
*Kun Liang,Clive Bai,Xin Xu,Chenming Tang,Sanwoo Lee,Weijie Liu,Saiyong Yang,Yunfang Wu*

Main category: cs.LG

TL;DR: ORBIT 通过多预算可控推理框架实现 LRMs 的可调理性，使用多阶段强化学习发现每个预算下的帕累托最优推理策略，并通过 on-policy 蒸馏将其融合为一个统一模型，实现多模态的可控推理行为、每模态下的竞争性密度，以及在保持明确模态分离的同时将 frontier 政策整合进单一学生模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言推理模型偏向长形式链式推理以提升准确性，但推理成本高且在不同场景下并非都需要。现有按输入自适应预算的方法在 worst-case 上不可靠，且在训练时固定了成本-准确性权衡，缺乏部署灵活性与多预算间的可切换能力。需要一个能在多预算之间提供清晰模态分离、并在单一模型内同时保留各模态性能的框架。

Method: 提出 ORBIT：通过多阶段强化学习在每个预算级别上发现帕累托最优的推理行为；随后使用基于策略的蒸馏（on-policy distillation）将这些行为融入一个统一的学生模型，同时确保不同预算模态之间保持清晰的分离。

Result: 实验表明：1) 实现跨多种模式的可控推理行为；2) 各模式内具有竞争性且密度可观的推理表现；3) 将 frontier 策略整合到单一学生模型，同时保持显著的模态分离和各模态的高性能。

Conclusion: ORBIT 提供一种灵活、可扩展的多预算推理框架，通过跨预算的前沿策略与统一模型的蒸馏，兼顾可控性、性能密度和部署灵活性。

Abstract: Recent Large Reasoning Models (LRMs) achieve strong performance by leveraging long-form Chain-of-Thought (CoT) reasoning, but uniformly applying overlong reasoning at inference time incurs substantial and often unnecessary computational cost. To address this, prior work explores various strategies to infer an appropriate reasoning budget from the input. However, such approaches are unreliable in the worst case, as estimating the minimal required reasoning effort is fundamentally difficult, and they implicitly fix the trade-off between reasoning cost and accuracy during training, limiting flexibility under varying deployment scenarios. Motivated by these limitations, we propose ORBIT, a controllable multi-budget reasoning framework with well-separated reasoning modes triggered by input. ORBIT employs multi-stage reinforcement learning to discover Pareto-optimal reasoning behaviors at each effort, followed by on-policy distillation to fuse these behaviors into a single unified model. Experiments show that ORBIT achieves (1) controllable reasoning behavior over multiple modes, (2) competitive reasoning density within each mode, and (3) integration of these frontier policies into a single unified student model while preserving clear mode separation and high per-mode performance.

</details>


### [33] [Automated Machine Learning in Radiomics: A Comparative Evaluation of Performance, Efficiency and Accessibility](https://arxiv.org/abs/2601.08334)
*Jose Lozano-Montoya,Emilio Soria-Olivas,Almudena Fuster-Matanzo,Angel Alberich-Bayarri,Ana Jimenez-Pastor*

Main category: cs.LG

TL;DR: AutoML 可以降低 radiomics 模型开发门槛，但通用框架在可及性方面更好，专用框架在性能方面有优势但存在可用性问题。Simplatab表现最佳，LightAutoML速度最快，但多数 radiomics 专用框架因过时或实现复杂而被排除在外。


<details>
  <summary>Details</summary>
Motivation: 评估不同 AutoML 框架在 radiomics 分类任务中的性能、效率和可访问性，以揭示当前工具的差距和未来改进方向。

Method: 在十个公开/私有 radiomics 数据集上，比较六个通用框架和五个 radiomics 专用框架。对参数设定统一，使用标准交叉验证，评估 AUC、运行时间，以及软件状态、可访问性与可解释性等定性指标。

Result: Simplatab 为 radiomics 专用工具，平均测试 AUC 81.81%，运行时间 ~1 小时；LightAutoML 作为通用框架，最快，六分钟，平均 AUC 78.74%；多数 radiomics 专用框架因过时、需要大量编程或计算效率低而未纳入性能分析；通用框架在可访问性与易实现性上表现更好。

Conclusion: Simplatab 在性能、效率和可访问性之间取得较好平衡，但仍存在生存分析支持不足、特征可重复性和归一化/集成的局限等问题。未来研究应致力于将 AutoML 更好地适配 radiomics 的特定挑战。

Abstract: Automated machine learning (AutoML) frameworks can lower technical barriers for predictive and prognostic model development in radiomics by enabling researchers without programming expertise to build models. However, their effectiveness in addressing radiomics-specific challenges remains unclear. This study evaluates the performance, efficiency, and accessibility of general-purpose and radiomics-specific AutoML frameworks on diverse radiomics classification tasks, thereby highlighting development needs for radiomics. Ten public/private radiomics datasets with varied imaging modalities (CT/MRI), sizes, anatomies and endpoints were used. Six general-purpose and five radiomics-specific frameworks were tested with predefined parameters using standardized cross-validation. Evaluation metrics included AUC, runtime, together with qualitative aspects related to software status, accessibility, and interpretability. Simplatab, a radiomics-specific tool with a no-code interface, achieved the highest average test AUC (81.81%) with a moderate runtime (~1 hour). LightAutoML, a general-purpose framework, showed the fastest execution with competitive performance (78.74% mean AUC in six minutes). Most radiomics-specific frameworks were excluded from the performance analysis due to obsolescence, extensive programming requirements, or computational inefficiency. Conversely, general-purpose frameworks demonstrated higher accessibility and ease of implementation. Simplatab provides an effective balance of performance, efficiency, and accessibility for radiomics classification problems. However, significant gaps remain, including the lack of accessible survival analysis support and the limited integration of feature reproducibility and harmonization within current AutoML frameworks. Future research should focus on adapting AutoML solutions to better address these radiomics-specific challenges.

</details>


### [34] [Decodable but not structured: linear probing enables Underwater Acoustic Target Recognition with pretrained audio embeddings](https://arxiv.org/abs/2601.08358)
*Hilde I. Hummel,Sandjai Bhulai,Rob D. van der Mei,Burooj Ghani*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Increasing levels of anthropogenic noise from ships contribute significantly to underwater sound pollution, posing risks to marine ecosystems. This makes monitoring crucial to understand and quantify the impact of the ship radiated noise. Passive Acoustic Monitoring (PAM) systems are widely deployed for this purpose, generating years of underwater recordings across diverse soundscapes. Manual analysis of such large-scale data is impractical, motivating the need for automated approaches based on machine learning. Recent advances in automatic Underwater Acoustic Target Recognition (UATR) have largely relied on supervised learning, which is constrained by the scarcity of labeled data. Transfer Learning (TL) offers a promising alternative to mitigate this limitation. In this work, we conduct the first empirical comparative study of transfer learning for UATR, evaluating multiple pretrained audio models originating from diverse audio domains. The pretrained model weights are frozen, and the resulting embeddings are analyzed through classification, clustering, and similarity-based evaluations. The analysis shows that the geometrical structure of the embedding space is largely dominated by recording-specific characteristics. However, a simple linear probe can effectively suppress this recording-specific information and isolate ship-type features from these embeddings. As a result, linear probing enables effective automatic UATR using pretrained audio models at low computational cost, significantly reducing the need for a large amounts of high-quality labeled ship recordings.

</details>


### [35] [Controlled LLM Training on Spectral Sphere](https://arxiv.org/abs/2601.08393)
*Tian Xie,Haoming Luo,Haoyu Tang,Yiwen Hu,Jason Klein Liu,Qingnan Ren,Yang Wang,Wayne Xin Zhao,Rui Yan,Bing Su,Chong Luo,Baining Guo*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scaling large models requires optimization strategies that ensure rapid convergence grounded in stability. Maximal Update Parametrization ($\boldsymbolμ$P) provides a theoretical safeguard for width-invariant $Θ(1)$ activation control, whereas emerging optimizers like Muon are only ``half-aligned'' with these constraints: they control updates but allow weights to drift. To address this limitation, we introduce the \textbf{Spectral Sphere Optimizer (SSO)}, which enforces strict module-wise spectral constraints on both weights and their updates. By deriving the steepest descent direction on the spectral sphere, SSO realizes a fully $\boldsymbolμ$P-aligned optimization process. To enable large-scale training, we implement SSO as an efficient parallel algorithm within Megatron. Through extensive pretraining on diverse architectures, including Dense 1.7B, MoE 8B-A1B, and 200-layer DeepNet models, SSO consistently outperforms AdamW and Muon. Furthermore, we observe significant practical stability benefits, including improved MoE router load balancing, suppressed outliers, and strictly bounded activations.

</details>


### [36] [Out-of-distribution generalization of deep-learning surrogates for 2D PDE-generated dynamics in the small-data regime](https://arxiv.org/abs/2601.08404)
*Binh Duong Nguyen,Stefan Sandfeld*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Partial differential equations (PDEs) are a central tool for modeling the dynamics of physical, engineering, and materials systems, but high-fidelity simulations are often computationally expensive. At the same time, many scientific applications can be viewed as the evolution of spatially distributed fields, making data-driven forecasting of such fields a core task in scientific machine learning. In this work we study autoregressive deep-learning surrogates for two-dimensional PDE dynamics on periodic domains, focusing on generalization to out-of-distribution initial conditions within a fixed PDE and parameter regime and on strict small-data settings with at most $\mathcal{O}(10^2)$ simulated trajectories per system. We introduce a multi-channel U-Net [...], evaluate it on five qualitatively different PDE families and compare it to ViT, AFNO, PDE-Transformer, and KAN-UNet under a common training setup. Across all datasets, me-UNet matches or outperforms these more complex architectures in terms of field-space error, spectral similarity, and physics-based metrics for in-distribution rollouts, while requiring substantially less training time. It also generalizes qualitatively to unseen initial conditions with as few as $\approx 20$ training simulations. A data-efficiency study and Grad-CAM analysis further suggest that, in small-data periodic 2D PDE settings, convolutional architectures with inductive biases aligned to locality and periodic boundary conditions remain strong contenders for accurate and moderately out-of-distribution-robust surrogate modeling.

</details>


### [37] [Taxon: Hierarchical Tax Code Prediction with Semantically Aligned LLM Expert Guidance](https://arxiv.org/abs/2601.08418)
*Jihang Li,Qing Liu,Zulong Chen,Jing Wang,Wei Wang,Chuanfei Xu,Zeyi Wen*

Main category: cs.LG

TL;DR: 提出 Taxon，一种用于分层税目预测的语义对齐、专家引导框架，结合特征门控混合专家、来自大语言模型的语义一致性蒸馏，以及多源监督，达到在公开和专有数据集上的SOTA，并在阿里巴巴生产环境中实现高并发部署。


<details>
  <summary>Details</summary>
Motivation: 税目在大规模电商场景的发票结算和合规管理中至关重要，需将商品精准映射到多级国家标准税目层级，错误会带来财政与合规风险，因此需要高准确性、可解释性和鲁棒性。

Method: 核心方法包括：1) 特征门控的混合专家架构，按税目层级自适应路由多模态特征；2) 以大语言模型为领域专家的语义一致性模型进行对齐验证；3) 以多源数据（税务数据库、发票校验日志、商家注册数据）进行结构与语义双重监督的训练流水线；4) 全层级路径重建过程提升结构一致性；

Result: 在专有 TaxCode 数据集和公开基准上实现SOTA，显著优于强基线；全层级路径重建提升结构一致性，获得最高F1；在阿里巴巴税务系统部署，日均处理>50万税码查询，峰值>500万，表现为准确性、可解释性、鲁棒性的提升。

Conclusion: Taxon 展示了在实际电商合规领域对分层税目预测的有效性，具备良好扩展性和鲁棒性，能显著提升发票报税的自动化水平与合规性。

Abstract: Tax code prediction is a crucial yet underexplored task in automating invoicing and compliance management for large-scale e-commerce platforms. Each product must be accurately mapped to a node within a multi-level taxonomic hierarchy defined by national standards, where errors lead to financial inconsistencies and regulatory risks. This paper presents Taxon, a semantically aligned and expert-guided framework for hierarchical tax code prediction. Taxon integrates (i) a feature-gating mixture-of-experts architecture that adaptively routes multi-modal features across taxonomy levels, and (ii) a semantic consistency model distilled from large language models acting as domain experts to verify alignment between product titles and official tax definitions. To address noisy supervision in real business records, we design a multi-source training pipeline that combines curated tax databases, invoice validation logs, and merchant registration data to provide both structural and semantic supervision. Extensive experiments on the proprietary TaxCode dataset and public benchmarks demonstrate that Taxon achieves state-of-the-art performance, outperforming strong baselines. Further, an additional full hierarchical paths reconstruction procedure significantly improves structural consistency, yielding the highest overall F1 scores. Taxon has been deployed in production within Alibaba's tax service system, handling an average of over 500,000 tax code queries per day and reaching peak volumes above five million requests during business event with improved accuracy, interpretability, and robustness.

</details>


### [38] [Coverage Improvement and Fast Convergence of On-policy Preference Learning](https://arxiv.org/abs/2601.08421)
*Juno Kim,Jihun Yun,Jason D. Lee,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Online on-policy preference learning algorithms for language model alignment such as online direct policy optimization (DPO) can significantly outperform their offline counterparts. We provide a theoretical explanation for this phenomenon by analyzing how the sampling policy's coverage evolves throughout on-policy training. We propose and rigorously justify the \emph{coverage improvement principle}: with sufficient batch size, each update moves into a region around the target where coverage is uniformly better, making subsequent data increasingly informative and enabling rapid convergence. In the contextual bandit setting with Bradley-Terry preferences and linear softmax policy class, we show that on-policy DPO converges exponentially in the number of iterations for batch size exceeding a generalized coverage threshold. In contrast, any learner restricted to offline samples from the initial policy suffers a slower minimax rate, leading to a sharp separation in total sample complexity. Motivated by this analysis, we further propose a simple hybrid sampler based on a novel \emph{preferential} G-optimal design, which removes dependence on coverage and guarantees convergence in just two rounds. Finally, we develop principled on-policy schemes for reward distillation in the general function class setting, and show faster noiseless rates under an alternative deviation-based notion of coverage. Experimentally, we confirm that on-policy DPO and our proposed reward distillation algorithms outperform their off-policy counterparts and enjoy stable, monotonic performance gains across iterations.

</details>


### [39] [DiffMM: Efficient Method for Accurate Noisy and Sparse Trajectory Map Matching via One Step Diffusion](https://arxiv.org/abs/2601.08482)
*Chenxu Han,Sean Bin Yang,Jilin Hu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Map matching for sparse trajectories is a fundamental problem for many trajectory-based applications, e.g., traffic scheduling and traffic flow analysis. Existing methods for map matching are generally based on Hidden Markov Model (HMM) or encoder-decoder framework. However, these methods continue to face significant challenges when handling noisy or sparsely sampled GPS trajectories. To address these limitations, we propose DiffMM, an encoder-diffusion-based map matching framework that produces effective yet efficient matching results through a one-step diffusion process. We first introduce a road segment-aware trajectory encoder that jointly embeds the input trajectory and its surrounding candidate road segments into a shared latent space through an attention mechanism. Next, we propose a one step diffusion method to realize map matching through a shortcut model by leveraging the joint embedding of the trajectory and candidate road segments as conditioning context. We conduct extensive experiments on large-scale trajectory datasets, demonstrating that our approach consistently outperforms state-of-the-art map matching methods in terms of both accuracy and efficiency, particularly for sparse trajectories and complex road network topologies.

</details>


### [40] [Temporal Fusion Nexus: A task-agnostic multi-modal embedding model for clinical narratives and irregular time series in post-kidney transplant care](https://arxiv.org/abs/2601.08503)
*Aditya Kumar,Simon Rauch,Mario Cypko,Marcel Naik,Matthieu-P Schapranow,Aadil Rashid,Fabian Halleck,Bilgin Osmanodja,Roland Roller,Lars Pape,Klemens Budde,Mario Schiffer,Oliver Amft*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce Temporal Fusion Nexus (TFN), a multi-modal and task-agnostic embedding model to integrate irregular time series and unstructured clinical narratives. We analysed TFN in post-kidney transplant (KTx) care, with a retrospective cohort of 3382 patients, on three key outcomes: graft loss, graft rejection, and mortality. Compared to state-of-the-art model in post KTx care, TFN achieved higher performance for graft loss (AUC 0.96 vs. 0.94) and graft rejection (AUC 0.84 vs. 0.74). In mortality prediction, TFN yielded an AUC of 0.86. TFN outperformed unimodal baselines (approx 10% AUC improvement over time series only baseline, approx 5% AUC improvement over time series with static patient data). Integrating clinical text improved performance across all tasks. Disentanglement metrics confirmed robust and interpretable latent factors in the embedding space, and SHAP-based attributions confirmed alignment with clinical reasoning. TFN has potential application in clinical tasks beyond KTx, where heterogeneous data sources, irregular longitudinal data, and rich narrative documentation are available.

</details>


### [41] [Your Group-Relative Advantage Is Biased](https://arxiv.org/abs/2601.08521)
*Fengkai Yang,Zherui Chen,Xiaohan Wang,Xiaodong Lu,Jiajun Chai,Guojun Yin,Wei Lin,Shuai Ma,Fuzhen Zhuang,Deqing Wang,Yaodong Yang,Jianxin Li,Yikun Ban*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood.
  In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.

</details>


### [42] [Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures](https://arxiv.org/abs/2601.08549)
*Sucheta Ghosh,Zahra Monfared,Felix Dietrich*

Main category: cs.LG

TL;DR: 提出一个两阶段的多任务学习框架用于EEG分析：阶段1通过去噪自编码器抑制伪影并稳定时序；阶段2在去噪信号上执行三任务学习：运动想象分类、基于Lyapunov指数的混沌与非混沌判别，以及自监督对比学习（NT-Xent）。使用卷积骨干+Transformer编码器捕捉时空结构， Dynamical 任务促使模型对非线性脑动力学敏感。分阶段设计降低重建与判别目标之间的干扰，提升跨数据集的稳定性与可重复性，并实现优于强基线和SOTA的EEG解码性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号受伪影影响且具有复杂非线性动力学。单一任务往往导致干扰、泛化差。通过分阶段、分任务整合去噪、动力学建模与自监督表示学习，可提升鲁棒性、可重复性与跨数据集泛化能力；此外引入Lyapunov指数标签以注入动力学信息，结合对比学习提升表征质量。

Method: 阶段1：训练去噪自编码器以抑制伪影、稳定时序。阶段2：在去噪信号上进行三任务学习：1) 运动想象分类；2) 基于Lyapunov指数的混沌/非混沌判别；3) 自监督对比学习（NT-Xent）。骨干采用卷积层+Transformer编码器，捕捉空间-时间结构，动力学任务提高对非线性脑动力学的敏感性。训练设计将重建与判别目标分离以提升稳定性和可重复性。

Result: 实证研究显示该框架在鲁棒性、泛化和EEG解码任务上优于强基线与近来SOTA，证实去噪、动力学特征与自监督学习的组合有效。

Conclusion: 分阶段整合去噪、动力学特征与自监督学习可实现更稳健、泛化能力更强的EEG表征；分离的训练流程提升可重复性，并在跨数据集上表现出良好稳健性。

Abstract: We introduce a two-stage multitask learning framework for analyzing Electroencephalography (EEG) signals that integrates denoising, dynamical modeling, and representation learning. In the first stage, a denoising autoencoder is trained to suppress artifacts and stabilize temporal dynamics, providing robust signal representations. In the second stage, a multitask architecture processes these denoised signals to achieve three objectives: motor imagery classification, chaotic versus non-chaotic regime discrimination using Lyapunov exponent-based labels, and self-supervised contrastive representation learning with NT-Xent loss. A convolutional backbone combined with a Transformer encoder captures spatial-temporal structure, while the dynamical task encourages sensitivity to nonlinear brain dynamics. This staged design mitigates interference between reconstruction and discriminative goals, improves stability across datasets, and supports reproducible training by clearly separating noise reduction from higher-level feature learning. Empirical studies show that our framework not only enhances robustness and generalization but also surpasses strong baselines and recent state-of-the-art methods in EEG decoding, highlighting the effectiveness of combining denoising, dynamical features, and self-supervised learning.

</details>


### [43] [EviNAM: Intelligibility and Uncertainty via Evidential Neural Additive Models](https://arxiv.org/abs/2601.08556)
*Sören Schleibaum,Anton Frederik Thielmann,Julian Teusch,Benjamin Säfken,Jörg P. Müller*

Main category: cs.LG

TL;DR: EviNAM 将 evidential learning 与 Neural Additive Models (NAMs) 结合，单次前向即可同时估计 aleatoric 与 epistemic 不确定性以及各特征的贡献，提升可解释性与预测性能，并可扩展到分类和广义加性模型（GAMs）。


<details>
  <summary>Details</summary>
Motivation: 在需要可靠决策的场景中，既要准确的不确定性估计，又要对预测过程具备可解释性。NAMs 提供逐特征的透明性，证据学习提供不确定性估计，但现有方法往往要么牺牲可解释性，要么代价较高。将两者结合可以在单遍前向中实现不确定性与特征贡献的联合推断。

Method: 在 evidential learning 框架下，将其扩展到可处理 NAM 的加性结构，输出每个输入特征的贡献，以及同时估计 aleatoric 与 epistemic 不确定性的一体化量化；实现单 pass 推断，且方法具备回归的核心能力，并可自然扩展到分类任务和广义加性模型（GAMs）。

Result: 在合成数据与真实数据上，EviNAM 的预测性能达到与最先进方法相当的水平；同时提供明确的特征贡献信息，提升模型的可解释性；方法聚焦回归问题，但具备向分类和 GAMs 的扩展潜力。

Conclusion: 该工作提供一种将可解释性与可信不确定性结合的新途径，使预测更易于理解且更可信，并为未来在分类与 GAM 领域的扩展奠定基础。

Abstract: Intelligibility and accurate uncertainty estimation are crucial for reliable decision-making. In this paper, we propose EviNAM, an extension of evidential learning that integrates the interpretability of Neural Additive Models (NAMs) with principled uncertainty estimation. Unlike standard Bayesian neural networks and previous evidential methods, EviNAM enables, in a single pass, both the estimation of the aleatoric and epistemic uncertainty as well as explicit feature contributions. Experiments on synthetic and real data demonstrate that EviNAM matches state-of-the-art predictive performance. While we focus on regression, our method extends naturally to classification and generalized additive models, offering a path toward more intelligible and trustworthy predictions.

</details>


### [44] [M$^2$FMoE: Multi-Resolution Multi-View Frequency Mixture-of-Experts for Extreme-Adaptive Time Series Forecasting](https://arxiv.org/abs/2601.08631)
*Yaohui Huang,Runmin Zou,Yun Wang,Laeeq Aslam,Ruipeng Dong*

Main category: cs.LG

TL;DR: M^2FMoE 提出一个极端自适应的时间序列预测模型，通过在 Fourier 与 Wavelet 两个域中进行多视角的频谱门控专家分配，以及多分辨率特征融合和时序门控，来同时建模常规模式与极端事件，从而在不依赖极端事件标签的情况下显著提升预测性能，适用于含极端模式的水文数据集等应用。


<details>
  <summary>Details</summary>
Motivation: 现实世界的极端事件具有高方差、非平滑的动态和稀疏但高影响的特征。现有方法在拟合常规模式方面表现良好，却在极端事件发生时性能显著下降，因此需要一种能同时捕捉正常与极端时间模式的模型。

Method: 提出三大模块：1) 多视角频率混合专家（M^2FMoE）在 Fourier 与 Wavelet 域中将专家分配到不同谱带，并通过跨视角的共享带分割实现频率分区的一致性与专家间协作，以捕捉主导与罕见波动；2) 多分辨率自适应融合模块，分层地从粗到细聚合频率特征，提升对短期变化与突发变化的敏感性；3) 时序门控整合模块，动态平衡长期趋势与短期频率特征，提升对常规与极端时间模式的适应性。

Result: 在具有极端模式的真实水文数据集上，所提方法比最先进的基线具有更优性能，且无需极端事件标签。

Conclusion: 通过在频域上的多视角门控专家和多分辨率融合，以及动态时序门控，M^2FMoE 能同时建模常规与极端时间模式，改善极端事件预测表现，且无需额外标签。

Abstract: Forecasting time series with extreme events is critical yet challenging due to their high variance, irregular dynamics, and sparse but high-impact nature. While existing methods excel in modeling dominant regular patterns, their performance degrades significantly during extreme events, constituting the primary source of forecasting errors in real-world applications. Although some approaches incorporate auxiliary signals to improve performance, they still fail to capture extreme events' complex temporal dynamics. To address these limitations, we propose M$^2$FMoE, an extreme-adaptive forecasting model that learns both regular and extreme patterns through multi-resolution and multi-view frequency modeling. It comprises three modules: (1) a multi-view frequency mixture-of-experts module assigns experts to distinct spectral bands in Fourier and Wavelet domains, with cross-view shared band splitter aligning frequency partitions and enabling inter-expert collaboration to capture both dominant and rare fluctuations; (2) a multi-resolution adaptive fusion module that hierarchically aggregates frequency features from coarse to fine resolutions, enhancing sensitivity to both short-term variations and sudden changes; (3) a temporal gating integration module that dynamically balances long-term trends and short-term frequency-aware features, improving adaptability to both regular and extreme temporal patterns. Experiments on real-world hydrological datasets with extreme patterns demonstrate that M$^2$FMoE outperforms state-of-the-art baselines without requiring extreme-event labels.

</details>


### [45] [Provably Safe Reinforcement Learning using Entropy Regularizer](https://arxiv.org/abs/2601.08646)
*Abhijit Mazumdar,Rafal Wisniewski,Manuela L. Bujorianu*

Main category: cs.LG

TL;DR: 在带有安全约束的 MDP 中，提出基于 OFU 的安全强化学习算法并引入熵正则化的改进版本，给出有限样本时的遗憾界，表明熵正则化能提升遗憾界性能并显著降低学习阶段的波动。


<details>
  <summary>Details</summary>
Motivation: 在学习阶段就需要对安全约束（reach-avoid）提供高概率的保障，这对强化学习算法提出了严格的安全性与稳定性要求。本文通过引入熵正则化，结合乐观性不确定性实现安全、高效的在线学习。

Method: 提出两个算法：基于乐观性不确定性（OFU）的初始安全 RL；在此基础上加入熵正则化形成主算法。对两种算法进行有限样本分析，推导遗憾界，并分析熵正则化对方差与回合间波动的影响。

Result: 给出两种算法的遗憾界，证明熵正则化不仅改善遗憾界，也显著降低学习过程中的 episode-to-episode 变动性，提升安全学习的稳定性。

Conclusion: 在包含 reach-avoid 安全约束的 MDP 中，熵正则化的 OFU 安全 RL 具有更好的遗憾性能与学习稳定性，且能够在学习阶段以高概率保持安全。

Abstract: We consider the problem of learning the optimal policy for Markov decision processes with safety constraints. We formulate the problem in a reach-avoid setup. Our goal is to design online reinforcement learning algorithms that ensure safety constraints with arbitrarily high probability during the learning phase. To this end, we first propose an algorithm based on the optimism in the face of uncertainty (OFU) principle. Based on the first algorithm, we propose our main algorithm, which utilizes entropy regularization. We investigate the finite-sample analysis of both algorithms and derive their regret bounds. We demonstrate that the inclusion of entropy regularization improves the regret and drastically controls the episode-to-episode variability that is inherent in OFU-based safe RL algorithms.

</details>


### [46] [TRACE: Reconstruction-Based Anomaly Detection in Ensemble and Time-Dependent Simulations](https://arxiv.org/abs/2601.08659)
*Hamid Gadirov,Martijn Westra,Steffen Frey*

Main category: cs.LG

TL;DR: 2D帧级自编码器与3D时序自编码器在高维时变仿真数据的异常检测中对比：3D方法利用时空上下文更好地检测动态异常并减少跨时间的冗余检测；重构误差受质量分布影响，集中区域产生更大误差。


<details>
  <summary>Details</summary>
Motivation: 在高维、时间相关的仿真数据中检测异常具有挑战性，因果与时空动态复杂。本文评估卷积自编码器在参数化卡门涡街组数据上的异常检测能力，并探究时间上下文与质量分布对重构误差的作用。

Method: 比较在单帧上工作的2D自编码器与处理短时间序列的3D自编码器；在参数化卡门涡街的集合数据上进行评估；扩展至体积时间相关数据，分析重构误差与质量分布的关系。

Result: 2D模型在单帧层面检测局部空间异常；3D模型利用时空上下文检测异常运动模式并降低跨时间的冗余检测；在体积数据中，重构误差显著受质量分布影响，高度集中的区域产生更大误差；强调时序上下文对鲁棒异常检测的重要性。

Conclusion: 时序上下文对动态仿真中的鲁棒异常检测至关重要；3D卷积自编码器在捕捉动态异常方面优于2D，并能减少时间维度上的重复检测；质量分布对检测灵敏度有显著影响。

Abstract: Detecting anomalies in high-dimensional, time-dependent simulation data is challenging due to complex spatial and temporal dynamics. We study reconstruction-based anomaly detection for ensemble data from parameterized Kármán vortex street simulations using convolutional autoencoders. We compare a 2D autoencoder operating on individual frames with a 3D autoencoder that processes short temporal stacks. The 2D model identifies localized spatial irregularities in single time steps, while the 3D model exploits spatio-temporal context to detect anomalous motion patterns and reduces redundant detections across time. We further evaluate volumetric time-dependent data and find that reconstruction errors are strongly influenced by the spatial distribution of mass, with highly concentrated regions yielding larger errors than dispersed configurations. Our results highlight the importance of temporal context for robust anomaly detection in dynamic simulations.

</details>


### [47] [Soft Partition-based KAPI-ELM for Multi-Scale PDEs](https://arxiv.org/abs/2601.08719)
*Vikas Dwivedi,Monica Sigovan,Bruno Sixou*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Physics-informed machine learning holds great promise for solving differential equations, yet existing methods struggle with highly oscillatory, multiscale, or singularly perturbed PDEs due to spectral bias, costly backpropagation, and manually tuned kernel or Fourier frequencies. This work introduces a soft partition--based Kernel-Adaptive Physics-Informed Extreme Learning Machine (KAPI-ELM), a deterministic low-dimensional parameterization in which smooth partition lengths jointly control collocation centers and Gaussian kernel widths, enabling continuous coarse-to-fine resolution without Fourier features, random sampling, or hard domain interfaces. A signed-distance-based weighting further stabilizes least-squares learning on irregular geometries. Across eight benchmarks--including oscillatory ODEs, high-frequency Poisson equations, irregular-shaped domains, and stiff singularly perturbed convection-diffusion problems-the proposed method matches or exceeds the accuracy of state-of-the-art Physics-Informed Neural Network (PINN) and Theory of Functional Connections (TFC) variants while using only a single linear solve. Although demonstrated on steady linear PDEs, the results show that soft-partition kernel adaptation provides a fast, architecture-free approach for multiscale PDEs with broad potential for future physics-informed modeling. For reproducibility, the reference codes are available at https://github.com/vikas-dwivedi-2022/soft_kapi

</details>


### [48] [Model-Agnostic Solutions for Deep Reinforcement Learning in Non-Ergodic Contexts](https://arxiv.org/abs/2601.08726)
*Bert Verbruggen,Arne Vanhoyweghen,Vincent Ginis*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement Learning (RL) remains a central optimisation framework in machine learning. Although RL agents can converge to optimal solutions, the definition of ``optimality'' depends on the environment's statistical properties. The Bellman equation, central to most RL algorithms, is formulated in terms of expected values of future rewards. However, when ergodicity is broken, long-term outcomes depend on the specific trajectory rather than on the ensemble average. In such settings, the ensemble average diverges from the time-average growth experienced by individual agents, with expected-value formulations yielding systematically suboptimal policies. Prior studies demonstrated that traditional RL architectures fail to recover the true optimum in non-ergodic environments. We extend this analysis to deep RL implementations and show that these, too, produce suboptimal policies under non-ergodic dynamics. Introducing explicit time dependence into the learning process can correct this limitation. By allowing the network's function approximation to incorporate temporal information, the agent can estimate value functions consistent with the process's intrinsic growth rate. This improvement does not require altering the environmental feedback, such as reward transformations or modified objective functions, but arises naturally from the agent's exposure to temporal trajectories. Our results contribute to the growing body of research on reinforcement learning methods for non-ergodic systems.

</details>


### [49] [A Novel Approach to Explainable AI with Quantized Active Ingredients in Decision Making](https://arxiv.org/abs/2601.08733)
*A. M. A. S. D. Alagiyawanna,Asoka Karunananda,Thushari Silva,A. Mahasinghe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Artificial Intelligence (AI) systems have shown good success at classifying. However, the lack of explainability is a true and significant challenge, especially in high-stakes domains, such as health and finance, where understanding is paramount. We propose a new solution to this challenge: an explainable AI framework based on our comparative study with Quantum Boltzmann Machines (QBMs) and Classical Boltzmann Machines (CBMs). We leverage principles of quantum computing within classical machine learning to provide substantive transparency around decision-making. The design involves training both models on a binarised and dimensionally reduced MNIST dataset, where Principal Component Analysis (PCA) is applied for preprocessing. For interpretability, we employ gradient-based saliency maps in QBMs and SHAP (SHapley Additive exPlanations) in CBMs to evaluate feature attributions.QBMs deploy hybrid quantum-classical circuits with strongly entangling layers, allowing for richer latent representations, whereas CBMs serve as a classical baseline that utilises contrastive divergence. Along the way, we found that QBMs outperformed CBMs on classification accuracy (83.5% vs. 54%) and had more concentrated distributions in feature attributions as quantified by entropy (1.27 vs. 1.39). In other words, QBMs not only produced better predictive performance than CBMs, but they also provided clearer identification of "active ingredient" or the most important features behind model predictions. To conclude, our results illustrate that quantum-classical hybrid models can display improvements in both accuracy and interpretability, which leads us toward more trustworthy and explainable AI systems.

</details>


### [50] [Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs](https://arxiv.org/abs/2601.08763)
*Zhiyuan Hu,Yucheng Wang,Yufei He,Jiaying Wu,Yilun Zhao,See-Kiong Ng,Cynthia Breazeal,Anh Tuan Luu,Hae Won Park,Bryan Hooi*

Main category: cs.LG

TL;DR: 提出一种在强化学习微调后的大语言模型中提升解题多样性的唯一性感知方法；通过对同一问题的不同高层解题策略进行聚类，在集群大小的逆权重下重分配策略优势，从而鼓励罕见但正确的高层策略，提升 pass@k 和 AUC@K，同时保持 pass@1。


<details>
  <summary>Details</summary>
Motivation: RL 在 LLM 任务中的探索崩溃问题：过度关注少数主导的推理模式，导致解题多样性下降。现有正则化聚焦局部 token 行为，而非对解集的多样性约束。

Method: 使用一个基于 LLM 的评判者，将同一问题的不同 rollouts 按高层解题策略聚类，忽略表层差异；以集群大小的逆数对策略优势进行重加权，使得罕见但正确的高层策略获得更高奖励。

Result: 在数学、物理、医学推理基准上，显著提升 pass@k 和 AUC@K，并在更大采样预算下保持或提高探索性，未牺牲 pass@1。实现对更丰富的解题策略的发现。

Conclusion: 唯一性感知 RL 提升了解题策略的多样性和探索性，适用于大规模样本下的后训练微调，能在不牺牲单步最优表现的前提下扩大有效解的集合。

Abstract: Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.

</details>


### [51] [Asymptotic Universal Alignment: A New Alignment Framework via Test-Time Scaling](https://arxiv.org/abs/2601.08777)
*Yang Cai,Weiqiang Zheng*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Aligning large language models (LLMs) to serve users with heterogeneous and potentially conflicting preferences is a central challenge for personalized and trustworthy AI. We formalize an ideal notion of universal alignment through test-time scaling: for each prompt, the model produces $k\ge 1$ candidate responses and a user selects their preferred one. We introduce $(k,f(k))$-robust alignment, which requires the $k$-output model to have win rate $f(k)$ against any other single-output model, and asymptotic universal alignment (U-alignment), which requires $f(k)\to 1$ as $k\to\infty$. Our main result characterizes the optimal convergence rate: there exists a family of single-output policies whose $k$-sample product policies achieve U-alignment at rate $f(k)=\frac{k}{k+1}$, and no method can achieve a faster rate in general.
  We show that popular post-training methods, including Nash learning from human feedback (NLHF), can fundamentally underutilize the benefits of test-time scaling. Even though NLHF is optimal for $k=1$, sampling from the resulting (often deterministic) policy cannot guarantee win rates above $\tfrac{1}{2}$ except for an arbitrarily small slack. This stems from a lack of output diversity: existing alignment methods can collapse to a single majority-preferred response, making additional samples redundant. In contrast, our approach preserves output diversity and achieves the optimal test-time scaling rate. In particular, we propose a family of symmetric multi-player alignment games and prove that any symmetric Nash equilibrium policy of the $(k+1)$-player alignment game achieves the optimal $(k,\frac{k}{k+1})$-robust alignment. Finally, we provide theoretical convergence guarantees for self-play learning dynamics in these games and extend the framework to opponents that also generate multiple responses.

</details>


### [52] [Fast and explainable clustering in the Manhattan and Tanimoto distance](https://arxiv.org/abs/2601.08781)
*Stefan Güttel,Kaustubh Roy*

Main category: cs.LG

TL;DR: 将 CLASSIX 聚类扩展到曼哈顿距离和 Tanimoto 距离，改用数据向量的范数排序并利用三角不等式进行搜索终止；在 Tanimoto 距离上采用更尖锐的交集界限。化学指纹基准上，CLASSIX Tanimoto 分别比 Taylor-Butina 快约 30 倍、比 DBSCAN 快约 80 倍，且簇的质量更高。


<details>
  <summary>Details</summary>
Motivation: 提高大规模高维化学指纹数据的聚类效率与可解释性，同时将原本基于主成分排序的思路推广到更多距离度量，提升在实际应用中的实用性和性能。

Method: 将原本基于第一主成分排序的数据点截断搜索的思路改为基于数据向量范数的排序；结合三角不等式实现搜索终止；扩展到曼哈顿距离；针对 Tanimoto 距离采用更锐的交集不等式以进一步提升性能。对真实化学指纹基准进行了评估。

Result: 在化学指纹基准上，CLASSIX Tanimoto 相较 Taylor-Butina 约快 30 倍，相较 DBSCAN 约快 80 倍，且获得更高质量的聚类。

Conclusion: CLASSIX 的排序-截断框架可扩展到其他距离度量，尤其是 Tanimoto 距离，且结合更严格的界限条件可显著提升在实际数据集上的效率与簇质量，显示该思路在化学信息学等领域的广泛适用性。

Abstract: The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.

</details>
