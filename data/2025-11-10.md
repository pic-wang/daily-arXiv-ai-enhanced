<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 61]
- [stat.ML](#stat.ML) [Total: 4]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Stateful KV Cache Management for LLMs: Balancing Space, Time, Accuracy, and Positional Fidelity](https://arxiv.org/abs/2511.04686)
*Pratik Poudel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Key-Value (KV) cache is integral to efficient autoregressive inference in
large language models (LLMs), yet its unbounded growth in stateful multi-turn
scenarios presents major challenges. This paper examines the interplay between
KV cache management strategies, the architectural context limits of models like
meta-llama/Meta-Llama-3-8b-instruct, and the often-overlooked integrity of
positional encodings. Through empirical analysis using a stateful benchmarking
framework, we show that LLM generation quality degrades sharply when the
accumulated KV cache approaches or exceeds the model's trained context window
(e.g., 8192 tokens for Llama 3), a failure mode distinct from GPU memory
exhaustion. Common eviction strategies, even high-retention ones (e.g., 99% via
AttentionTop), can worsen performance if they disrupt positional coherence.
Because LLMs rely on consistent positional signals (e.g., RoPE), compacting a
cache by removing non-contiguous tokens can scramble these signals and lead to
degenerative outputs. We further show that simple strategies preserving
contiguous context blocks (e.g., keeping an initial "gist") can yield more
coherent generations than complex or positionally disruptive ones. We advocate
for eviction techniques that respect architectural limits, preserve positional
structure, and view "cache health" holistically beyond mere size.

</details>


### [2] [Causal Structure and Representation Learning with Biomedical Applications](https://arxiv.org/abs/2511.04790)
*Caroline Uhler,Jiaqi Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Massive data collection holds the promise of a better understanding of
complex phenomena and, ultimately, better decisions. Representation learning
has become a key driver of deep learning applications, as it allows learning
latent spaces that capture important properties of the data without requiring
any supervised annotations. Although representation learning has been hugely
successful in predictive tasks, it can fail miserably in causal tasks including
predicting the effect of a perturbation/intervention. This calls for a marriage
between representation learning and causal inference. An exciting opportunity
in this regard stems from the growing availability of multi-modal data
(observational and perturbational, imaging-based and sequencing-based, at the
single-cell level, tissue-level, and organism-level). We outline a statistical
and computational framework for causal structure and representation learning
motivated by fundamental biomedical questions: how to effectively use
observational and perturbational data to perform causal discovery on observed
causal variables; how to use multi-modal views of the system to learn causal
variables; and how to design optimal perturbations.

</details>


### [3] [Temporal convolutional and fusional transformer model with Bi-LSTM encoder-decoder for multi-time-window remaining useful life prediction](https://arxiv.org/abs/2511.04723)
*Mohamadreza Akbari Pour,Mohamad Sadeq Karimi,Amir Hossein Mazloumi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Health prediction is crucial for ensuring reliability, minimizing downtime,
and optimizing maintenance in industrial systems. Remaining Useful Life (RUL)
prediction is a key component of this process; however, many existing models
struggle to capture fine-grained temporal dependencies while dynamically
prioritizing critical features across time for robust prognostics. To address
these challenges, we propose a novel framework that integrates Temporal
Convolutional Networks (TCNs) for localized temporal feature extraction with a
modified Temporal Fusion Transformer (TFT) enhanced by Bi-LSTM encoder-decoder.
This architecture effectively bridges short- and long-term dependencies while
emphasizing salient temporal patterns. Furthermore, the incorporation of a
multi-time-window methodology improves adaptability across diverse operating
conditions. Extensive evaluations on benchmark datasets demonstrate that the
proposed model reduces the average RMSE by up to 5.5%, underscoring its
improved predictive accuracy compared to state-of-the-art methods. By closing
critical gaps in current approaches, this framework advances the effectiveness
of industrial prognostic systems and highlights the potential of advanced
time-series transformers for RUL prediction.

</details>


### [4] [Efficient Swap Multicalibration of Elicitable Properties](https://arxiv.org/abs/2511.04907)
*Lunjia Hu,Haipeng Luo,Spandan Senapati,Vatsal Sharan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multicalibration [HJKRR18] is an algorithmic fairness perspective that
demands that the predictions of a predictor are correct conditional on
themselves and membership in a collection of potentially overlapping subgroups
of a population. The work of [NR23] established a surprising connection between
multicalibration for an arbitrary property $\Gamma$ (e.g., mean or median) and
property elicitation: a property $\Gamma$ can be multicalibrated if and only if
it is elicitable, where elicitability is the notion that the true property
value of a distribution can be obtained by solving a regression problem over
the distribution. In the online setting, [NR23] proposed an inefficient
algorithm that achieves $\sqrt T$ $\ell_2$-multicalibration error for a
hypothesis class of group membership functions and an elicitable property
$\Gamma$, after $T$ rounds of interaction between a forecaster and adversary.
  In this paper, we generalize multicalibration for an elicitable property
$\Gamma$ from group membership functions to arbitrary bounded hypothesis
classes and introduce a stronger notion -- swap multicalibration, following
[GKR23]. Subsequently, we propose an oracle-efficient algorithm which, when
given access to an online agnostic learner, achieves $T^{1/(r+1)}$
$\ell_r$-swap multicalibration error with high probability (for $r\ge2$) for a
hypothesis class with bounded sequential Rademacher complexity and an
elicitable property $\Gamma$. For the special case of $r=2$, this implies an
oracle-efficient algorithm that achieves $T^{1/3}$ $\ell_2$-swap
multicalibration error, which significantly improves on the previously
established bounds for the problem [NR23, GMS25, LSS25a], and completely
resolves an open question raised in [GJRR24] on the possibility of an
oracle-efficient algorithm that achieves $\sqrt{T}$ $\ell_2$-mean
multicalibration error by answering it in a strongly affirmative sense.

</details>


### [5] [Regularized GLISp for sensor-guided human-in-the-loop optimization](https://arxiv.org/abs/2511.04751)
*Matteo Cercola,Michele Lomuscio,Dario Piga,Simone Formentin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human-in-the-loop calibration is often addressed via preference-based
optimization, where algorithms learn from pairwise comparisons rather than
explicit cost evaluations. While effective, methods such as Preferential
Bayesian Optimization or Global optimization based on active preference
learning with radial basis functions (GLISp) treat the system as a black box
and ignore informative sensor measurements. In this work, we introduce a
sensor-guided regularized extension of GLISp that integrates measurable
descriptors into the preference-learning loop through a physics-informed
hypothesis function and a least-squares regularization term. This injects
grey-box structure, combining subjective feedback with quantitative sensor
information while preserving the flexibility of preference-based search.
Numerical evaluations on an analytical benchmark and on a human-in-the-loop
vehicle suspension tuning task show faster convergence and superior final
solutions compared to baseline GLISp.

</details>


### [6] [Scaling Up ROC-Optimizing Support Vector Machines](https://arxiv.org/abs/2511.04979)
*Gimun Bae,Seung Jun Shin*

Main category: cs.LG

TL;DR: A scalable ROC-SVM using incomplete U-statistics and low-rank kernel approximation to reduce computational cost while maintaining AUC performance; includes theoretical error bound and empirical validation.


<details>
  <summary>Details</summary>
Motivation: 原始 ROC-SVM 因为需要对所有样本对进行比较，计算成本高，在样本不平衡场景中实现可扩展的 AUC 优化变得困难。

Method: 提出基于不完全 U-统计量的近似以降低对偶对的计算量，并通过低秩核近似实现对非线性分类的高效训练，给出理论误差边界。

Result: 在合成数据与真实数据集上，所提方法在训练时间显著减少的同时，AUC 性能与原始 ROC-SVM 相当。

Conclusion: 方法提供了一个可扩展的 ROC 优化框架，结合不完全 U-统计近似和低秩核，具备理论保证并在实践中表现出色。

Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the
area under the ROC curve (AUC) and has become an attractive alternative of the
conventional binary classification under the presence of class imbalance.
However, its practical use is limited by high computational cost, as training
involves evaluating all $O(n^2)$. To overcome this limitation, we develop a
scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby
substantially reducing computational complexity. We further extend the
framework to nonlinear classification through a low-rank kernel approximation,
enabling efficient training in reproducing kernel Hilbert spaces. Theoretical
analysis establishes an error bound that justifies the proposed approximation,
and empirical results on both synthetic and real datasets demonstrate that the
proposed method achieves comparable AUC performance to the original ROC-SVM
with drastically reduced training time.

</details>


### [7] [When Data Falls Short: Grokking Below the Critical Threshold](https://arxiv.org/abs/2511.04760)
*Vaibhav Singh,Eugene Belilovsky,Rahaf Aljundi*

Main category: cs.LG

TL;DR: 知识蒸馏（KD）可以在数据稀缺和分布转移的情境中，触发并加速“grokking”现象，即模型在训练过拟合后 delayed 仍能显著泛化；在多分布/联合分布以及连续预训练场景中，KD使得从已grokked模型迁移到新分布成为可能，甚至在仅有10%数据时也能显著减少灾难性遗忘并提升泛化。


<details>
  <summary>Details</summary>
Motivation: 揭示在数据受限和分布漂移条件下，grokking 的机制以及知识蒸馏在实现跨分布泛化中的作用，特别是在实际部署需应对新分布且数据有限的场景。

Method: 系统研究数据稀缺情形下的 grokking；首先证明来自已 grokked 的 p1 模型的 KD 可以在另一分布 p2 上诱导并加速 grokking，数据低于临界阈值也可实现；接着在联合分布（p1, p2）上训练，证明若单独分布数据不足，直接训练会失败，但从在各自分布上 grokked 的模型进行蒸馏则能实现泛化；最后在持续预训练场景中，将模型从 p1 转向 p2，发现 KD 可加速泛化并缓解灾难性遗忘，即使只有 10% 的数据也能达到强性能。

Result: 关键发现包括：1) KD 可以将 grokking 从一个分布迁移到另一个分布，即使目标分布数据很少；2) 在联合分布下，普通监督训练在任一分布数据不足时会失效，而从对各自分布 grokked 的模型蒸馏则能实现泛化；3) 连续预训练中，KD 不仅加速泛化，还抑制遗忘，数据量只有 10% 时亦能达到显著表现。

Conclusion: KD 在低数据与分布演化场景中对 grokking 的促进作用居于核心地位，提供了在实际部署中利用知识蒸馏实现跨分布快速适应与持续学习的理论与方法论。

Abstract: In this paper, we investigate the phenomenon of grokking, where models
exhibit delayed generalization following overfitting on training data. We focus
on data-scarce regimes where the number of training samples falls below the
critical threshold, making grokking unobservable, and on practical scenarios
involving distribution shift. We first show that Knowledge Distillation (KD)
from a model that has already grokked on a distribution (p1) can induce and
accelerate grokking on a different distribution (p2), even when the available
data lies below the critical threshold. This highlights the value of KD for
deployed models that must adapt to new distributions under limited data. We
then study training on the joint distribution (p1, p2) and demonstrate that
while standard supervised training fails when either distribution has
insufficient data, distilling from models grokked on the individual
distributions enables generalization. Finally, we examine a continual
pretraining setup, where a grokked model transitions from p1 to p2, and find
that KD both accelerates generalization and mitigates catastrophic forgetting,
achieving strong performance even with only 10% of the data. Together, our
results provide new insights into the mechanics of grokking under knowledge
transfer and underscore the central role of KD in enabling generalization in
low-data and evolving distribution settings.

</details>


### [8] [Linear Gradient Prediction with Control Variates](https://arxiv.org/abs/2511.05187)
*Kamil Ciosek,Nicolò Felicioni,Juan Elenter Litwin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a new way of training neural networks, with the goal of reducing
training cost. Our method uses approximate predicted gradients instead of the
full gradients that require an expensive backward pass. We derive a
control-variate-based technique that ensures our updates are unbiased estimates
of the true gradient. Moreover, we propose a novel way to derive a predictor
for the gradient inspired by the theory of the Neural Tangent Kernel. We
empirically show the efficacy of the technique on a vision transformer
classification task.

</details>


### [9] [FuseFlow: A Fusion-Centric Compilation Framework for Sparse Deep Learning on Streaming Dataflow](https://arxiv.org/abs/2511.04768)
*Rubens Lacouture,Nathan Zhang,Ritvik Sharma,Marco Siracusa,Fredrik Kjolstad,Kunle Olukotun,Olivia Hsu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As deep learning models scale, sparse computation and specialized dataflow
hardware have emerged as powerful solutions to address efficiency. We propose
FuseFlow, a compiler that converts sparse machine learning models written in
PyTorch to fused sparse dataflow graphs for reconfigurable dataflow
architectures (RDAs). FuseFlow is the first compiler to support general
cross-expression fusion of sparse operations. In addition to fusion across
kernels (expressions), FuseFlow also supports optimizations like
parallelization, dataflow ordering, and sparsity blocking. It targets a
cycle-accurate dataflow simulator for microarchitectural analysis of fusion
strategies. We use FuseFlow for design-space exploration across four real-world
machine learning applications with sparsity, showing that full fusion (entire
cross-expression fusion across all computation in an end-to-end model) is not
always optimal for sparse models-fusion granularity depends on the model
itself. FuseFlow also provides a heuristic to identify and prune suboptimal
configurations. Using Fuseflow, we achieve performance improvements, including
a ~2.7x speedup over an unfused baseline for GPT-3 with BigBird block-sparse
attention.

</details>


### [10] [Sample Complexity of Distributionally Robust Off-Dynamics Reinforcement Learning with Online Interaction](https://arxiv.org/abs/2511.05396)
*Yiting He,Zhishuai Liu,Weixin Wang,Pan Xu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Off-dynamics reinforcement learning (RL), where training and deployment
transition dynamics are different, can be formulated as learning in a robust
Markov decision process (RMDP) where uncertainties in transition dynamics are
imposed. Existing literature mostly assumes access to generative models
allowing arbitrary state-action queries or pre-collected datasets with a good
state coverage of the deployment environment, bypassing the challenge of
exploration. In this work, we study a more realistic and challenging setting
where the agent is limited to online interaction with the training environment.
To capture the intrinsic difficulty of exploration in online RMDPs, we
introduce the supremal visitation ratio, a novel quantity that measures the
mismatch between the training dynamics and the deployment dynamics. We show
that if this ratio is unbounded, online learning becomes exponentially hard. We
propose the first computationally efficient algorithm that achieves sublinear
regret in online RMDPs with $f$-divergence based transition uncertainties. We
also establish matching regret lower bounds, demonstrating that our algorithm
achieves optimal dependence on both the supremal visitation ratio and the
number of interaction episodes. Finally, we validate our theoretical results
through comprehensive numerical experiments.

</details>


### [11] [Synapse: Adaptive Arbitration of Complementary Expertise in Time Series Foundational Models](https://arxiv.org/abs/2511.05460)
*Sarkar Snigdha Sarathi Das,Palash Goyal,Mihir Parmar,Yiwen Song,Long T. Le,Lesly Miculicich,Jinsung Yoon,Rui Zhang,Hamid Palangi,Tomas Pfister*

Main category: cs.LG

TL;DR: 提出 Synapse 的 TSFM 仲裁框架，通过基于上下文的动态权重分配与输出分位数采样，提升多模型时序预测的鲁棒性和准确性，优于常见集成方法和单一 TSFM。


<details>
  <summary>Details</summary>
Motivation: 不同的预训练时序模型由不同的训练协议和数据源驱动，导致在不同任务、领域和预测 horizons 上表现差异显著。需要一个能有效对多模型输出进行仲裁、发挥各自优势的框架来实现更稳健的整体预测。

Method: 先分析各 TSFM 在不同设定下的专门化性能分布、受模型选择与预测区间分布等因素影响；提出 Synapse，通过动态分配预测权重来利用上下文相关的相对表现，并通过对组成模型的输出分位数进行自适应采样构造鲁棒的预测分布。

Result: 实验结果表明，Synapse 在多数情形下优于其他流行的集成方法以及单一 TSFM，展示了在时序预测中的有效性。

Conclusion: 基于上下文对不同 TSFM 的仲裁与分位数采样构建鲁棒分布，能够有效提升时序预测的性能与稳定性。

Abstract: Pre-trained Time Series Foundational Models (TSFMs) represent a significant
advance, capable of forecasting diverse time series with complex
characteristics, including varied seasonalities, trends, and long-range
dependencies. Despite their primary goal of universal time series forecasting,
their efficacy is far from uniform; divergent training protocols and data
sources cause individual TSFMs to exhibit highly variable performance across
different forecasting tasks, domains, and horizons. Leveraging this
complementary expertise by arbitrating existing TSFM outputs presents a
compelling strategy, yet this remains a largely unexplored area of research. In
this paper, we conduct a thorough examination of how different TSFMs exhibit
specialized performance profiles across various forecasting settings, and how
we can effectively leverage this behavior in arbitration between different time
series models. We specifically analyze how factors such as model selection and
forecast horizon distribution can influence the efficacy of arbitration
strategies. Based on this analysis, we propose Synapse, a novel arbitration
framework for TSFMs. Synapse is designed to dynamically leverage a pool of
TSFMs, assign and adjust predictive weights based on their relative,
context-dependent performance, and construct a robust forecast distribution by
adaptively sampling from the output quantiles of constituent models.
Experimental results demonstrate that Synapse consistently outperforms other
popular ensembling techniques as well as individual TSFMs, demonstrating
Synapse's efficacy in time series forecasting.

</details>


### [12] [Conditional Neural ODE for Longitudinal Parkinson's Disease Progression Forecasting](https://arxiv.org/abs/2511.04789)
*Xiaoda Wang,Yuji Zhao,Kaiqiao Han,Xiao Luo,Sanne van Rooij,Jennifer Stevens,Lifang He,Liang Zhan,Yizhou Sun,Wei Wang,Carl Yang*

Main category: cs.LG

TL;DR: 提出 CNODE（Conditional Neural ODE），通过连续神经常微分方程建模PD大脑形态随时间的连续变化，并联合学习个体初始时间及进度速度以对齐轨迹，从而在PPMI数据上实现对PD进展的长期预测，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: PD展现出异质性、逐步演变的脑形态模式，且MRI数据通常不规则且稀疏；现有RNN/Transformer在处理不规则数据和捕捉个体差异方面存在局限。

Method: 提出 CNODE，使用神经ODE将脑结构变化建模为连续时间过程；联合学习患者特异的初始时间与进度速度以对齐个体轨迹至共享进展轨迹；在Parkinson's Progression Markers Initiative (PPMI) 数据集上进行验证并与基线方法对比。

Result: 在长期PD进展预测任务中，CNODE的预测性能超越最先进的基线方法。

Conclusion: CNODE证明了在连续时间、个体化框架下对PD进程进行有效预测的可行性，有望提升疾病机制理解、治疗开发与数字“数字孪生”预测的应用前景。

Abstract: Parkinson's disease (PD) shows heterogeneous, evolving brain-morphometry
patterns. Modeling these longitudinal trajectories enables mechanistic insight,
treatment development, and individualized 'digital-twin' forecasting. However,
existing methods usually adopt recurrent neural networks and transformer
architectures, which rely on discrete, regularly sampled data while struggling
to handle irregular and sparse magnetic resonance imaging (MRI) in PD cohorts.
Moreover, these methods have difficulty capturing individual heterogeneity
including variations in disease onset, progression rate, and symptom severity,
which is a hallmark of PD. To address these challenges, we propose CNODE
(Conditional Neural ODE), a novel framework for continuous, individualized PD
progression forecasting. The core of CNODE is to model morphological brain
changes as continuous temporal processes using a neural ODE model. In addition,
we jointly learn patient-specific initial time and progress speed to align
individual trajectories into a shared progression trajectory. We validate CNODE
on the Parkinson's Progression Markers Initiative (PPMI) dataset. Experimental
results show that our method outperforms state-of-the-art baselines in
forecasting longitudinal PD progression.

</details>


### [13] [On Flow Matching KL Divergence](https://arxiv.org/abs/2511.05480)
*Maojiang Su,Jerry Yao-Chieh Hu,Sophia Pi,Han Liu*

Main category: cs.LG

TL;DR: 提出一个非渐进的KL散度上界，基于L2流动匹配损失epsilon，给出KL(p_true || p_est) <= A1*epsilon + A2*epsilon^2，且在TV距离下给出统计收敛率；显示Flow Matching Transformers接近minimax效率，与扩散模型在TV距离下的统计效率相当；并给出数值验证。


<details>
  <summary>Details</summary>
Motivation: 提高流动匹配在统计学习中的效率与理论保障，给出非渐进的误差界，并与扩散模型的理论巩固进行比较。

Method: 建立确定性、非渐进的KL上界，依赖于L2流动匹配损失的上界epsilon，以及数据和速度场的正则性，推导出KL误差与epsilon之间的关系；进一步推导在TV距离下的收敛性，并与Diffusion模型进行对比，辅以数值实验。

Result: KL散度上界为A1*epsilon + A2*epsilon^2，A1,A2仅依赖于数据与速度场的正则性；在平滑分布下，Flow Matching Transformers达到近似minimax的效率，在TV距离下的统计效率与扩散模型相当；数值研究在合成数据与学习到的速度场上验证理论。

Conclusion: 给出流动匹配的非渐近统计界与理论/实验证据，表明其在TV距离下的收敛性和统计效率可与扩散模型媲美，且对A1,A2等常数的依赖性明确。

Abstract: We derive a deterministic, non-asymptotic upper bound on the Kullback-Leibler
(KL) divergence of the flow-matching distribution approximation. In particular,
if the $L_2$ flow-matching loss is bounded by $\epsilon^2 > 0$, then the KL
divergence between the true data distribution and the estimated distribution is
bounded by $A_1 \epsilon + A_2 \epsilon^2$. Here, the constants $A_1$ and $A_2$
depend only on the regularities of the data and velocity fields. Consequently,
this bound implies statistical convergence rates of Flow Matching Transformers
under the Total Variation (TV) distance. We show that, flow matching achieves
nearly minimax-optimal efficiency in estimating smooth distributions. Our
results make the statistical efficiency of flow matching comparable to that of
diffusion models under the TV distance. Numerical studies on synthetic and
learned velocities corroborate our theory.

</details>


### [14] [Simplex-FEM Networks (SiFEN): Learning A Triangulated Function Approximator](https://arxiv.org/abs/2511.04804)
*Chaymae Yahyati,Ismail Lamaakal,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce Simplex-FEM Networks (SiFEN), a learned piecewise-polynomial
predictor that represents f: R^d -> R^k as a globally C^r finite-element field
on a learned simplicial mesh in an optionally warped input space. Each query
activates exactly one simplex and at most d+1 basis functions via barycentric
coordinates, yielding explicit locality, controllable smoothness, and
cache-friendly sparsity. SiFEN pairs degree-m Bernstein-Bezier polynomials with
a light invertible warp and trains end-to-end with shape regularization,
semi-discrete OT coverage, and differentiable edge flips. Under standard
shape-regularity and bi-Lipschitz warp assumptions, SiFEN achieves the classic
FEM approximation rate M^(-m/d) with M mesh vertices. Empirically, on synthetic
approximation tasks, tabular regression/classification, and as a drop-in head
on compact CNNs, SiFEN matches or surpasses MLPs and KANs at matched parameter
budgets, improves calibration (lower ECE/Brier), and reduces inference latency
due to geometric locality. These properties make SiFEN a compact,
interpretable, and theoretically grounded alternative to dense MLPs and
edge-spline networks.

</details>


### [15] [PuzzleMoE: Efficient Compression of Large Mixture-of-Experts Models via Sparse Expert Merging and Bit-packed inference](https://arxiv.org/abs/2511.04805)
*Yushu Zhao,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Mixture-of-Experts (MoE) models have shown strong potential in scaling
language models efficiently by activating only a small subset of experts per
input. However, their widespread deployment remains limited due to the high
memory overhead associated with storing all expert parameters, particularly as
the number of experts increases. To address this challenge, prior works have
explored expert dropping and merging strategies, yet they often suffer from
performance drop at high compression ratios. In this paper, we introduce
PuzzleMoE, a training-free MoE compression method that achieves both high
accuracy and efficient inference through two key innovations: First, PuzzleMoE
performs sparse expert merging by identifying element-wise weight redundancy
and specialization. It uses a dual-mask to capture both shared and
expert-specific parameters. Second, to avoid the overhead of storing binary
masks and signs, PuzzleMoE introduces a bit-packed encoding scheme that reuses
underutilized exponent bits, enabling efficient MoE inference on GPUs.
Extensive experiments demonstrate that PuzzleMoE can compress MoE models by up
to 50% while maintaining accuracy across various tasks. Specifically, it
outperforms prior MoE compression methods by up to 16.7% on MMLU at 50%
compression ratio, and achieves up to 1.28\times inference speedup.

</details>


### [16] [Autoencoding Dynamics: Topological Limitations and Capabilities](https://arxiv.org/abs/2511.04807)
*Matthew D. Kvalheim,Eduardo D. Sontag*

Main category: cs.LG

TL;DR: Autoencoders map data lying on a manifold M in R^n to a latent space R^l via E and D, with D∘E approximating the identity on M. The paper analyzes topological limitations and capabilities of autoencoders and discusses encoding dynamical systems where M is an invariant manifold.


<details>
  <summary>Details</summary>
Motivation: Understand fundamental topological constraints and capabilities of representation learning with autoencoders, especially how manifold topology restricts or enables encoding/decoding and preserving dynamical structure on invariant manifolds.

Method: Theoretical/topological analysis of continuous encoder/decoder maps E: R^n -> R^l and D: R^l -> R^n, focusing on the condition D∘E ≈ id_M and exploring implications for manifolds M and invariant dynamical systems; discuss existence, obstructions, and potential constructive approaches.

Result: Characterizes limitations and opportunities imposed by topology on autoencoder existence and quality; identifies when continuous encoders/decoders can approximate the identity on M; describes how autoencoders can retain or reflect dynamical structure on invariant manifolds.

Conclusion: Topology imposes fundamental constraints on autoencoder representability; under favorable conditions (e.g., appropriate M, embedding dimension l), D∘E can approximate id_M, enabling faithful encoding/decoding and dynamical system embedding on invariant manifolds; practical design must consider manifold topology and invariant properties.

Abstract: Given a "data manifold" $M\subset \mathbb{R}^n$ and "latent space"
$\mathbb{R}^\ell$, an autoencoder is a pair of continuous maps consisting of an
"encoder" $E\colon \mathbb{R}^n\to \mathbb{R}^\ell$ and "decoder" $D\colon
\mathbb{R}^\ell\to \mathbb{R}^n$ such that the "round trip" map $D\circ E$ is
as close as possible to the identity map $\mbox{id}_M$ on $M$. We present
various topological limitations and capabilites inherent to the search for an
autoencoder, and describe capabilities for autoencoding dynamical systems
having $M$ as an invariant manifold.

</details>


### [17] [Sharp Minima Can Generalize: A Loss Landscape Perspective On Data](https://arxiv.org/abs/2511.04808)
*Raymond Fan,Bryce Sandlund,Lin Myat Ko*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The volume hypothesis suggests deep learning is effective because it is
likely to find flat minima due to their large volumes, and flat minima
generalize well. This picture does not explain the role of large datasets in
generalization. Measuring minima volumes under varying amounts of training data
reveals sharp minima which generalize well exist, but are unlikely to be found
due to their small volumes. Increasing data changes the loss landscape, such
that previously small generalizing minima become (relatively) large.

</details>


### [18] [A Standardized Benchmark for Multilabel Antimicrobial Peptide Classification](https://arxiv.org/abs/2511.04814)
*Sebastian Ojeda,Rafael Velasquez,Nicolás Aparicio,Juanita Puentes,Paula Cárdenas,Nicolás Andrade,Gabriel González,Sergio Rincón,Carolina Muñoz-Camargo,Pablo Arbeláez*

Main category: cs.LG

TL;DR: ESCAPE数据集和基于Transformer的多标签抗微生物肽功能预测方法，显著提升多标签分类性能，达到新的state-of-the-art。


<details>
  <summary>Details</summary>
Motivation: 由于数据分散、注释不一致、缺乏标准基准，阻碍AI驱动的抗微生物肽发现与评估。

Method: 构建ESCAPE数据集，整合8万+肽来自27个验证仓库，区分抗菌肽与负序列并建立功能注释的多标签层级；提出基于序列与结构信息的Transformer模型以预测肽的多种功能。

Result: 相对于第二最佳方法，平均精度的相对提升可达2.56%，实现多标签肽分类的新State-of-the-Art。

Conclusion: ESCAPE提供一个全面且可重复的评估框架，推进AI驱动的抗微生物肽研究。

Abstract: Antimicrobial peptides have emerged as promising molecules to combat
antimicrobial resistance. However, fragmented datasets, inconsistent
annotations, and the lack of standardized benchmarks hinder computational
approaches and slow down the discovery of new candidates. To address these
challenges, we present the Expanded Standardized Collection for Antimicrobial
Peptide Evaluation (ESCAPE), an experimental framework integrating over 80.000
peptides from 27 validated repositories. Our dataset separates antimicrobial
peptides from negative sequences and incorporates their functional annotations
into a biologically coherent multilabel hierarchy, capturing activities across
antibacterial, antifungal, antiviral, and antiparasitic classes. Building on
ESCAPE, we propose a transformer-based model that leverages sequence and
structural information to predict multiple functional activities of peptides.
Our method achieves up to a 2.56% relative average improvement in mean Average
Precision over the second-best method adapted for this task, establishing a new
state-of-the-art multilabel peptide classification. ESCAPE provides a
comprehensive and reproducible evaluation framework to advance AI-driven
antimicrobial peptide research.

</details>


### [19] [Persistent reachability homology in machine learning applications](https://arxiv.org/abs/2511.04825)
*Luigi Caputi,Nicholas Meadows,Henri Riihimäki*

Main category: cs.LG

TL;DR: PRH (persistent reachability homology) improves epilepsy detection from directed graphs compared to the traditional DPH (directed flag complex). Uses Betti curves as features with an SVM classifier.


<details>
  <summary>Details</summary>
Motivation: Enhance topological analysis of directed graphs for network classification in neuroscience, and reduce computational complexity by using condensations in the persistent filtration.

Method: Compute PRH and DPH on digraphs, extract Betti curves and their integrals from the persistent filtrations, and feed them into a support vector machine for classification.

Result: PRH outperforms DPH in the epilepsy classification task, according to the reported experiments.

Conclusion: PRH is a promising and computationally efficient alternative to DPH for directed graphs in neural data analysis, with Betti-curve based features yielding effective classification.

Abstract: We explore the recently introduced persistent reachability homology (PRH) of
digraph data, i.e. data in the form of directed graphs. In particular, we study
the effectiveness of PRH in network classification task in a key neuroscience
problem: epilepsy detection. PRH is a variation of the persistent homology of
digraphs, more traditionally based on the directed flag complex (DPH). A main
advantage of PRH is that it considers the condensations of the digraphs
appearing in the persistent filtration and thus is computed from smaller
digraphs. We compare the effectiveness of PRH to that of DPH and we show that
PRH outperforms DPH in the classification task. We use the Betti curves and
their integrals as topological features and implement our pipeline on support
vector machine.

</details>


### [20] [Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models](https://arxiv.org/abs/2511.04834)
*Jiwoo Shin,Byeonghu Na,Mina Kang,Wonhyeok Choi,Il-chul Moon*

Main category: cs.LG

TL;DR: 本论文揭示了两类文本到图像生成模型防御方法（对有害概念进行微调以“忘记”它们；以及不训练的负提示引导）之间的本质不兼容，提出用概念反演得到的隐式负嵌入来替代负提示，从而实现对两者的无改动整合。实验在裸露与暴力等基准上验证，防御成功率显著提升，同时尽量保留输入提示的语义。


<details>
  <summary>Details</summary>
Motivation: 解决两类防御范式（微调以去除有害概念与训练无关的负提示）在联合时常表现出边际或甚至下降的防御效果的问题，寻找一种简单但鲁棒的融合方式，提升对有害输出的防护能力而不显著改变输入语义。

Method: 在训练无关的防御方法中，用概念反演得到的隐式负嵌入替代传统的负提示，实现对有害输出的约束。该方法无需修改任何已有防御方法的框架，可无缝集成到现有管线中。

Result: 在裸露与暴力等基准上进行实验，显示该策略在提升防御成功率方面具有一致性，同时对输入提示的核心语义保持良好保存。

Conclusion: 隐式负嵌入的方法简单、鲁棒且与现有两大防御范式兼容，能在不改动原有提示的前提下提升防御效果。

Abstract: Recent advances in text-to-image generative models have raised concerns about
their potential to produce harmful content when provided with malicious input
text prompts. To address this issue, two main approaches have emerged: (1)
fine-tuning the model to unlearn harmful concepts and (2) training-free
guidance methods that leverage negative prompts. However, we observe that
combining these two orthogonal approaches often leads to marginal or even
degraded defense performance. This observation indicates a critical
incompatibility between two paradigms, which hinders their combined
effectiveness. In this work, we address this issue by proposing a conceptually
simple yet experimentally robust method: replacing the negative prompts used in
training-free methods with implicit negative embeddings obtained through
concept inversion. Our method requires no modification to either approach and
can be easily integrated into existing pipelines. We experimentally validate
its effectiveness on nudity and violence benchmarks, demonstrating consistent
improvements in defense success rate while preserving the core semantics of
input prompts.

</details>


### [21] [Sublinear iterations can suffice even for DDPMs](https://arxiv.org/abs/2511.04844)
*Matthew S. Zhang,Stephen Huan,Jerry Huang,Nicholas M. Boffi,Sitan Chen,Sinho Chewi*

Main category: cs.LG

TL;DR: 引入 denoising diffusion randomized midpoint method (DDRaM)，通过随机中点提升对 DDPM 的数值积分近似，在适当光滑假设下实现子线性复杂度的采样（O~(√d) 次分数评估），首次给出纯 DDPM 采样的子线性复杂度界，并在预训练图像模型上实验验证实用性。


<details>
  <summary>Details</summary>
Motivation: 希望超越依赖维度线性或初始 Fisher 信息的收敛性分析；受 log-concave 采样的启发，寻求在 DDPM 的 SDE 离散化中实现更低的计算复杂度与更好的数值近似。

Method: 提出 DDRaM，通过引入一个随机中点来改进对 SDE 的近似；结合“shifted composition rule”的分析框架，在适当的光滑性假设下证明其离散化性质良好。

Result: 理论上得到子线性复杂度界，具体为需要约 ~√d 次分数评估以保证收敛；这是纯 DDPM 采样的首个子线性复杂度界；在实验中对预训练的图像生成模型进行了验证，显示实际效果良好。

Conclusion: DDRaM 提供更优的离散化特性与实际效率，证实在不改动采样器的前提下实现子线性维度依赖的采样成本的可行性。

Abstract: SDE-based methods such as denoising diffusion probabilistic models (DDPMs)
have shown remarkable success in real-world sample generation tasks. Prior
analyses of DDPMs have been focused on the exponential Euler discretization,
showing guarantees that generally depend at least linearly on the dimension or
initial Fisher information. Inspired by works in log-concave sampling (Shen and
Lee, 2019), we analyze an integrator -- the denoising diffusion randomized
midpoint method (DDRaM) -- that leverages an additional randomized midpoint to
better approximate the SDE. Using a recently-developed analytic framework
called the "shifted composition rule", we show that this algorithm enjoys
favorable discretization properties under appropriate smoothness assumptions,
with sublinear $\widetilde{O}(\sqrt{d})$ score evaluations needed to ensure
convergence. This is the first sublinear complexity bound for pure DDPM
sampling -- prior works which obtained such bounds worked instead with
ODE-based sampling and had to make modifications to the sampler which deviate
from how they are used in practice. We also provide experimental validation of
the advantages of our method, showing that it performs well in practice with
pre-trained image synthesis models.

</details>


### [22] [Investigating U.S. Consumer Demand for Food Products with Innovative Transportation Certificates Based on Stated Preferences and Machine Learning Approaches](https://arxiv.org/abs/2511.04845)
*Jingchen Bi,Rodrigo Mesa-Arango*

Main category: cs.LG

TL;DR: 研究利用机器学习评估消费者对具备创新运输证书的食品产品购买偏好，确定五种运输证书并给出数据驱动的改进建议。


<details>
  <summary>Details</summary>
Motivation: 扩展对具供应链可追溯性食品的需求研究，明确运输因素在消费者购买决策中的影响，并发现被广泛重视的运输属性。

Method: 分两步研究：1) 使用机器学习模型估计消费者对带有创新运输证书的食品产品的行为；2) 通过偏好实验确定消费者重视的具体运输属性，并控制产品类型和决策者因素。提出五类证书：运输模式、物联网（IoT）、安全措施、能源来源、必须按时到达日期（MABD）。

Result: 发现对安全与能源证书有显著偏好；价格、产品类型、证书与决策者因素对购买选择有影响；提供数据驱动的食品供应链改进建议。

Conclusion: 研究为提升食品供应链可追溯性及效率提供证据与建议，强调在运输属性—尤其是安全性和能源管理方面的关注。

Abstract: This paper utilizes a machine learning model to estimate the consumer's
behavior for food products with innovative transportation certificates in the
U.S. Building on previous research that examined demand for food products with
supply chain traceability using stated preference analysis, transportation
factors were identified as significant in consumer food purchasing choices.
Consequently, a second experiment was conducted to pinpoint the specific
transportation attributes valued by consumers. A machine learning model was
applied, and five innovative certificates related to transportation were
proposed: Transportation Mode, Internet of Things (IoT), Safety measures,
Energy Source, and Must Arrive By Dates (MABDs). The preference experiment also
incorporated product-specific and decision-maker factors for control purposes.
The findings reveal a notable inclination toward safety and energy certificates
within the transportation domain of the U.S. food supply chain. Additionally,
the study examined the influence of price, product type, certificates, and
decision-maker factors on purchasing choices. Ultimately, the study offers
data-driven recommendations for improving food supply chain systems.

</details>


### [23] [SigmaDock: Untwisting Molecular Docking With Fragment-Based SE(3) Diffusion](https://arxiv.org/abs/2511.04854)
*Alvaro Prat,Leo Zhang,Charlotte M. Deane,Yee Whye Teh,Garrett M. Morris*

Main category: cs.LG

TL;DR: SigmaDock 是一种在 SE(3) 流形上以分子片段为单位的扩散模型，通过将配体分解为刚性碎片并在蛋白质结合口中重组来预测结合姿态；在 PoseBusters 上达到顶级 Top-1 成功率，显著优于现有深度学习方法，并在未见蛋白上具良好泛化性，首次在 PB 训练-测试分割下超越经典物理对接方法。


<details>
  <summary>Details</summary>
Motivation: 解决生成式对接中化学输出不可可信、泛化能力不足、计算成本高的问题；通过引入结构化先验的分割策略，将配体分解为刚性片段，并利用 SE(3) 的几何性质进行稳定的扩散建模。

Method: 将配体分解为刚性片段；在 SE(3) Riemannian diffusion 框架下学习如何在结合口内重新组装这些片段，生成可解化的配体姿态；简化扩散过程、避免复杂训练不稳定性，利用已有几何先验提升表现和泛化。

Result: 在 PoseBusters 集上，Top-1 成功率（RMSD<2 Å 且 PB-valid）超过 79.9%，显著高于近来深度学习方法的 12.7–30.8%；在未见蛋白上表现出良好泛化。

Conclusion: SigmaDock 首次在 PB 训练-测试分割下超越经典物理对接，标志着将几何先验融入深度学习对接任务的有效性，推动深度学习在分子对接中的可靠性与可行性提升。

Abstract: Determining the binding pose of a ligand to a protein, known as molecular
docking, is a fundamental task in drug discovery. Generative approaches promise
faster, improved, and more diverse pose sampling than physics-based methods,
but are often hindered by chemically implausible outputs, poor
generalisability, and high computational cost. To address these challenges, we
introduce a novel fragmentation scheme, leveraging inductive biases from
structural chemistry, to decompose ligands into rigid-body fragments. Building
on this decomposition, we present SigmaDock, an SE(3) Riemannian diffusion
model that generates poses by learning to reassemble these rigid bodies within
the binding pocket. By operating at the level of fragments in SE(3), SigmaDock
exploits well-established geometric priors while avoiding overly complex
diffusion processes and unstable training dynamics. Experimentally, we show
SigmaDock achieves state-of-the-art performance, reaching Top-1 success rates
(RMSD<2 & PB-valid) above 79.9% on the PoseBusters set, compared to 12.7-30.8%
reported by recent deep learning approaches, whilst demonstrating consistent
generalisation to unseen proteins. SigmaDock is the first deep learning
approach to surpass classical physics-based docking under the PB train-test
split, marking a significant leap forward in the reliability and feasibility of
deep learning for molecular modelling.

</details>


### [24] [Quantum Boltzmann Machines for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2511.04856)
*Thore Gerlach,Michael Schenk,Verena Kain*

Main category: cs.LG

TL;DR: 提出 Continuous Semi-Quantum Boltzmann Machines (CSQBMs)，结合可指数族先验的可见单元与量子 Boltzmann 分布的隐单元，形成混合量子-经典模型，具表达力且可在连续动作的强化学习中直接使用梯度；并通过从 CSQBM 分布采样替代全局最大化，给出一个稳定的连续 Q 学习框架。


<details>
  <summary>Details</summary>
Motivation: 解决连续动作强化学习中的稳定性与样本效率问题，同时在保持较低量子比特需求的前提下提升模型表达能力。

Method: 提出 CSQBMs；在可见单元引入指数族先验，在隐单元上使用量子 Boltzmann 分布，形成混合模型；推导连续变量的解析梯度，便于直接嵌入 Actor-Critic；进一步提出基于 CSQBM 分布采样的连续 Q 学习框架，取代全局最优化。

Result: 理论上实现了对连续动作的强化学习的可操作性，显著降低对量子资源的需求，同时保持较强的表达能力；可解析梯度使与 Actor-Critic 的耦合变得直接；采样式的 Q 学习框架提升稳定性，缓解连续控制中的振荡问题。

Conclusion: CSQBM 为连续动作强化学习提供一种理论扎根的混合量子-经典框架，兼具资源效率与学习稳定性；未来工作可进一步验证在实际任务中的性能、扩展到更广的策略和价值学习设定。

Abstract: We introduce theoretically grounded Continuous Semi-Quantum Boltzmann
Machines (CSQBMs) that supports continuous-action reinforcement learning. By
combining exponential-family priors over visible units with quantum Boltzmann
distributions over hidden units, CSQBMs yield a hybrid quantum-classical model
that reduces qubit requirements while retaining strong expressiveness.
Crucially, gradients with respect to continuous variables can be computed
analytically, enabling direct integration into Actor-Critic algorithms.
Building on this, we propose a continuous Q-learning framework that replaces
global maximization by efficient sampling from the CSQBM distribution, thereby
overcoming instability issues in continuous control.

</details>


### [25] [FoodRL: A Reinforcement Learning Ensembling Framework For In-Kind Food Donation Forecasting](https://arxiv.org/abs/2511.04865)
*Esha Sharma,Lauren Davis,Julie Ivy,Min Chi*

Main category: cs.LG

TL;DR: FoodRL通过RL元学习的自适应集成框架改进食品银行捐赠预测，在灾害波动中表现更稳健，潜在提升年度餐食供给。


<details>
  <summary>Details</summary>
Motivation: 应对高度波动和概念漂移导致的预测不准确，确保资源公平有效分配。

Method: 提出FoodRL，结合强化学习与元学习，聚类并动态加权多模型预测，依据最近表现与上下文信息自适应集成，使用两家结构不同的食品银行数据进行评估。

Result: 在西海岸野火和东海岸飓风的长期数据上持续优于基线，尤其在干扰期表现突出，估算可额外实现约170万份餐食的分配潜力。

Conclusion: 显示其对人道供应链的自适应集成学习价值及潜在社会影响，未来可扩展迁移性和实际部署研究。

Abstract: Food banks are crucial for alleviating food insecurity, but their
effectiveness hinges on accurately forecasting highly volatile in-kind
donations to ensure equitable and efficient resource distribution. Traditional
forecasting models often fail to maintain consistent accuracy due to
unpredictable fluctuations and concept drift driven by seasonal variations and
natural disasters such as hurricanes in the Southeastern U.S. and wildfires in
the West Coast. To address these challenges, we propose FoodRL, a novel
reinforcement learning (RL) based metalearning framework that clusters and
dynamically weights diverse forecasting models based on recent performance and
contextual information. Evaluated on multi-year data from two structurally
distinct U.S. food banks-one large regional West Coast food bank affected by
wildfires and another state-level East Coast food bank consistently impacted by
hurricanes, FoodRL consistently outperforms baseline methods, particularly
during periods of disruption or decline. By delivering more reliable and
adaptive forecasts, FoodRL can facilitate the redistribution of food equivalent
to 1.7 million additional meals annually, demonstrating its significant
potential for social impact as well as adaptive ensemble learning for
humanitarian supply chains.

</details>


### [26] [Self-Interest and Systemic Benefits: Emergence of Collective Rationality in Mixed Autonomy Traffic Through Deep Reinforcement Learning](https://arxiv.org/abs/2511.04883)
*Di Chen,Jia Li,Michael Zhang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autonomous vehicles (AVs) are expected to be commercially available in the
near future, leading to mixed autonomy traffic consisting of both AVs and
human-driven vehicles (HVs). Although numerous studies have shown that AVs can
be deployed to benefit the overall traffic system performance by incorporating
system-level goals into their decision making, it is not clear whether the
benefits still exist when agents act out of self-interest -- a trait common to
all driving agents, both human and autonomous. This study aims to understand
whether self-interested AVs can bring benefits to all driving agents in mixed
autonomy traffic systems. The research is centered on the concept of collective
rationality (CR). This concept, originating from game theory and behavioral
economics, means that driving agents may cooperate collectively even when
pursuing individual interests. Our recent research has proven the existence of
CR in an analytical game-theoretical model and empirically in mixed
human-driven traffic. In this paper, we demonstrate that CR can be attained
among driving agents trained using deep reinforcement learning (DRL) with a
simple reward design. We examine the extent to which self-interested traffic
agents can achieve CR without directly incorporating system-level objectives.
Results show that CR consistently emerges in various scenarios, which indicates
the robustness of this property. We also postulate a mechanism to explain the
emergence of CR in the microscopic and dynamic environment and verify it based
on simulation evidence. This research suggests the possibility of leveraging
advanced learning methods (such as federated learning) to achieve collective
cooperation among self-interested driving agents in mixed-autonomy systems.

</details>


### [27] [You Need Reasoning to Learn Reasoning: The Limitations of Label-Free RL in Weak Base Models](https://arxiv.org/abs/2511.04902)
*Shuvendu Roy,Hossein Hajimirsadeghi,Mengyao Zhai,Golnoosh Samei*

Main category: cs.LG

TL;DR: 对比不同规模（0.5B–7B）模型在无监督强化学习（label-free RL）下的推理强化效果，揭示弱基线模型的局限性并提出基于课程学习的训练策略与数据筛选管线（CuMa）以提升性能，适用于资源受限模型。


<details>
  <summary>Details</summary>
Motivation: 研究面向较小基线模型的无监督RL在推理能力提升中的可迁移性与鲁棒性不足的问题，探索在不依赖外部监督的情况下，如何让不同规模模型也能有效从自反思（self-reflection）中获得推理提升。

Method: 提出一种简单有效的label-free RL方法，结合课程学习（逐步引入更难的问题）并在训练中屏蔽非主导（no-majority）回放；同时构建数据筛选管线以产生具有预设难度的样本，用以系统地验证模型规模对无监督RL的影响。

Result: 在所有尺度的模型上均展现出稳定的改进，尤其显著的是弱模型也能通过课程驱动的训练和数据筛选获得更强的推理能力，从而缓解以往对基线能力的高度依赖问题。

Conclusion: 通过课程化的标签无关RL结合数据难度控制，可以在资源受限模型上实现更鲁棒的推理提升，为无监督RL的可扩展性提供了可行路径，并给出开源实现。

Abstract: Recent advances in large language models have demonstrated the promise of
unsupervised reinforcement learning (RL) methods for enhancing reasoning
capabilities without external supervision. However, the generalizability of
these label-free RL approaches to smaller base models with limited reasoning
capabilities remains unexplored. In this work, we systematically investigate
the performance of label-free RL methods across different model sizes and
reasoning strengths, from 0.5B to 7B parameters. Our empirical analysis reveals
critical limitations: label-free RL is highly dependent on the base model's
pre-existing reasoning capability, with performance often degrading below
baseline levels for weaker models. We find that smaller models fail to generate
sufficiently long or diverse chain-of-thought reasoning to enable effective
self-reflection, and that training data difficulty plays a crucial role in
determining success. To address these challenges, we propose a simple yet
effective method for label-free RL that utilizes curriculum learning to
progressively introduce harder problems during training and mask no-majority
rollouts during training. Additionally, we introduce a data curation pipeline
to generate samples with predefined difficulty. Our approach demonstrates
consistent improvements across all model sizes and reasoning capabilities,
providing a path toward more robust unsupervised RL that can bootstrap
reasoning abilities in resource-constrained models. We make our code available
at https://github.com/BorealisAI/CuMa

</details>


### [28] [A Dual Perspective on Decision-Focused Learning: Scalable Training via Dual-Guided Surrogates](https://arxiv.org/abs/2511.04909)
*Paula Rodriguez-Diaz,Kirk Bansak Elisabeth Paulson*

Main category: cs.LG

TL;DR: Dual-Guided Learning (DGL) leverages downstream dual variables to guide training in predict-then-optimize, enabling periodic optimization to decouple training from heavy solvers and using dual-adjusted targets between refreshes; applied to combinatorial selection problems (matching, knapsack, shortest path); achieves diminishing decision regret, lower runtime, and fewer solver calls, matching or surpassing state-of-the-art DFL.


<details>
  <summary>Details</summary>
Motivation: Real-world decision problems under uncertainty are often solved by predicting inputs and then optimizing. State-of-the-art decision-focused learning requires frequent solver calls or task-specific surrogates, making scaling difficult. There is a need for a scalable approach that preserves decision alignment with cheaper training.

Method: Introduce Dual-Guided Loss (DGL) which uses dual variables from the downstream optimization to shape learning. The optimization is decoupled from gradient updates by solving the downstream problem only periodically; between refreshes, train on dual-adjusted targets using simple differentiable surrogate losses. Applicable to combinatorial one-of-many constraints (matching, knapsack, shortest path). Provide theoretical analysis showing asymptotic diminishing decision regret and runtime complexity; empirical results on two problem classes showing DGL matches or exceeds state-of-the-art DFL with far fewer solver calls and reduced training time; code release.

Result: DGL achieves performance comparable to or better than state-of-the-art decision-focused learning methods on two combinatorial problems, while dramatically reducing solver calls and training time.

Conclusion: DGL offers a scalable, decision-aligned learning framework for predict-then-optimize tasks with combinatorial constraints, reducing solver dependence without sacrificing decision quality; supported by theory and experiments, with open-source implementation.

Abstract: Many real-world decisions are made under uncertainty by solving optimization
problems using predicted quantities. This predict-then-optimize paradigm has
motivated decision-focused learning, which trains models with awareness of how
the optimizer uses predictions, improving the performance of downstream
decisions. Despite its promise, scaling is challenging: state-of-the-art
methods either differentiate through a solver or rely on task-specific
surrogates, both of which require frequent and expensive calls to an optimizer,
often a combinatorial one. In this paper, we leverage dual variables from the
downstream problem to shape learning and introduce Dual-Guided Loss (DGL), a
simple, scalable objective that preserves decision alignment while reducing
solver dependence. We construct DGL specifically for combinatorial selection
problems with natural one-of-many constraints, such as matching, knapsack, and
shortest path. Our approach (a) decouples optimization from gradient updates by
solving the downstream problem only periodically; (b) between refreshes, trains
on dual-adjusted targets using simple differentiable surrogate losses; and (c)
as refreshes become less frequent, drives training cost toward standard
supervised learning while retaining strong decision alignment. We prove that
DGL has asymptotically diminishing decision regret, analyze runtime complexity,
and show on two problem classes that DGL matches or exceeds state-of-the-art
DFL methods while using far fewer solver calls and substantially less training
time. Code is available at https://github.com/paularodr/Dual-Guided-Learning.

</details>


### [29] [Machine Learning Algorithms in Statistical Modelling Bridging Theory and Application](https://arxiv.org/abs/2511.04918)
*A. Ganapathi Rao,Sathish Krishna Anumula,Aditya Kumar Singh,Renukhadevi M,Y. Jeevan Nagendra Kumar,Tammineni Rama Tulasi*

Main category: cs.LG

TL;DR: 将ML算法与传统统计模型融合，显著提升预测性能、鲁棒性、扩展性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 探究机器学习与传统统计建模之间的联系，了解通过混合方法如何“丰富”传统模型，并提升其预测准确性、鲁棒性、可扩展性与解释能力。

Method: 概述多种将ML算法与统计模型整合的途径，并通过实验或理论分析展示混合模型在预测准确性、规模化、灵活性与鲁棒性方面的改进。强调混合模型在实践中的可用性与稳定性。

Result: 混合模型在预测精度、鲁棒性和可解释性等方面表现出显著提升，证明了将现代ML与传统统计方法结合的有效性。

Conclusion: ML与统计模型的混合方法为数据分析、预测和决策提供了更强的工具箱，具有广泛的研究与应用前景。

Abstract: It involves the completely novel ways of integrating ML algorithms with
traditional statistical modelling that has changed the way we analyze data, do
predictive analytics or make decisions in the fields of the data. In this
paper, we study some ML and statistical model connections to understand ways in
which some modern ML algorithms help 'enrich' conventional models; we
demonstrate how new algorithms improve performance, scale, flexibility and
robustness of the traditional models. It shows that the hybrid models are of
great improvement in predictive accuracy, robustness, and interpretability

</details>


### [30] [Leak@$k$: Unlearning Does Not Make LLMs Forget Under Probabilistic Decoding](https://arxiv.org/abs/2511.04934)
*Hadi Reisizadeh,Jiajun Ruan,Yiwei Chen,Soumyadeep Pal,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 现有的“撤记/忘记”方法很难在实际中实现真正的遗忘；在概率解码下，敏感信息会重新出现；提出 leak@k 评估指示重新出现的概率；在 TOFU、MUSE、WMDP 上进行大规模评估，结果显示泄露仍然存在，需更鲁棒的在 LLM 上的 unlearning。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型在合规、隐私和伦理方面的安全性，避免泄露私人、有害、非法或受版权保护的内容；现有的 unlearning 方法在实际应用中无法实现真正的“忘记”，需要更可靠的评估和方法。

Method: 提出 leak@k 这一元评估指标，量化在通常解码策略下从模型生成 k 个样本时，已忘记的知识重新出现的可能性。基于 TOFU、MUSE、WMDP 三个基准，对现有主流 unlearning 方法进行大规模评估，比较确定性解码与随机采样情形下的效果。

Result: 研究发现：知识泄漏在多种方法和任务中持续存在，即使在常用的“已撤记”评估下看似成功，概率解码下才揭示出忘记的脆弱性。leak@k 能揭示这一隐蔽风险，表明当前最先进的 unlearning 技术仅提供有限的遗忘，需要更鲁棒的解决方案。

Conclusion: 当前的 unlearning 技术在现实场景中仍不足以保证真正的忘记，应发展更强的去记忆策略并采用更贴近实际解码条件的评估框架（如 leak@k）来验证鲁棒性。

Abstract: Unlearning in large language models (LLMs) is critical for regulatory
compliance and for building ethical generative AI systems that avoid producing
private, toxic, illegal, or copyrighted content. Despite rapid progress, in
this work we show that \textit{almost all} existing unlearning methods fail to
achieve true forgetting in practice. Specifically, while evaluations of these
`unlearned' models under deterministic (greedy) decoding often suggest
successful knowledge removal using standard benchmarks (as has been done in the
literature), we show that sensitive information reliably resurfaces when models
are sampled with standard probabilistic decoding. To rigorously capture this
vulnerability, we introduce \texttt{leak@$k$}, a new meta-evaluation metric
that quantifies the likelihood of forgotten knowledge reappearing when
generating $k$ samples from the model under realistic decoding strategies.
Using three widely adopted benchmarks, TOFU, MUSE, and WMDP, we conduct the
first large-scale, systematic study of unlearning reliability using our newly
defined \texttt{leak@$k$} metric. Our findings demonstrate that knowledge
leakage persists across methods and tasks, underscoring that current
state-of-the-art unlearning techniques provide only limited forgetting and
highlighting the urgent need for more robust approaches to LLM unlearning.

</details>


### [31] [Structural Properties, Cycloid Trajectories and Non-Asymptotic Guarantees of EM Algorithm for Mixed Linear Regression](https://arxiv.org/abs/2511.04937)
*Zhankun Luo,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work investigates the structural properties, cycloid trajectories, and
non-asymptotic convergence guarantees of the Expectation-Maximization (EM)
algorithm for two-component Mixed Linear Regression (2MLR) with unknown mixing
weights and regression parameters. Recent studies have established global
convergence for 2MLR with known balanced weights and super-linear convergence
in noiseless and high signal-to-noise ratio (SNR) regimes. However, the
theoretical behavior of EM in the fully unknown setting remains unclear, with
its trajectory and convergence order not yet fully characterized. We derive
explicit EM update expressions for 2MLR with unknown mixing weights and
regression parameters across all SNR regimes and analyze their structural
properties and cycloid trajectories. In the noiseless case, we prove that the
trajectory of the regression parameters in EM iterations traces a cycloid by
establishing a recurrence relation for the sub-optimality angle, while in high
SNR regimes we quantify its discrepancy from the cycloid trajectory. The
trajectory-based analysis reveals the order of convergence: linear when the EM
estimate is nearly orthogonal to the ground truth, and quadratic when the angle
between the estimate and ground truth is small at the population level. Our
analysis establishes non-asymptotic guarantees by sharpening bounds on
statistical errors between finite-sample and population EM updates, relating
EM's statistical accuracy to the sub-optimality angle, and proving convergence
with arbitrary initialization at the finite-sample level. This work provides a
novel trajectory-based framework for analyzing EM in Mixed Linear Regression.

</details>


### [32] [Risk Prediction of Cardiovascular Disease for Diabetic Patients with Machine Learning and Deep Learning Techniques](https://arxiv.org/abs/2511.04971)
*Esha Chowdhury*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate prediction of cardiovascular disease (CVD) risk is crucial for
healthcare institutions. This study addresses the growing prevalence of
diabetes and its strong link to heart disease by proposing an efficient CVD
risk prediction model for diabetic patients using machine learning (ML) and
hybrid deep learning (DL) approaches. The BRFSS dataset was preprocessed by
removing duplicates, handling missing values, identifying categorical and
numerical features, and applying Principal Component Analysis (PCA) for feature
extraction. Several ML models, including Decision Trees (DT), Random Forest
(RF), k-Nearest Neighbors (KNN), Support Vector Machine (SVM), AdaBoost, and
XGBoost, were implemented, with XGBoost achieving the highest accuracy of
0.9050. Various DL models, such as Artificial Neural Networks (ANN), Deep
Neural Networks (DNN), Recurrent Neural Networks (RNN), Convolutional Neural
Networks (CNN), Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), and
Gated Recurrent Unit (GRU), as well as hybrid models combining CNN with LSTM,
BiLSTM, and GRU, were also explored. Some of these models achieved perfect
recall (1.00), with the LSTM model achieving the highest accuracy of 0.9050.
Our research highlights the effectiveness of ML and DL models in predicting CVD
risk among diabetic patients, automating and enhancing clinical
decision-making. High accuracy and F1 scores demonstrate these models'
potential to improve personalized risk management and preventive strategies.

</details>


### [33] [Less Is More: Generating Time Series with LLaMA-Style Autoregression in Simple Factorized Latent Spaces](https://arxiv.org/abs/2511.04973)
*Siyuan Li,Yifan Sun,Lei Cheng,Lewen Wang,Yang Liu,Weiqing Liu,Jianlong Li,Jiang Bian,Shikai Fang*

Main category: cs.LG

TL;DR: FAR-TS 通过把多变量时间序列分解为数据自适应基底和离散化的时间系数，再用类 LLaMA 的自回归 Transformer 在离散令牌上建模，实现快速、可控、任意长度的时间序列生成，显著快于扩散式方法Diffusion-TS，同时保持跨通道相关性和可解释的潜在空间。


<details>
  <summary>Details</summary>
Motivation: 解决基于扩散的时序生成在速度和固定长度窗口方面的局限，提供一个快速、可控且可扩展的时序生成框架，以用于数据增强、仿真和隐私保护。

Method: 将时间序列分解为捕捉静态跨通道相关性的数据自适应基底，与表示时间系数的离散量化令牌；使用类似 LLaMA 的自回归 Transformer 对这些离散令牌序列进行建模，实现任意长度、可控的序列生成。

Result: 相比 Diffusion-TS，FAR-TS 达到数量级的更快生成速度；在保持跨通道相关性和可解释潜在空间的同时，提供高质量的时序合成并具备灵活的长度控制。

Conclusion: 设计简洁高效，FAR-TS 展现出强大性能及广泛适用性，适用于需要快速、可控且可扩展的多通道时间序列生成的场景。

Abstract: Generative models for multivariate time series are essential for data
augmentation, simulation, and privacy preservation, yet current
state-of-the-art diffusion-based approaches are slow and limited to
fixed-length windows. We propose FAR-TS, a simple yet effective framework that
combines disentangled factorization with an autoregressive Transformer over a
discrete, quantized latent space to generate time series. Each time series is
decomposed into a data-adaptive basis that captures static cross-channel
correlations and temporal coefficients that are vector-quantized into discrete
tokens. A LLaMA-style autoregressive Transformer then models these token
sequences, enabling fast and controllable generation of sequences with
arbitrary length. Owing to its streamlined design, FAR-TS achieves
orders-of-magnitude faster generation than Diffusion-TS while preserving
cross-channel correlations and an interpretable latent space, enabling
high-quality and flexible time series synthesis.

</details>


### [34] [Deep Progressive Training: scaling up depth capacity of zero/one-layer models](https://arxiv.org/abs/2511.04981)
*Zhiqi Bu*

Main category: cs.LG

TL;DR: 提出零/一层渐进训练以在保持性能的前提下降低训练成本，通过分阶段扩展模型深度，GPT-2 实验显示显著的计算节省（约80%）并接近完全训练60层模型的损失。


<details>
  <summary>Details</summary>
Motivation: 深层模型提升准确性但计算成本高；需要在训练阶段动态扩展模型容量以实现高效训练。

Method: 从优化理论和特征学习角度分析新层初始化、超参数迁移、学习率调度和扩展时机，提出零/一层渐进训练策略。

Result: 在GPT-2 60层、7B参数的对比中，零/一层渐进训练可节省约80%计算，等效地提升约5x训练速度，同时损失几乎相同。

Conclusion: 渐进扩展深度是权衡计算与模型性能的有效策略，零/一层策略实现了高效训练的可行性。

Abstract: Model depth is a double-edged sword in deep learning: deeper models achieve
higher accuracy but require higher computational cost. To efficiently train
models at scale, an effective strategy is the progressive training, which
scales up model capacity during training, hence significantly reducing
computation with little to none performance degradation. In this work, we study
the depth expansion of large models through the lens of optimization theory and
feature learning, offering insights on the initialization of new layers,
hyperparameter transfer, learning rate schedule, and timing of model expansion.
Specifically, we propose zero/one-layer progressive training for the optimal
tradeoff between computation and loss. For example, zero/one-layer progressive
training on GPT2 can save $\approx 80\%$ compute, or equivalently accelerate
$\approx 5\times$ while achieving almost the same loss, compared to to a fully
trained 60-layer model with 7B parameters.

</details>


### [35] [Peptide2Mol: A Diffusion Model for Generating Small Molecules as Peptide Mimics for Targeted Protein Binding](https://arxiv.org/abs/2511.04984)
*Xinheng He,Yijia Zhang,Haowei Lin,Xingang Peng,Xiangzhe Kong,Mingyu Li,Jianzhu Ma*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Structure-based drug design has seen significant advancements with the
integration of artificial intelligence (AI), particularly in the generation of
hit and lead compounds. However, most AI-driven approaches neglect the
importance of endogenous protein interactions with peptides, which may result
in suboptimal molecule designs. In this work, we present Peptide2Mol, an
E(3)-equivariant graph neural network diffusion model that generates small
molecules by referencing both the original peptide binders and their
surrounding protein pocket environments. Trained on large datasets and
leveraging sophisticated modeling techniques, Peptide2Mol not only achieves
state-of-the-art performance in non-autoregressive generative tasks, but also
produces molecules with similarity to the original peptide binder.
Additionally, the model allows for molecule optimization and peptidomimetic
design through a partial diffusion process. Our results highlight Peptide2Mol
as an effective deep generative model for generating and optimizing bioactive
small molecules from protein binding pockets.

</details>


### [36] [Carbon Price Forecasting with Structural Breaks: A Comparative Study of Deep Learning Models](https://arxiv.org/abs/2511.04988)
*Runsheng Ren,Jing Li,Yanxiu Li,Shixun Huang,Jun Shen,Wanqing Li,John Le,Sheng Wang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurately forecasting carbon prices is essential for informed energy market
decision-making, guiding sustainable energy planning, and supporting effective
decarbonization strategies. However, it remains challenging due to structural
breaks and high-frequency noise caused by frequent policy interventions and
market shocks. Existing studies, including the most recent baseline approaches,
have attempted to incorporate breakpoints but often treat denoising and
modeling as separate processes and lack systematic evaluation across advanced
deep learning architectures, limiting the robustness and the generalization
capability. To address these gaps, this paper proposes a comprehensive hybrid
framework that integrates structural break detection (Bai-Perron, ICSS, and
PELT algorithms), wavelet signal denoising, and three state-of-the-art deep
learning models (LSTM, GRU, and TCN). Using European Union Allowance (EUA) spot
prices from 2007 to 2024 and exogenous features such as energy prices and
policy indicators, the framework constructs univariate and multivariate
datasets for comparative evaluation. Experimental results demonstrate that our
proposed PELT-WT-TCN achieves the highest prediction accuracy, reducing
forecasting errors by 22.35% in RMSE and 18.63% in MAE compared to the
state-of-the-art baseline model (Breakpoints with Wavelet and LSTM), and by
70.55% in RMSE and 74.42% in MAE compared to the original LSTM without
decomposition from the same baseline study. These findings underscore the value
of integrating structural awareness and multiscale decomposition into deep
learning architectures to enhance accuracy and interpretability in carbon price
forecasting and other nonstationary financial time series.

</details>


### [37] [BiPETE: A Bi-Positional Embedding Transformer Encoder for Risk Assessment of Alcohol and Substance Use Disorder with Electronic Health Records](https://arxiv.org/abs/2511.04998)
*Daniel S. Lee,Mayra S. Haedo-Cruz,Chen Jiang,Oshin Miranda,LiRong Wang*

Main category: cs.LG

TL;DR: BiPETE将双重位置编码引入Transformer以处理EHR中的不规则就诊时间，从而提升单病种风险预测的性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统EHR时间序列建模受就诊间隔不规则和结构缺失影响，且大型预训练成本高。该研究希望在无需大规模预训练的前提下，利用双重位置信息提升预测准确性并实现可解释性。

Method: 提出Bi-Positional Embedding Transformer Encoder (BiPETE)，在Transformer中结合旋转位置编码来编码相对就诊时间，和正弦位置编码来保留就诊顺序；在两个心理健康队列（抑郁症和PTSD）上训练，用于预测ASUD风险；使用Integrated Gradients进行特征解释。

Result: BiPETE在抑郁与PTSD队列中相比基线模型提升AUPRC，分别提高34%和50%。通过消融实验验证双位置编码策略有效；引入Integrated Gradients揭示与ASUD风险相关的临床特征（如炎性、血液学、代谢指标异常，以及特定药物和共病）。

Conclusion: 该研究提供一个实用且可解释的EHR风险预测框架，具有较强的预测性能，为ASUD风险评估与干预提供线索。

Abstract: Transformer-based deep learning models have shown promise for disease risk
prediction using electronic health records(EHRs), but modeling temporal
dependencies remains a key challenge due to irregular visit intervals and lack
of uniform structure. We propose a Bi-Positional Embedding Transformer Encoder
or BiPETE for single-disease prediction, which integrates rotary positional
embeddings to encode relative visit timing and sinusoidal embeddings to
preserve visit order. Without relying on large-scale pretraining, BiPETE is
trained on EHR data from two mental health cohorts-depressive disorder and
post-traumatic stress disorder (PTSD)-to predict the risk of alcohol and
substance use disorders (ASUD). BiPETE outperforms baseline models, improving
the area under the precision-recall curve (AUPRC) by 34% and 50% in the
depression and PTSD cohorts, respectively. An ablation study further confirms
the effectiveness of the dual positional encoding strategy. We apply the
Integrated Gradients method to interpret model predictions, identifying key
clinical features associated with ASUD risk and protection, such as abnormal
inflammatory, hematologic, and metabolic markers, as well as specific
medications and comorbidities. Overall, these key clinical features identified
by the attribution methods contribute to a deeper understanding of the risk
assessment process and offer valuable clues for mitigating potential risks. In
summary, our study presents a practical and interpretable framework for disease
risk prediction using EHR data, which can achieve strong performance.

</details>


### [38] [Multi-agent Coordination via Flow Matching](https://arxiv.org/abs/2511.05005)
*Dongsu Lee,Daehee Lee,Amy Zhang*

Main category: cs.LG

TL;DR: MAC-Flow 在离线多智能体强化学习中，先学习一个用于描述联合行为的流模型，再蒸馏成去中心化的一步策略，既保留协调性又实现快速推断；相比扩散式方法推断快约 14.5x，且速度接近现有高斯策略的离线 MARL 方法，同时保持良好性能。


<details>
  <summary>Details</summary>
Motivation: 需要在离线数据中获得对多智能体协同行为的丰富表示，并在实时或近实时环境中高效执行。现有扩散模型虽表达力强但推理慢，高斯策略虽快但对多智能体交互的鲁棒性较弱，存在性能与计算成本之间的权衡。

Method: 首先学习一个流式表示来建模联合行为的分布，然后将该表示蒸馏到去中心化的一步策略，以实现快速且协同的执行。

Result: 在四个基准、涵盖 12 个环境和 34 组数据集上，MAC-Flow 的推断成本约降低 14.5 倍，相较扩散式 MARL 方法；同时保持良好性能，推断速度与既有高斯策略的离线 MARL 方法相近。

Conclusion: MAC-Flow 有效缓解了性能与计算成本之间的折中，提供一个在表达力与执行效率之间取得平衡的离线多智能体协作框架。

Abstract: This work presents MAC-Flow, a simple yet expressive framework for
multi-agent coordination. We argue that requirements of effective coordination
are twofold: (i) a rich representation of the diverse joint behaviors present
in offline data and (ii) the ability to act efficiently in real time. However,
prior approaches often sacrifice one for the other, i.e., denoising
diffusion-based solutions capture complex coordination but are computationally
slow, while Gaussian policy-based solutions are fast but brittle in handling
multi-agent interaction. MAC-Flow addresses this trade-off by first learning a
flow-based representation of joint behaviors, and then distilling it into
decentralized one-step policies that preserve coordination while enabling fast
execution. Across four different benchmarks, including $12$ environments and
$34$ datasets, MAC-Flow alleviates the trade-off between performance and
computational cost, specifically achieving about $\boldsymbol{\times14.5}$
faster inference compared to diffusion-based MARL methods, while maintaining
good performance. At the same time, its inference speed is similar to that of
prior Gaussian policy-based offline multi-agent reinforcement learning (MARL)
methods.

</details>


### [39] [OvA-LP: A Simple and Efficient Framework for Federated Learning on Non-IID Data](https://arxiv.org/abs/2511.05028)
*Dongjin Park,Hasung Yeo,Joon-Woo Lee*

Main category: cs.LG

TL;DR: OvA-LP是一种在联邦微调框架（FFT）中从源头抑制漂移的极简框架。通过对冻结编码器进行线性探测并使用一-vs-all分类头，在两阶段流程中保持特征几何、解耦 logits，显著提升对非IID数据的鲁棒性与效率。


<details>
  <summary>Details</summary>
Motivation: 在分布异质的客户端环境下，FFT易受局部漂移影响，导致全局模型偏差放大；现有聚合与个性化方法多在事后纠正漂移，极端非IID条件下脆弱。因此需要一种在源头就抑制漂移的高效方法，且兼容PEFT（参数高效微调）框架。

Method: 在冻结的编码器上进行线性探测（线性层只训练探针），采用一对一多分类头（one-vs-all head），并通过一个两阶段流程完成微调；保持预训练特征几何，解耦 logits 以防止放大漂移的机制；通过预计算编码器特征降低每轮成本，使成本与编码器大小近似独立。

Result: 在CIFAR-100/100客户端的实验中，OvA-LP在IID条件下保持95.9%的准确率，而同条件下的FFT基线PFPT仅保持10.1%，FFT-MoE为34.5%；在对称与非对称标签噪声下也具备鲁棒性；预计算特征使得每轮成本几乎与编码器大小无关。

Conclusion: OvA-LP为在异质环境下进行鲁棒FFT提供了一个具有原理性与高效性的基线，能够从源头抑制漂移，提升在非IID数据上的稳定性与性能。

Abstract: Federated fine-tuning (FFT) adapts foundation models to decentralized data
but remains fragile under heterogeneous client distributions due to local
drift, i.e., client-level update divergences that induce systematic bias and
amplified variance in the global model. Existing aggregation and
personalization methods largely correct drift post hoc, which proves brittle
under extreme non-IID conditions. We introduce OvA-LP, a minimalist framework
that is, to our knowledge, the first explicitly designed to suppress drift at
its source within the PEFT-based FFT paradigm. OvA-LP combines linear probing
on a frozen encoder with a one-vs-all head and a simple two-stage procedure,
preserving pretrained feature geometry and decoupling logits to prevent the
mechanisms that amplify drift. On CIFAR-100 with 100 clients, averaged over
shard-1, shard-2, and Bernoulli-Dirichlet partitions, OvA-LP retains 95.9% of
its IID accuracy, whereas state-of-the-art FFT baselines retain only 10.1%
(PFPT) and 34.5% (FFT-MoE) under the same conditions. OvA-LP further maintains
resilience under both symmetric and asymmetric label noise. In addition,
precomputing encoder features makes per-round cost nearly independent of
encoder size. Together, these results demonstrate that OvA-LP provides a
principled and efficient basis for robust FFT under heterogeneity.

</details>


### [40] [Usando LLMs para Programar Jogos de Tabuleiro e Variações](https://arxiv.org/abs/2511.05114)
*Álvaro Guglielmin Becker,Lana Bertoldo Rossato,Anderson Rocha Tavares*

Main category: cs.LG

TL;DR: 评估 Claude、DeepSeek、ChatGPT 在为棋盘游戏生成代码及其变体方面的能力


<details>
  <summary>Details</summary>
Motivation: 希望通过LLMs降低棋盘游戏代码实现的时间成本，提升开发效率

Method: 提出用于测试三种LLM在棋盘游戏代码生成与变体开发中的能力的评测方法框架/流程

Result: 摘要未提供实验结果或定量评估，需阅读全文获取结果

Conclusion: 若方法有效，表明LLMs对棋盘游戏编码有潜在帮助，但具体有效性需进一步实验验证

Abstract: Creating programs to represent board games can be a time-consuming task.
Large Language Models (LLMs) arise as appealing tools to expedite this process,
given their capacity to efficiently generate code from simple contextual
information. In this work, we propose a method to test how capable three LLMs
(Claude, DeepSeek and ChatGPT) are at creating code for board games, as well as
new variants of existing games.

</details>


### [41] [QuAnTS: Question Answering on Time Series](https://arxiv.org/abs/2511.05124)
*Felix Divo,Maurice Kraus,Anh Q. Nguyen,Hao Xue,Imran Razzak,Flora D. Salim,Kristian Kersting,Devendra Singh Dhami*

Main category: cs.LG

TL;DR: 提出 QuAnTS，一个面向时间序列问答的的大规模数据集，聚焦骨架轨迹的人类动作，提供基线与人类性能以促进TSQA研究。


<details>
  <summary>Details</summary>
Motivation: 现有问答研究多聚焦于视觉与文本，对时间序列的问答关注不足，缺乏标准数据集和评估基线；文本接入时间序列有助于可访问性、决策和系统透明度，因此需要大规模、结构化的TSQA数据集。

Method: 设计并公开 QuAnTS 数据集，覆盖多样化的关于时间序列数据（骨架轨迹）的问答；进行广泛实验以验证数据质量与覆盖；评估现有与新提出的基线，提供人工基准作为参考。

Result: 证实 QuAnTS 是一个高质量、覆盖广泛的大规模数据集；基线评估揭示当前方法的能力与局限，提供了后续研究的起点；同时给出人类表现以作为实用参考。

Conclusion: 本文为通过文本与时间序列交互进行问答的研究奠定基石，推动在时间序列模型上用文本进行解释和决策，促成更透明和可用的系统。

Abstract: Text offers intuitive access to information. This can, in particular,
complement the density of numerical time series, thereby allowing improved
interactions with time series models to enhance accessibility and
decision-making. While the creation of question-answering datasets and models
has recently seen remarkable growth, most research focuses on question
answering (QA) on vision and text, with time series receiving minute attention.
To bridge this gap, we propose a challenging novel time series QA (TSQA)
dataset, QuAnTS, for Question Answering on Time Series data. Specifically, we
pose a wide variety of questions and answers about human motion in the form of
tracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is
well-formed and comprehensive through extensive experiments. Thoroughly
evaluating existing and newly proposed baselines then lays the groundwork for a
deeper exploration of TSQA using QuAnTS. Additionally, we provide human
performances as a key reference for gauging the practical usability of such
models. We hope to encourage future research on interacting with time series
models through text, enabling better decision-making and more transparent
systems.

</details>


### [42] [Consecutive Preferential Bayesian Optimization](https://arxiv.org/abs/2511.05163)
*Aras Erarslan,Carlos Sevilla Salcedo,Ville Tanskanen,Anni Nisov,Eero Päiväkumpu,Heikki Aisala,Kaisu Honkapää,Arto Klami,Petrus Mikkola*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Preferential Bayesian optimization allows optimization of objectives that are
either expensive or difficult to measure directly, by relying on a minimal
number of comparative evaluations done by a human expert. Generating candidate
solutions for evaluation is also often expensive, but this cost is ignored by
existing methods. We generalize preference-based optimization to explicitly
account for production and evaluation costs with Consecutive Preferential
Bayesian Optimization, reducing production cost by constraining comparisons to
involve previously generated candidates. We also account for the perceptual
ambiguity of the oracle providing the feedback by incorporating a
Just-Noticeable Difference threshold into a probabilistic preference model to
capture indifference to small utility differences. We adapt an
information-theoretic acquisition strategy to this setting, selecting new
configurations that are most informative about the unknown optimum under a
preference model accounting for the perceptual ambiguity. We empirically
demonstrate a notable increase in accuracy in setups with high production costs
or with indifference feedback.

</details>


### [43] [Model Merging Improves Zero-Shot Generalization in Bioacoustic Foundation Models](https://arxiv.org/abs/2511.05171)
*Davide Marincione,Donato Crisostomi,Roberto Dessi,Emanuele Rodolà,Emanuele Rossi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Foundation models capable of generalizing across species and tasks represent
a promising new frontier in bioacoustics, with NatureLM being one of the most
prominent examples. While its domain-specific fine-tuning yields strong
performance on bioacoustic benchmarks, we observe that it also introduces
trade-offs in instruction-following flexibility. For instance, NatureLM
achieves high accuracy when prompted for either the common or scientific name
individually, but its accuracy drops significantly when both are requested in a
single prompt. We address this by applying a simple model merging strategy that
interpolates NatureLM with its base language model, recovering
instruction-following capabilities with minimal loss of domain expertise.
Finally, we show that the merged model exhibits markedly stronger zero-shot
generalization, achieving over a 200% relative improvement and setting a new
state-of-the-art in closed-set zero-shot classification of unseen species.

</details>


### [44] [Associative Poisoning to Generative Machine Learning](https://arxiv.org/abs/2511.05177)
*Mathias Lundteigen Mohus,Jingyue Li,Zhirong Yang*

Main category: cs.LG

TL;DR: 提出名为关联中毒的无训练过程控制的数据中毒攻击，通过扰动训练数据来操纵生成输出中的特征对统计关联，同时保持边际分布和输出质量，具隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 生成模型的广泛应用带来安全威胁；现有污染攻击要么导致广泛退化，要么需要对训练过程的控制，限制现实场景。

Method: 给出形式化攻击模型，理论证明其可行性与隐蔽性；通过对训练数据的微扰来改变输出特征对的统计关联；在两种尖端生成模型上进行实验。

Result: 能够显著改变特征对的关联性，同时保持目标特征的边际分布及输出质量，难以通过可视化检测发现；证据来自两种模型实验。

Conclusion: 揭示对统计完整性的潜在威胁，讨论现有防御的局限性，提出新型防御框架的方向。

Abstract: The widespread adoption of generative models such as Stable Diffusion and
ChatGPT has made them increasingly attractive targets for malicious
exploitation, particularly through data poisoning. Existing poisoning attacks
compromising synthesised data typically either cause broad degradation of
generated data or require control over the training process, limiting their
applicability in real-world scenarios. In this paper, we introduce a novel data
poisoning technique called associative poisoning, which compromises
fine-grained features of the generated data without requiring control of the
training process. This attack perturbs only the training data to manipulate
statistical associations between specific feature pairs in the generated
outputs. We provide a formal mathematical formulation of the attack and prove
its theoretical feasibility and stealthiness. Empirical evaluations using two
state-of-the-art generative models demonstrate that associative poisoning
effectively induces or suppresses feature associations while preserving the
marginal distributions of the targeted features and maintaining high-quality
outputs, thereby evading visual detection. These results suggest that
generative systems used in image synthesis, synthetic dataset generation, and
natural language processing are susceptible to subtle, stealthy manipulations
that compromise their statistical integrity. To address this risk, we examine
the limitations of existing defensive strategies and propose a novel
countermeasure strategy.

</details>


### [45] [No One-Model-Fits-All: Uncovering Spatio-Temporal Forecasting Trade-offs with Graph Neural Networks and Foundation Models](https://arxiv.org/abs/2511.05179)
*Ragini Gupta,Naman Raina,Bo Chen,Li Chen,Claudiu Danilov,Josh Eckhardt,Keyshla Bernard,Klara Nahrstedt*

Main category: cs.LG

TL;DR: 一项系统性比较研究，评估 VAR、GRU、Transformer、STGNNs 和时间序列基金模（TSFMs：Chronos Moirai、TimesFM）在不同传感器密度与采样间隔下的天气温度预测表现。结论是：在传感器稀疏、采样频率中等时，STGNNs 最优；高频场景 TSFMs 具竞争力但在空间覆盖减少时退化；多变量 TSFM Moirai 能原生学习跨传感器依赖，整体超越其他模型；并提供开源代码以便复现实验。


<details>
  <summary>Details</summary>
Motivation: 探索采样频率与空间覆盖对下游时空预测模型性能的影响，弥补现有工作多关注数据体量与边缘过滤，而忽视模型架构对不同传感配置的响应的空白。

Method: 在真实世界的无线传感网温度数据上，比较经典VAR、GRU、Transformer、STGNNs以及TSFMs（Chronos Moirai、TimesFM）在不同的空间传感节点密度和采样间隔设置下的预测效果，评估各自对时空建模的优势，同时公开模型配置、数据集与训练日志以便复现。

Result: 当传感部署稀疏且采样率为中等时，STGNNs 能通过编码的图结构有效利用空间相关性来缓解覆盖不足；TSFMs 在高频场景表现竞争力，但邻近传感器覆盖下降时性能下降；多变量TSFM Moirai 在跨传感器关系学习方面表现最佳，超越其他模型。

Conclusion: 为构建高效的时空预测管线提供可操作的设计洞见，研究并开源了模型配置、数据集和日志，便于复现实验与比较。

Abstract: Modern IoT deployments for environmental sensing produce high volume
spatiotemporal data to support downstream tasks such as forecasting, typically
powered by machine learning models. While existing filtering and strategic
deployment techniques optimize collected data volume at the edge, they overlook
how variations in sampling frequencies and spatial coverage affect downstream
model performance. In many forecasting models, incorporating data from
additional sensors denoise predictions by providing broader spatial contexts.
This interplay between sampling frequency, spatial coverage and different
forecasting model architectures remain underexplored. This work presents a
systematic study of forecasting models - classical models (VAR), neural
networks (GRU, Transformer), spatio-temporal graph neural networks (STGNNs),
and time series foundation models (TSFMs: Chronos Moirai, TimesFM) under
varying spatial sensor nodes density and sampling intervals using real-world
temperature data in a wireless sensor network. Our results show that STGNNs are
effective when sensor deployments are sparse and sampling rate is moderate,
leveraging spatial correlations via encoded graph structure to compensate for
limited coverage. In contrast, TSFMs perform competitively at high frequencies
but degrade when spatial coverage from neighboring sensors is reduced.
Crucially, the multivariate TSFM Moirai outperforms all models by natively
learning cross-sensor dependencies. These findings offer actionable insights
for building efficient forecasting pipelines in spatio-temporal systems. All
code for model configurations, training, dataset, and logs are open-sourced for
reproducibility:
https://github.com/UIUC-MONET-Projects/Benchmarking-Spatiotemporal-Forecast-Models

</details>


### [46] [ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy](https://arxiv.org/abs/2511.05221)
*David Bertram,Anja Ophey,Sinah Röttgen,Konstantin Kuffer,Gereon R. Fink,Elke Kalbe,Clint Hansen,Walter Maetzler,Maximilian Kapsecker,Lara M. Reimer,Stephan Jonas,Andreas T. Damgaard,Natasha B. Bertelsen,Casper Skjaerbaek,Per Borghammer,Karolien Groenewald,Pietro-Luca Ratti,Michele T. Hu,No émie Moreau,Michael Sommerauer,Katarzyna Bozek*

Main category: cs.LG

TL;DR: ActiTect is a fully automated, open-source ML tool to identify RBD from actigraphy, showing robust cross-dataset performance in multi-center cohorts.


<details>
  <summary>Details</summary>
Motivation: RBD is a key prodromal marker of alpha-synucleinopathies; reliable, scalable detection from wearables requires a robust, generalizable pipeline across heterogeneous data.

Method: ActiTect pipeline with robust preprocessing, automated sleep-wake detection, and interpretable motion features. Developed on 78 individuals with nested cross-validation (AUROC 0.95); generalization tested on blinded local set (n=31, AUROC 0.86) and two external cohorts (n=113 AUROC 0.84; n=57 AUROC 0.94); leave-one-dataset-out CV shows AUROC 0.84–0.89; stability analysis confirms feature reproducibility.

Result: Nested CV AUROC 0.95; blinded local AUROC 0.86; external cohorts AUROCs 0.84 and 0.94; leave-one-dataset-out CV AUROC 0.84–0.89; core predictive features stable across datasets.

Conclusion: Open-source, user-friendly tool enables widespread adoption, independent validation, and progression toward a unified, generalizable RBD detection model using wearables.

Abstract: Isolated rapid eye movement sleep behavior disorder (iRBD) is a major
prodromal marker of $\alpha$-synucleinopathies, often preceding the clinical
onset of Parkinson's disease, dementia with Lewy bodies, or multiple system
atrophy. While wrist-worn actimeters hold significant potential for detecting
RBD in large-scale screening efforts by capturing abnormal nocturnal movements,
they become inoperable without a reliable and efficient analysis pipeline. This
study presents ActiTect, a fully automated, open-source machine learning tool
to identify RBD from actigraphy recordings. To ensure generalizability across
heterogeneous acquisition settings, our pipeline includes robust preprocessing
and automated sleep-wake detection to harmonize multi-device data and extract
physiologically interpretable motion features characterizing activity patterns.
Model development was conducted on a cohort of 78 individuals, yielding strong
discrimination under nested cross-validation (AUROC = 0.95). Generalization was
confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two
independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To
assess real-world robustness, leave-one-dataset-out cross-validation across the
internal and external cohorts demonstrated consistent performance (AUROC range
= 0.84-0.89). A complementary stability analysis showed that key predictive
features remained reproducible across datasets, supporting the final pooled
multi-center model as a robust pre-trained resource for broader deployment. By
being open-source and easy to use, our tool promotes widespread adoption and
facilitates independent validation and collaborative improvements, thereby
advancing the field toward a unified and generalizable RBD detection model
using wearable devices.

</details>


### [47] [The Causal Round Trip: Generating Authentic Counterfactuals by Eliminating Information Loss](https://arxiv.org/abs/2511.05236)
*Rui Wu,Lizheng Wang,Yongjun Li*

Main category: cs.LG

TL;DR: 提出Zero-SRE扩散框架BELM-MDCM，通过满足CIC实现忠实Abduction，消除结构重建误差，提升个体级反事实推理的保真性。


<details>
  <summary>Details</summary>
Motivation: 长期以来，复杂非线性机制中的忠实Abduction难以实现；扩散模型虽强，但其设计在推理任务中导致信息损失，形成结构重建误差（SRE），阻碍因果推理的精确性。

Method: 提出Causal Information Conservation (CIC)作为忠实Abduction的必要条件；设计BELM-MDCM作为首个通过解析可逆机制消除SRE的扩散框架；引入Targeted Modeling策略进行结构正则化；采用Hybrid Training Objective来强化因果归纳偏置。

Result: 在多组实验中，Zero-SRE框架达到state-of-the-art准确度，并实现高保真、个体级反事实推断，提升因果研究的可行性。

Conclusion: 提供一个基础蓝图，将现代生成模型的能力与经典因果理论的严格性结合起来，为该领域建立更严格的新标准。

Abstract: Judea Pearl's vision of Structural Causal Models (SCMs) as engines for
counterfactual reasoning hinges on faithful abduction: the precise inference of
latent exogenous noise. For decades, operationalizing this step for complex,
non-linear mechanisms has remained a significant computational challenge. The
advent of diffusion models, powerful universal function approximators, offers a
promising solution. However, we argue that their standard design, optimized for
perceptual generation over logical inference, introduces a fundamental flaw for
this classical problem: an inherent information loss we term the Structural
Reconstruction Error (SRE). To address this challenge, we formalize the
principle of Causal Information Conservation (CIC) as the necessary condition
for faithful abduction. We then introduce BELM-MDCM, the first diffusion-based
framework engineered to be causally sound by eliminating SRE by construction
through an analytically invertible mechanism. To operationalize this framework,
a Targeted Modeling strategy provides structural regularization, while a Hybrid
Training Objective instills a strong causal inductive bias. Rigorous
experiments demonstrate that our Zero-SRE framework not only achieves
state-of-the-art accuracy but, more importantly, enables the high-fidelity,
individual-level counterfactuals required for deep causal inquiries. Our work
provides a foundational blueprint that reconciles the power of modern
generative models with the rigor of classical causal theory, establishing a new
and more rigorous standard for this emerging field.

</details>


### [48] [An End-to-End Deep Reinforcement Learning Approach for Solving the Traveling Salesman Problem with Drones](https://arxiv.org/abs/2511.05265)
*Taihelong Zeng,Yun Lin,Yuhe Shi,Yan Li,Zhiqing Wei,Xuanru Ji*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The emergence of truck-drone collaborative systems in last-mile logistics has
positioned the Traveling Salesman Problem with Drones (TSP-D) as a pivotal
extension of classical routing optimization, where synchronized vehicle
coordination promises substantial operational efficiency and reduced
environmental impact, yet introduces NP-hard combinatorial complexity beyond
the reach of conventional optimization paradigms. Deep reinforcement learning
offers a theoretically grounded framework to address TSP-D's inherent
challenges through self-supervised policy learning and adaptive
decision-making. This study proposes a hierarchical Actor-Critic deep
reinforcement learning framework for solving the TSP-D problem. The
architecture consists of two primary components: a Transformer-inspired encoder
and an efficient Minimal Gated Unit decoder. The encoder incorporates a novel,
optimized k-nearest neighbors sparse attention mechanism specifically for
focusing on relevant spatial relationships, further enhanced by the integration
of global node features. The Minimal Gated Unit decoder processes these encoded
representations to efficiently generate solution sequences. The entire
framework operates within an asynchronous advantage actor-critic paradigm.
Experimental results show that, on benchmark TSP-D instances of various scales
(N=10 to 100), the proposed model can obtain competitive or even superior
solutions in shorter average computation times compared to high-performance
heuristic algorithms and existing reinforcement learning methods. Moreover,
compared to advanced reinforcement learning algorithm benchmarks, the proposed
framework significantly reduces the total training time required while
achieving superior final performance, highlighting its notable advantage in
training efficiency.

</details>


### [49] [Embedding-Space Data Augmentation to Prevent Membership Inference Attacks in Clinical Time Series Forecasting](https://arxiv.org/abs/2511.05289)
*Marius Fracarolli,Michael Staniek,Stefan Riezler*

Main category: cs.LG

TL;DR: 通过数据增强提升对成员推断攻击的鲁棒性，ZOO-PCA在时间序列健康记录预测中对MIA防御效果最好，同时不牺牲预测性能。


<details>
  <summary>Details</summary>
Motivation: 在电子病历(EHR)的时间序列预测任务中，需要在保护隐私（抵抗MIA）和保持高预测准确性之间取得权衡。数据增强的合成样本可能Confuse攻击者并提升泛化能力。

Method: 比较三种数据增强策略——ZOO、受PCA约束的ZOO（ZOO-PCA）和MixUp，对TSF模型进行重新训练，评估其对基于损失的MIA的攻防效果（TPR/FPR比）以及对测试集的预测性能。

Result: ZOO-PCA在降低MIA的真实阳性率与假阳性率之比方面表现最佳，且不会损害测试数据的预测性能。ZOO和MixUp也有一定的防御效果，但不及ZOO-PCA。

Conclusion: 在TSF场景下，结合合成数据进行再训练可显著提升对MIA的鲁棒性，ZOO-PCA是最优选择。

Abstract: Balancing strong privacy guarantees with high predictive performance is
critical for time series forecasting (TSF) tasks involving Electronic Health
Records (EHR). In this study, we explore how data augmentation can mitigate
Membership Inference Attacks (MIA) on TSF models. We show that retraining with
synthetic data can substantially reduce the effectiveness of loss-based MIAs by
reducing the attacker's true-positive to false-positive ratio. The key
challenge is generating synthetic samples that closely resemble the original
training data to confuse the attacker, while also introducing enough novelty to
enhance the model's ability to generalize to unseen data. We examine multiple
augmentation strategies - Zeroth-Order Optimization (ZOO), a variant of ZOO
constrained by Principal Component Analysis (ZOO-PCA), and MixUp - to
strengthen model resilience without sacrificing accuracy. Our experimental
results show that ZOO-PCA yields the best reductions in TPR/FPR ratio for MIA
attacks without sacrificing performance on test data.

</details>


### [50] [Attention and Compression is all you need for Controllably Efficient Language Models](https://arxiv.org/abs/2511.05313)
*Jatin Prakash,Aahlad Puli,Rajesh Ranganath*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The quadratic cost of attention in transformers motivated the development of
efficient approaches: namely sparse and sliding window attention, convolutions
and linear attention. Although these approaches result in impressive reductions
in compute and memory, they often trade-off with quality, specifically
in-context recall performance. Moreover, apriori fixing this quality-compute
tradeoff means being suboptimal from the get-go: some downstream applications
require more memory for in-context recall, while others require lower latency
and memory. Further, these approaches rely on heuristic choices that
artificially restrict attention, or require handcrafted and complex recurrent
state update rules, or they must be carefully composed with attention at
specific layers to form a hybrid architecture that complicates the design
process, especially at scale. To address above issues, we propose Compress &
Attend Transformer (CAT), a conceptually simple architecture employing two
simple ingredients only: dense attention and compression. CAT decodes chunks of
tokens by attending to compressed chunks of the sequence so far. Compression
results in decoding from a reduced sequence length that yields compute and
memory savings, while choosing a particular chunk size trades-off quality for
efficiency. Moreover, CAT can be trained with multiple chunk sizes at once,
unlocking control of quality-compute trade-offs directly at test-time without
any retraining, all in a single adaptive architecture. In exhaustive
evaluations on common language modeling tasks, in-context recall, and
long-context understanding, a single adaptive CAT model outperforms existing
efficient baselines, including hybrid architectures, across different
compute-memory budgets. Further, a single CAT matches dense transformer in
language modeling across model scales while being 1.4-3x faster and requiring
2-9x lower total memory usage.

</details>


### [51] [Turning Adversaries into Allies: Reversing Typographic Attacks for Multimodal E-Commerce Product Retrieval](https://arxiv.org/abs/2511.05325)
*Janet Jenq,Hongda Shen*

Main category: cs.LG

TL;DR: 提出通过在产品图像上渲染相关文本元数据来实现视觉-文本压缩，从而提升零-shot多模态商品检索的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-文本模型（如 CLIP）对图像中嵌入文本的误导性攻击的潜在脆弱性；通过把文本信息直接嵌入图像，强化图像-文本对齐并提升检索性能。

Method: 在三类电商数据集（球鞋、手袋、交易卡）上，将产品标题/描述等元数据渲染到商品图像上，进行视觉-文本压缩；使用六种前沿视觉基础模型进行评估。

Result: 在单模态和多模态检索准确性上实现一致提升，跨类别和模型族表现稳健。

Conclusion: 在电商场景中，显式将产品元数据渲染到图像中是一种简单且有效的零-shot多模态检索增强策略。

Abstract: Multimodal product retrieval systems in e-commerce platforms rely on
effectively combining visual and textual signals to improve search relevance
and user experience. However, vision-language models such as CLIP are
vulnerable to typographic attacks, where misleading or irrelevant text embedded
in images skews model predictions. In this work, we propose a novel method that
reverses the logic of typographic attacks by rendering relevant textual content
(e.g., titles, descriptions) directly onto product images to perform
vision-text compression, thereby strengthening image-text alignment and
boosting multimodal product retrieval performance. We evaluate our method on
three vertical-specific e-commerce datasets (sneakers, handbags, and trading
cards) using six state-of-the-art vision foundation models. Our experiments
demonstrate consistent improvements in unimodal and multimodal retrieval
accuracy across categories and model families. Our findings suggest that
visually rendering product metadata is a simple yet effective enhancement for
zero-shot multimodal retrieval in e-commerce applications.

</details>


### [52] [Learning Dynamics from Input-Output Data with Hamiltonian Gaussian Processes](https://arxiv.org/abs/2511.05330)
*Jan-Hendrik Ewering,Robin E. Herrmann,Niklas Wahlström,Thomas B. Schön,Thomas Seel*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Embedding non-restrictive prior knowledge, such as energy conservation laws,
in learning-based approaches is a key motive to construct physically consistent
models from limited data, relevant for, e.g., model-based control. Recent work
incorporates Hamiltonian dynamics into Gaussian Process (GP) regression to
obtain uncertainty-quantifying models that adhere to the underlying physical
principles. However, these works rely on velocity or momentum data, which is
rarely available in practice. In this paper, we consider dynamics learning with
non-conservative Hamiltonian GPs, and address the more realistic problem
setting of learning from input-output data. We provide a fully Bayesian scheme
for estimating probability densities of unknown hidden states, of GP
hyperparameters, as well as of structural hyperparameters, such as damping
coefficients. Considering the computational complexity of GPs, we take
advantage of a reduced-rank GP approximation and leverage its properties for
computationally efficient prediction and training. The proposed method is
evaluated in a nonlinear simulation case study and compared to a
state-of-the-art approach that relies on momentum measurements.

</details>


### [53] [SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning](https://arxiv.org/abs/2511.05355)
*Tzu-Yuan Huang,Armin Lederer,Dai-Jie Wu,Xiaobing Dai,Sihua Zhang,Stefan Sosnowski,Shao-Hua Sun,Sandra Hirche*

Main category: cs.LG

TL;DR: 在流匹配（FM）基础上提出 SAD-Flower 框架，通过引入虚拟控制输入，对轨迹规划过程进行形式化约束保证，实现安全、可接受性和动态一致性；无需重新训练即可在测试时满足未见约束，且在多任务中优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有的 FM 缺乏对状态/动作约束和动态一致性的正式保证，可能生成不可执行或不安全的轨迹。需要在不重新训练的前提下、在测试时对未见约束进行满足，并确保轨迹的可行性与安全性。

Method: 给流扩展一个虚拟控制输入，利用非线性控制理论的方法推导出对状态约束、动作约束和动力学一致性的系统保证，SAD-Flower 在测试时即可应用，且不需要重新训练。

Result: 在多个任务上进行广泛实验，SAD-Flower 相对于基于生成模型的多种基线，在约束满足性方面表现更优。

Conclusion: SAD-Flower 提供了一个可安全、可接受且动态一致的轨迹生成框架，具备形式化保证、测试时适应性强，能显著改进现有 FM 规划器在约束满足方面的能力。

Abstract: Flow matching (FM) has shown promising results in data-driven planning.
However, it inherently lacks formal guarantees for ensuring state and action
constraints, whose satisfaction is a fundamental and crucial requirement for
the safety and admissibility of planned trajectories on various systems.
Moreover, existing FM planners do not ensure the dynamical consistency, which
potentially renders trajectories inexecutable. We address these shortcomings by
proposing SAD-Flower, a novel framework for generating Safe, Admissible, and
Dynamically consistent trajectories. Our approach relies on an augmentation of
the flow with a virtual control input. Thereby, principled guidance can be
derived using techniques from nonlinear control theory, providing formal
guarantees for state constraints, action constraints, and dynamic consistency.
Crucially, SAD-Flower operates without retraining, enabling test-time
satisfaction of unseen constraints. Through extensive experiments across
several tasks, we demonstrate that SAD-Flower outperforms various
generative-model-based baselines in ensuring constraint satisfaction.

</details>


### [54] [Diffusion-Based Electromagnetic Inverse Design of Scattering Structured Media](https://arxiv.org/abs/2511.05357)
*Mikhail Tsukerman,Konstantin Grotov,Pavel Ginzburg*

Main category: cs.LG

TL;DR: 使用扩散模型用于电磁逆设计，直接从目标散射轮廓生成2x2介电球结构，显著加速设计并处理非唯一性。


<details>
  <summary>Details</summary>
Motivation: 逆设计问题具有非唯一性且需要昂贵的迭代优化；需要快速、可扩展的方法来探索复杂的超表面结构以支持光子与无线通信系统的发展。

Method: 提出一个条件扩散模型中的1D U-Net，结合FiLM进行条件调控，直接从目标差分散射截面映射到2x2的介电球结构；通过采样处理非唯一性，模型在11,000个仿真 metasurfaces 上训练，能够实现从目标模式到具体结构的快速设计。

Result: 在未见目标上中位 MPE < 19%，最佳为 1.39%；相比 CMA-ES 进化优化，设计时间从小时级降至秒级，显示出在逆设计任务中的显著效率提升。

Conclusion: 证明扩散模型在电磁逆设计中具有潜力，能够快速探索复杂 metasurface 架构并促进新一代光子与无线系统的发展，且代码公开以便复现与扩展。

Abstract: We present a conditional diffusion model for electromagnetic inverse design
that generates structured media geometries directly from target differential
scattering cross-section profiles, bypassing expensive iterative optimization.
Our 1D U-Net architecture with Feature-wise Linear Modulation learns to map
desired angular scattering patterns to 2x2 dielectric sphere structure,
naturally handling the non-uniqueness of inverse problems by sampling diverse
valid designs. Trained on 11,000 simulated metasurfaces, the model achieves
median MPE below 19% on unseen targets (best: 1.39%), outperforming CMA-ES
evolutionary optimization while reducing design time from hours to seconds.
These results demonstrate that employing diffusion models is promising for
advancing electromagnetic inverse design research, potentially enabling rapid
exploration of complex metasurface architectures and accelerating the
development of next-generation photonic and wireless communication systems. The
code is publicly available at
https://github.com/mikzuker/inverse_design_metasurface_generation.

</details>


### [55] [ProDER: A Continual Learning Approach for Fault Prediction in Evolving Smart Grids](https://arxiv.org/abs/2511.05420)
*Emad Efatinasab,Nahal Azadi,Davide Dalle Pezze,Gian Antonio Susto,Chuadhry Mujeeb Ahmed,Mirco Rampazzo*

Main category: cs.LG

TL;DR: 提出 ProDER 的持续学习框架，用于智能电网的故障预测，结合原型记忆和蒸馏等，适应新增故障类型和区域，取得最佳性能，误差很小。


<details>
  <summary>Details</summary>
Motivation: 随着智能电网变得更复杂，故障预测需在环境演变中保持可靠性，传统AI模型难以适应新故障类型与区域。

Method: 提出 Prototype-based Dark Experience Replay (ProDER)，融合原型特征正则化、logit蒸馏、原型引导的回放记忆，并在四个基于类别增量与领域增量的评估场景中验证。

Result: 在测试的CL技术中，ProDER表现最佳，故障类型预测的准确率仅下降0.045，故障区域预测下降0.015。

Conclusion: 证明将持续学习应用于智能电网的故障预测具有可扩展性和现实性。

Abstract: As smart grids evolve to meet growing energy demands and modern operational
challenges, the ability to accurately predict faults becomes increasingly
critical. However, existing AI-based fault prediction models struggle to ensure
reliability in evolving environments where they are required to adapt to new
fault types and operational zones. In this paper, we propose a continual
learning (CL) framework in the smart grid context to evolve the model together
with the environment. We design four realistic evaluation scenarios grounded in
class-incremental and domain-incremental learning to emulate evolving grid
conditions. We further introduce Prototype-based Dark Experience Replay
(ProDER), a unified replay-based approach that integrates prototype-based
feature regularization, logit distillation, and a prototype-guided replay
memory. ProDER achieves the best performance among tested CL techniques, with
only a 0.045 accuracy drop for fault type prediction and 0.015 for fault zone
prediction. These results demonstrate the practicality of CL for scalable,
real-world fault prediction in smart grids.

</details>


### [56] [APP: Accelerated Path Patching with Task-Specific Pruning](https://arxiv.org/abs/2511.05442)
*Frauke Andersen,William Rudman,Ruochen Zhang,Carsten Eickhoff*

Main category: cs.LG

TL;DR: Accelerated Path Patching (APP) speeds up circuit discovery by pruning attention heads with a novel Contrastive-FLAP method based on causal mediation, then applying Path Patching on the reduced set.


<details>
  <summary>Details</summary>
Motivation: Circuit discovery is computationally expensive, especially for smaller models, and existing methods struggle with minimality constraints; there is a need for faster, smaller, yet accurate circuit analyses.

Method: 1) Use Contrastive-FLAP pruning to assign higher scores to task-specific attention heads using causal mediation analysis to produce sparse models. 2) Apply traditional Path Patching to the remaining heads to identify circuits. 3) Compare search space and performance against dense-path patching.

Result: Contrastive-FLAP reduces the search space for circuit discovery by about 56% on average. When Path Patching is applied to the pruned set, the speedup ranges from 59.63% to 93.27% compared to Path Patching on dense models. Circuits obtained via APP show substantial overlap and similar performance to prior Path Patching circuits.

Conclusion: APP delivers substantial computational savings for circuit discovery with comparable circuit quality to previous methods, though the resulting circuits may still exhibit overlap and redundancy; further work could aim to reduce overlap while preserving performance.

Abstract: Circuit discovery is a key step in many mechanistic interpretability
pipelines. Current methods, such as Path Patching, are computationally
expensive and have limited in-depth circuit analysis for smaller models. In
this study, we propose Accelerated Path Patching (APP), a hybrid approach
leveraging our novel contrastive attention head pruning method to drastically
reduce the search space of circuit discovery methods. Our Contrastive-FLAP
pruning algorithm uses techniques from causal mediation analysis to assign
higher pruning scores to task-specific attention heads, leading to higher
performing sparse models compared to traditional pruning techniques. Although
Contrastive-FLAP is successful at preserving task-specific heads that existing
pruning algorithms remove at low sparsity ratios, the circuits found by
Contrastive-FLAP alone are too large to satisfy the minimality constraint
required in circuit analysis. APP first applies Contrastive-FLAP to reduce the
search space on required for circuit discovery algorithms by, on average, 56\%.
Next, APP, applies traditional Path Patching on the remaining attention heads,
leading to a speed up of 59.63\%-93.27\% compared to Path Patching applied to
the dense model. Despite the substantial computational saving that APP
provides, circuits obtained from APP exhibit substantial overlap and similar
performance to previously established Path Patching circuits

</details>


### [57] [Adversarially Robust Multitask Adaptive Control](https://arxiv.org/abs/2511.05444)
*Kasra Fallah,Leonardo F. Toso,James Anderson*

Main category: cs.LG

TL;DR: 提出一种集群式多任务鲁棒自适应LQR学习框架，通过聚类、系统辨识和鲁棒聚合来对抗对模型更新的恶意污染，给出非渐近 regret 界，表明在簇内诚实系统数量增加时 regret 降低且对有限比例的对抗仍成立。


<details>
  <summary>Details</summary>
Motivation: 在模型不确定性和对抗污染的情形下，多个系统需要协同学习控制策略，但单点系统易受污染，集群协同可提升鲁棒性与样本效率。

Method: 将聚类与系统辨识结合，并引入鲁棒聚合，对每个簇内的系统进行CE-LQR学习，分析聚类准确性、簇内异质性和对抗行为对期望 regret 的影响；给出非渐近界。

Result: 理论上证明 regret 随簇内的诚实系统数量的增加而以反比速度下降；即使簇内存在有界比例的对抗系统，该下降趋势仍被保留。

Conclusion: 该方法在含对抗污染的多任务LQR学习中具有鲁棒性提升与样本效率改进，量化揭示了集群结构、异质性和对抗行为对性能的关键影响。

Abstract: We study adversarially robust multitask adaptive linear quadratic control; a
setting where multiple systems collaboratively learn control policies under
model uncertainty and adversarial corruption. We propose a clustered multitask
approach that integrates clustering and system identification with resilient
aggregation to mitigate corrupted model updates. Our analysis characterizes how
clustering accuracy, intra-cluster heterogeneity, and adversarial behavior
affect the expected regret of certainty-equivalent (CE) control across LQR
tasks. We establish non-asymptotic bounds demonstrating that the regret
decreases inversely with the number of honest systems per cluster and that this
reduction is preserved under a bounded fraction of adversarial systems within
each cluster.

</details>


### [58] [SiamMM: A Mixture Model Perspective on Deep Unsupervised Learning](https://arxiv.org/abs/2511.05462)
*Xiaodong Wang,Jing Huang,Kevin J Liang*

Main category: cs.LG

TL;DR: 将自监督/无监督聚类方法与经典混合模型联系起来，提出 SiamMM，显著提升聚类基础自监督学习的性能，并能揭示潜在的标签错误。


<details>
  <summary>Details</summary>
Motivation: 克服聚类在自监督学习中的启发式做法，建立统一的统计框架以提升方法效果并理解学习到的聚类。

Method: 将聚类方法框架映射到混合模型，并在此基础上提出 SiamMM 模型，用以优化和评估自监督聚类在不同基准上的表现。

Result: 实现了自监督学习基准上的当前最优性能，并且对学习得到的簇进行分析，显示与未见 Ground Truth 标签具有高度相似性，揭示可能的标签错误。

Conclusion: 为聚类驱动的自监督学习提供一个统计学统一框架，SiamMM 显著提升效果并具备发现标签噪声的潜力。

Abstract: Recent studies have demonstrated the effectiveness of clustering-based
approaches for self-supervised and unsupervised learning. However, the
application of clustering is often heuristic, and the optimal methodology
remains unclear. In this work, we establish connections between these
unsupervised clustering methods and classical mixture models from statistics.
Through this framework, we demonstrate significant enhancements to these
clustering methods, leading to the development of a novel model named SiamMM.
Our method attains state-of-the-art performance across various self-supervised
learning benchmarks. Inspection of the learned clusters reveals a strong
resemblance to unseen ground truth labels, uncovering potential instances of
mislabeling.

</details>


### [59] [Precipitation nowcasting of satellite data using physically conditioned neural networks](https://arxiv.org/abs/2511.05471)
*Antônio Catão,Melvin Poveda,Leonardo Voltarelli,Paulo Orenstein*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate short-term precipitation forecasts predominantly rely on dense
weather-radar networks, limiting operational value in places most exposed to
climate extremes. We present TUPANN (Transferable and Universal Physics-Aligned
Nowcasting Network), a satellite-only model trained on GOES-16 RRQPE. Unlike
most deep learning models for nowcasting, TUPANN decomposes the forecast into
physically meaningful components: a variational encoder-decoder infers motion
and intensity fields from recent imagery under optical-flow supervision, a
lead-time-conditioned MaxViT evolves the latent state, and a differentiable
advection operator reconstructs future frames. We evaluate TUPANN on both
GOES-16 and IMERG data, in up to four distinct climates (Rio de Janeiro,
Manaus, Miami, La Paz) at 10-180min lead times using the CSI and HSS metrics
over 4-64 mm/h thresholds. Comparisons against optical-flow, deep learning and
hybrid baselines show that TUPANN achieves the best or second-best skill in
most settings, with pronounced gains at higher thresholds. Training on multiple
cities further improves performance, while cross-city experiments show modest
degradation and occasional gains for rare heavy-rain regimes. The model
produces smooth, interpretable motion fields aligned with numerical optical
flow and runs in near real time due to the low latency of GOES-16. These
results indicate that physically aligned learning can provide nowcasts that are
skillful, transferable and global.

</details>


### [60] [SoilX: Calibration-Free Comprehensive Soil Sensing Through Contrastive Cross-Component Learning](https://arxiv.org/abs/2511.05482)
*Kang Yang,Yuanlin Yang,Yuning Chen,Sikai Yang,Xinyu Zhang,Wan Du*

Main category: cs.LG

TL;DR: 提出 SoilX，使用对比跨组分学习和新型四面体天线阵列，实现对 M、N、P、K、C、Al 的无标定土壤传感，降低纹理和碳相关再校准需求，提升估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有无线土壤传感系统在土壤纹理变化（Al、C）时需要重新标定，限制实际应用；需要一种在不同纹理条件下仍能保持准确的无标定测量的方法。

Method: 设计两部分：(1) 3CL 框架（对比跨组分学习），包含正交性正则器和分离损失，以解耦不同组分之间的干扰；(2) 新型四面体天线阵列与天线切换机制，鲁棒测量土壤介电常数且对放置位置不敏感。

Result: 实验表明 SoilX 相比基线，误差降低约 23.8%–31.5%，并在未见场地上具有更好的泛化能力。

Conclusion: 通过显式建模碳和铝等纹理分量并联合学习，可以实现无标定的多组分土壤感知，提升实用性与稳健性。

Abstract: Precision agriculture demands continuous and accurate monitoring of soil
moisture (M) and key macronutrients, including nitrogen (N), phosphorus (P),
and potassium (K), to optimize yields and conserve resources. Wireless soil
sensing has been explored to measure these four components; however, current
solutions require recalibration (i.e., retraining the data processing model) to
handle variations in soil texture, characterized by aluminosilicates (Al) and
organic carbon (C), limiting their practicality. To address this, we introduce
SoilX, a calibration-free soil sensing system that jointly measures six key
components: {M, N, P, K, C, Al}. By explicitly modeling C and Al, SoilX
eliminates texture- and carbon-dependent recalibration. SoilX incorporates
Contrastive Cross-Component Learning (3CL), with two customized terms: the
Orthogonality Regularizer and the Separation Loss, to effectively disentangle
cross-component interference. Additionally, we design a novel tetrahedral
antenna array with an antenna-switching mechanism, which can robustly measure
soil dielectric permittivity independent of device placement. Extensive
experiments demonstrate that SoilX reduces estimation errors by 23.8% to 31.5%
over baselines and generalizes well to unseen fields.

</details>


### [61] [DGTN: Graph-Enhanced Transformer with Diffusive Attention Gating Mechanism for Enzyme DDG Prediction](https://arxiv.org/abs/2511.05483)
*Abigail Lin*

Main category: cs.LG

TL;DR: A bidirectional diffusion framework DGTN that jointly learns GNN structural priors and transformer sequence representations to predict ΔΔG of amino acid mutations, achieving state-of-the-art performance and theoretical convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing methods often process sequence and structure independently, missing the critical coupling between local structural geometry and global sequence patterns. A joint, diffusion-based framework can leverage both representations and provide convergence guarantees for structure–sequence integration.

Method: DGTN (Diffused Graph-Transformer Network) enables bidirectional diffusion between a GNN and a Transformer. (1) GNN-derived structural embeddings guide Transformer attention via learnable diffusion kernels; (2) Transformer representations refine GNN message passing through attention-modulated graph updates. The two modules co-learn via diffusion, with mathematical analysis showing improved approximation bounds. Convergence of diffused attention is proved with rate O(1/√T).

Result: On ProTherm and SKEMPI benchmarks, DGTN achieves state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with a 6.2% improvement over strong baselines. Ablation shows the diffusion mechanism contributes 4.8 points to correlation.

Conclusion: The work establishes a principled framework for integrating heterogeneous protein representations through learnable diffusion, providing theoretical guarantees and empirical gains, and setting a new baseline for structure–sequence joint models in protein engineering.

Abstract: Predicting the effect of amino acid mutations on enzyme thermodynamic
stability (DDG) is fundamental to protein engineering and drug design. While
recent deep learning approaches have shown promise, they often process sequence
and structure information independently, failing to capture the intricate
coupling between local structural geometry and global sequential patterns. We
present DGTN (Diffused Graph-Transformer Network), a novel architecture that
co-learns graph neural network (GNN) weights for structural priors and
transformer attention through a diffusion mechanism. Our key innovation is a
bidirectional diffusion process where: (1) GNN-derived structural embeddings
guide transformer attention via learnable diffusion kernels, and (2)
transformer representations refine GNN message passing through
attention-modulated graph updates. We provide rigorous mathematical analysis
showing this co-learning scheme achieves provably better approximation bounds
than independent processing. On ProTherm and SKEMPI benchmarks, DGTN achieves
state-of-the-art performance (Pearson Rho = 0.87, RMSE = 1.21 kcal/mol), with
6.2% improvement over best baselines. Ablation studies confirm the diffusion
mechanism contributes 4.8 points to correlation. Our theoretical analysis
proves the diffused attention converges to optimal structure-sequence coupling,
with convergence rate O(1/sqrt(T) ) where T is diffusion steps. This work
establishes a principled framework for integrating heterogeneous protein
representations through learnable diffusion.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [62] [Prototype Selection Using Topological Data Analysis](https://arxiv.org/abs/2511.04873)
*Jordan Eckert,Elvan Ceyhan,Henry Schenck*

Main category: stat.ML

TL;DR: 提出并验证 Topological Prototype Selector (TPS)——一个基于拓扑数据分析的原型选择框架，用于从大规模数据中选择代表性原型，在模拟和真实数据中保持或提升分类性能的同时显著减少数据量。


<details>
  <summary>Details</summary>
Motivation: 随着统计学习日益关注用拓扑原则表示数据结构，现有原型选择方法在大规模数据场景中可能效率低、信息损失或未能充分利用数据的拓扑结构，需要一个既保留分类性能又能高效压缩的数据表示框架。

Method: 提出 TPS 框架，利用拓扑数据分析从数据中识别并选择代表性原型；在不同数据内在特征的模拟数据以及真实数据集上评估，并与其他原型选择方法进行比较。

Result: 在所有模拟与真实数据场景中，TPS 能显著保持或提升分类性能，同时大幅降低数据规模。

Conclusion: 该工作在原型学习的算法和几何方面均有贡献，提供可并行化、可解释且高效的分类工具。

Abstract: Recently, there has been an explosion in statistical learning literature to
represent data using topological principles to capture structure and
relationships. We propose a topological data analysis (TDA)-based framework,
named Topological Prototype Selector (TPS), for selecting representative
subsets (prototypes) from large datasets. We demonstrate the effectiveness of
TPS on simulated data under different data intrinsic characteristics, and
compare TPS against other currently used prototype selection methods in real
data settings. In all simulated and real data settings, TPS significantly
preserves or improves classification performance while substantially reducing
data size. These contributions advance both algorithmic and geometric aspects
of prototype learning and offer practical tools for parallelized,
interpretable, and efficient classification.

</details>


### [63] [Estimating Bidirectional Causal Effects with Large Scale Online Kernel Learning](https://arxiv.org/abs/2511.05050)
*Masahiro Tanaka*

Main category: stat.ML

TL;DR: A scalable online kernel learning framework for estimating bidirectional causal effects in systems with mutual dependence and heteroskedasticity, combining heteroskedasticity-based identification with online kernel learning and random Fourier features for efficiency and nonlinear modeling.


<details>
  <summary>Details</summary>
Motivation: Real-world systems often exhibit bidirectional causal relationships and heteroskedasticity. Traditional methods focusing on unidirectional effects may miss these interactions. A scalable, theoretically grounded approach is needed for large-scale causal inference.

Method: Integrates a quasi-maximum likelihood estimator for simultaneous equation models with online kernel learning. Uses random Fourier feature approximations to model nonlinear conditional means and variances. Employs an adaptive online gradient descent algorithm for efficiency in streaming and high-dimensional data.

Result: Simulation studies show the proposed method achieves higher accuracy and stability than single-equation and polynomial baselines, with lower bias and RMSE across multiple data-generating processes and near-linear computational scaling.

Conclusion: By uniting econometric identification with modern machine learning, the framework provides a practical, scalable, and theoretically grounded solution for large-scale causal inference in applications ranging from natural/social sciences to policy, business, and industry.

Abstract: In this study, a scalable online kernel learning framework is proposed for
estimating bidirectional causal effects in systems characterized by mutual
dependence and heteroskedasticity. Traditional causal inference often focuses
on unidirectional effects, overlooking the common bidirectional relationships
in real-world phenomena. Building on heteroskedasticity-based identification,
the proposed method integrates a quasi-maximum likelihood estimator for
simultaneous equation models with large scale online kernel learning. It
employs random Fourier feature approximations to flexibly model nonlinear
conditional means and variances, while an adaptive online gradient descent
algorithm ensures computational efficiency for streaming and high-dimensional
data. Results from extensive simulations demonstrate that the proposed method
achieves superior accuracy and stability than single equation and polynomial
approximation baselines, exhibiting lower bias and root mean squared error
across various data-generating processes. These results confirm that the
proposed approach effectively captures complex bidirectional causal effects
with near-linear computational scaling. By combining econometric identification
with modern machine learning techniques, the proposed framework offers a
practical, scalable, and theoretically grounded solution for large scale causal
inference in natural/social science, policy making, business, and industrial
applications.

</details>


### [64] [A New Framework for Convex Clustering in Kernel Spaces: Finite Sample Bounds, Consistency and Performance Insights](https://arxiv.org/abs/2511.05159)
*Shubhayan Pan,Saptarshi Chakraborty,Debolina Paul,Kushal Bose,Swagatam Das*

Main category: stat.ML

TL;DR: Kernelized convex clustering in RKHS with finite-dimensional embedding, supported by convergence proofs and finite-sample guarantees; shows improved performance on nonlinear/non-convex data.


<details>
  <summary>Details</summary>
Motivation: Convex clustering often struggles with linearly non-separable or non-convex structures. A kernelization extends clustering to a Reproducing Kernel Hilbert Space to capture complex distributions while yielding a practical finite-dimensional embedding.

Method: Introduce a kernelized extension of convex clustering by mapping data into an RKHS through a feature map; formulate and optimize a convex clustering objective in the transformed space; leverage kernel trick to obtain a finite-dimensional embedding and prove algorithmic convergence.

Result: Theoretical results include convergence guarantees and finite-sample bounds for the estimates. Empirical results on synthetic and real datasets demonstrate superior clustering performance compared with state-of-the-art methods.

Conclusion: Kernelized convex clustering advances clustering of nonlinear/non-convex data by combining convex clustering with kernel methods, supported by theory and experiments, and provides a practical finite-dimensional embedding.

Abstract: Convex clustering is a well-regarded clustering method, resembling the
similar centroid-based approach of Lloyd's $k$-means, without requiring a
predefined cluster count. It starts with each data point as its centroid and
iteratively merges them. Despite its advantages, this method can fail when
dealing with data exhibiting linearly non-separable or non-convex structures.
To mitigate the limitations, we propose a kernelized extension of the convex
clustering method. This approach projects the data points into a Reproducing
Kernel Hilbert Space (RKHS) using a feature map, enabling convex clustering in
this transformed space. This kernelization not only allows for better handling
of complex data distributions but also produces an embedding in a
finite-dimensional vector space. We provide a comprehensive theoretical
underpinnings for our kernelized approach, proving algorithmic convergence and
establishing finite sample bounds for our estimates. The effectiveness of our
method is demonstrated through extensive experiments on both synthetic and
real-world datasets, showing superior performance compared to state-of-the-art
clustering techniques. This work marks a significant advancement in the field,
offering an effective solution for clustering in non-linear and non-convex data
scenarios.

</details>


### [65] [Self-adaptive weighting and sampling for physics-informed neural networks](https://arxiv.org/abs/2511.05452)
*Wenqian Chen,Amanda Howard,Panos Stinis*

Main category: stat.ML

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Physics-informed deep learning has emerged as a promising framework for
solving partial differential equations (PDEs). Nevertheless, training these
models on complex problems remains challenging, often leading to limited
accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling
and weighting method to enhance the performance of physics-informed neural
networks (PINNs). The adaptive sampling component identifies training points in
regions where the solution exhibits rapid variation, while the adaptive
weighting component balances the convergence rate across training points.
Numerical experiments show that applying only adaptive sampling or only
adaptive weighting is insufficient to consistently achieve accurate
predictions, particularly when training points are scarce. Since each method
emphasizes different aspects of the solution, their effectiveness is problem
dependent. By combining both strategies, the proposed framework consistently
improves prediction accuracy and training efficiency, offering a more robust
approach for solving PDEs with PINNs.

</details>
