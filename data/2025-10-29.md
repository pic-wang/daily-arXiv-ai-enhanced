<div id=toc></div>

# Table of Contents

- [stat.ML](#stat.ML) [Total: 5]
- [cs.LG](#cs.LG) [Total: 97]


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [1] [Beyond Normality: Reliable A/B Testing with Non-Gaussian Data](https://arxiv.org/abs/2510.23666)
*Junpeng Gong,Chunkai Wang,Hao Li,Jinyong Ma,Haoxuan Li,Xu He*

Main category: stat.ML

TL;DR: 本论文在在线A/B测试中揭示配对t检验在偏态、长尾分布以及非等样本量条件下的误差控制问题，给出最小样本量公式并提出基于Edgeworth展开的p值修正以提高小样本的可靠性。


<details>
  <summary>Details</summary>
Motivation: A/B测试广泛用于特征上线、定价优化等，但在样本量不足或分布偏离正态时，传统t检验易导致类型I错误率失控，真实场景中数据往往偏态、长尾、分组不平衡，需要更严格的误差控制。

Method: 推导在偏度、峰度及分配不均情况下t检验的最小样本量公式，量化误差率的畸变；基于Edgeworth展开提出更准确的p值修正；在离线平台数据上验证公式并比较修正方法的效能。

Result: 给出明确的最小样本量界限，指出某些指标可能需要数亿样本才能达到可信的检验；离线平台数据验证修正方法显著提升A/B测试的可靠性。

Conclusion: Edgeworth-based修正为有限样本条件下的t检验提供更稳健的p值，帮助实践中更可靠地进行A/B测试，并给出样本量规划的实用指引。

Abstract: A/B testing has become the cornerstone of decision-making in online markets,
guiding how platforms launch new features, optimize pricing strategies, and
improve user experience. In practice, we typically employ the pairwise $t$-test
to compare outcomes between the treatment and control groups, thereby assessing
the effectiveness of a given strategy. To be trustworthy, these experiments
must keep Type I error (i.e., false positive rate) under control; otherwise, we
may launch harmful strategies. However, in real-world applications, we find
that A/B testing often fails to deliver reliable results. When the data
distribution departs from normality or when the treatment and control groups
differ in sample size, the commonly used pairwise $t$-test is no longer
trustworthy. In this paper, we quantify how skewed, long tailed data and
unequal allocation distort error rates and derive explicit formulas for the
minimum sample size required for the $t$-test to remain valid. We find that
many online feedback metrics require hundreds of millions samples to ensure
reliable A/B testing. Thus we introduce an Edgeworth-based correction that
provides more accurate $p$-values when the available sample size is limited.
Offline experiments on a leading A/B testing platform corroborate the practical
value of our theoretical minimum sample size thresholds and demonstrate that
the corrected method substantially improves the reliability of A/B testing in
real-world conditions.

</details>


### [2] [Understanding Fairness and Prediction Error through Subspace Decomposition and Influence Analysis](https://arxiv.org/abs/2510.23935)
*Enze Shi,Pankaj Bhagwat,Zhixian Yang,Linglong Kong,Bei Jiang*

Main category: stat.ML

TL;DR: 通过显式分解数据表示并去除敏感信息，提出一种在保证预测性能的同时提升公平性的表示学习框架。


<details>
  <summary>Details</summary>
Motivation: 当前公平性研究多聚焦于预测层面的约束，而数据表示层的偏见会被放大。需要一个从表示层入手、可控公平性与效用的框架。

Method: 使用充分降维（Sufficient Dimension Reduction）将特征空间分解为目标相关、敏感和共享子空间，通过选择性移除敏感信息来控制公平-效用权衡；理论分析预测误差与公平差距随共享子空间的加入变化；使用影响函数量化对参数估计渐近行为的影响。

Result: 在合成数据和真实数据上验证理论分析，所提方法能在提高公平性的同时尽量保持预测性能。

Conclusion: 提出了一种利用表示层调整实现公平性的原理框架，并提供了对误差与公平差异的渐近分析和实证验证。

Abstract: Machine learning models have achieved widespread success but often inherit
and amplify historical biases, resulting in unfair outcomes. Traditional
fairness methods typically impose constraints at the prediction level, without
addressing underlying biases in data representations. In this work, we propose
a principled framework that adjusts data representations to balance predictive
utility and fairness. Using sufficient dimension reduction, we decompose the
feature space into target-relevant, sensitive, and shared components, and
control the fairness-utility trade-off by selectively removing sensitive
information. We provide a theoretical analysis of how prediction error and
fairness gaps evolve as shared subspaces are added, and employ influence
functions to quantify their effects on the asymptotic behavior of parameter
estimates. Experiments on both synthetic and real-world datasets validate our
theoretical insights and show that the proposed method effectively improves
fairness while preserving predictive performance.

</details>


### [3] [Score-based constrained generative modeling via Langevin diffusions with boundary conditions](https://arxiv.org/abs/2510.23985)
*Adam Nordenhög,Akash Sharma*

Main category: stat.ML

TL;DR: 提出使用带镜面边界的动力学（欠阻尼Langevin）进行约束的生成建模，并与基于局部时间的反射扩散进行比较，给出具最优离散化收敛率的高效采样器。


<details>
  <summary>Details</summary>
Motivation: 解决基于SDE的评分模型在约束条件下难以保持约束的问题；通过在边界上对速度进行镜面反射实现受限的生成过程，并对现有带局部时间项的反射SDE进行扩展与对比。

Method: 采用带镜面边界的欠阻尼Langevin动力学，在边界处对速度进行镜面反射，得到分段可微的加噪与去噪过程；所得到的时间反转动力学在带边界的域内定义；此外对以局部时间项限制的反射SDE进行了补充，并提出高效数值采样器，证明其在离散步长下具有最优收敛速率；给出两类受限扩散的系统性比较。

Result: 提出并实现了带镜面反射的受限Langevin生成模型及其高效采样器；对比分析了带镜面反射与局部时间两类受限扩散的性能与收敛性，证实在离散步长下达到最优收敛速率。

Conclusion: 镜面反射的受限Langevin扩散为约束生成建模提供了可行且高效的框架，与基于局部时间的反射扩散相比，具备良好的理论性质和数值表现。

Abstract: Score-based generative models based on stochastic differential equations
(SDEs) achieve impressive performance in sampling from unknown distributions,
but often fail to satisfy underlying constraints. We propose a constrained
generative model using kinetic (underdamped) Langevin dynamics with specular
reflection of velocity on the boundary defining constraints. This results in
piecewise continuously differentiable noising and denoising process where the
latter is characterized by a time-reversed dynamics restricted to a domain with
boundary due to specular boundary condition. In addition, we also contribute to
existing reflected SDEs based constrained generative models, where the
stochastic dynamics is restricted through an abstract local time term. By
presenting efficient numerical samplers which converge with optimal rate in
terms of discretizations step, we provide a comprehensive comparison of models
based on confined (specularly reflected kinetic) Langevin diffusion with models
based on reflected diffusion with local time.

</details>


### [4] [Copula-Stein Discrepancy: A Generator-Based Stein Operator for Archimedean Dependence](https://arxiv.org/abs/2510.24056)
*Agnideep Aich,Ashit Baran Aich*

Main category: stat.ML

TL;DR: 提出 Copula-Stein Discrepancy (CSD)，基于 copula 密度的 Stein 偏差，用于捕捉依赖结构，尤其尾部依赖；对 Archimedean Copula 有闭式核，并扩展到非 Archimedean；可线性维度计算，随机特征近似使复杂度近似线性；理论性保证包括弱收敛等价性、收敛率、对尾部依赖的敏感性。


<details>
  <summary>Details</summary>
Motivation: 与标准 Kernel Stein Discrepancies (KSD) 主要关注边缘分布不同，CSD 专注于统计依赖的几何结构，尤其是尾部依赖，满足很多领域的需求（如金融风险）。

Method: 在 copula 密度上定义 Stein 算符，通过标量生成器导出对 Archimedean copulas 的闭式 Stein kernel；给出对非 Archimedean 的扩展（包括椭圆和藤本 Copulas）。提出经验估计量并证明其收敛到 minimax 速率 O_P(n^{-1/2})；实现上核的直接计算线性于维度，并给出随机特征近似将复杂度从 O(n^2) 降到近线性 tilde O(n)。

Result: CSD 可以刻画 copula 分布的弱收敛，能够检测任何依赖结构的错配；对尾部依赖差异具有敏感性；对 Archimedean copulas 提供闭式核，扩展到其他 Copulas；随机特征近似使大规模数据可行。

Conclusion: CSD 为依赖感知推断提供一个理论上严格、可计算的工具，尤其适用于强调尾部和更高阶依赖结构的场景；未来工作包括对其他非 Archimedean Copula 的进一步数值实现和在实际数据上的应用评估。

Abstract: Kernel Stein discrepancies (KSDs) have become a principal tool for
goodness-of-fit testing, but standard KSDs are often insensitive to
higher-order dependency structures, such as tail dependence, which are critical
in many scientific and financial domains. We address this gap by introducing
the Copula-Stein Discrepancy (CSD), a novel class of discrepancies tailored to
the geometry of statistical dependence. By defining a Stein operator directly
on the copula density, CSD leverages the generative structure of dependence,
rather than relying on the joint density's score function. For the broad class
of Archimedean copulas, this approach yields a closed-form Stein kernel derived
from the scalar generator function. We provide a comprehensive theoretical
analysis, proving that CSD (i) metrizes weak convergence of copula
distributions, ensuring it detects any mismatch in dependence; (ii) has an
empirical estimator that converges at the minimax optimal rate of
$O_P(n^{-1/2})$; and (iii) is provably sensitive to differences in tail
dependence coefficients. The framework is extended to general non-Archimedean
copulas, including elliptical and vine copulas. Computationally, the exact CSD
kernel evaluation scales linearly in dimension, while a novel random feature
approximation reduces the $n$-dependence from quadratic $O(n^2)$ to near-linear
$\tilde{O}(n)$, making CSD a practical and theoretically principled tool for
dependence-aware inference.

</details>


### [5] [Self-Concordant Perturbations for Linear Bandits](https://arxiv.org/abs/2510.24187)
*Lucas Lévy,Jean-Lou Valeau,Arya Akhavan,Patrick Rebeschini*

Main category: stat.ML

TL;DR: 提出一个统一的 FTPL-FTRL 框架用于对抗性线性赌博带来自同收敛扰动的算法，结合自同收敛正则化与随机探索，在 d 维超立方体和欧几里得球中实现 O(d√(n ln n)) 的遗憾界，超球情形与现有自同收敛 FTRL 匹配，超立方体情形较之 FTRL 提升 √d。


<details>
  <summary>Details</summary>
Motivation: 将 FTRL 与 FTPL 的联系扩展到对抗性线性带来问题，提出自同收敛扰动以在一个统一框架内同时实现有效的正则化与探索，从而在高维几何域获得接近最优的遗憾界。

Method: 提出一个统一的框架，将 FTRL 与 FTPL 连接起来，定义自同收敛扰动分布并将其用于 FTPL 基于算法，同时结合自同收敛正则化和高效随机探索策略，针对 d 维的超立方体和欧几里得球给出分析。

Result: 在超立方体和欧几里得球上均得到遗憾界 O(d√(n ln n))。在欧几里得球上与现有的自同收敛 FTRL（SCRiBLe）匹配速率；在超立方体上比相关的 FTRL 方法提升了 √d，并在对数因子范围内达到最优界。

Conclusion: 通过提出自同收敛扰动的概念，将 FTPL 与 FTRL 的联系拓展到对抗性线性带来问题，构建了一个高效且具有理论保证的新算法，既实现了更好的几何域适应性，又保持接近最优的遗憾界。

Abstract: We study the adversarial linear bandits problem and present a unified
algorithmic framework that bridges Follow-the-Regularized-Leader (FTRL) and
Follow-the-Perturbed-Leader (FTPL) methods, extending the known connection
between them from the full-information setting. Within this framework, we
introduce self-concordant perturbations, a family of probability distributions
that mirror the role of self-concordant barriers previously employed in the
FTRL-based SCRiBLe algorithm. Using this idea, we design a novel FTPL-based
algorithm that combines self-concordant regularization with efficient
stochastic exploration. Our approach achieves a regret of $O(d\sqrt{n \ln n})$
on both the $d$-dimensional hypercube and the Euclidean ball. On the Euclidean
ball, this matches the rate attained by existing self-concordant FTRL methods.
For the hypercube, this represents a $\sqrt{d}$ improvement over these methods
and matches the optimal bound up to logarithmic factors.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [6] [An Enhanced Dual Transformer Contrastive Network for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.23617)
*Phuong Q. Dao,Mark Roantree,Vuong M. Ngo*

Main category: cs.LG

TL;DR: 提出并验证了 BERT-ViT-EF 及其扩展 DTCN，通过文本-图像的早期融合与对比学习提升多模态情感分析性能，在 TumEmo 上取得领先或有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析需要更深入的跨模态交互与对齐；现有方法在融合策略与上下文建模方面存在不足，需通过更强的特征对齐与上下文建模来提升性能。

Method: 提出 BERT-ViT-EF 进行早期融合的文本-图像 Transformer 编码；在此基础上扩展 DTCN，增加文本上下文 Transformer 层并引入对比学习以对齐文本与图像表征。

Result: 在 MVSA-Single 与 TumEmo 基准上验证有效性。TumEmo 上达到最佳准确率 78.4% 与 F1 78.3%，MVSA-Single 表现也具竞争力，分别为 76.6% 与 75.9%。

Conclusion: 早期融合与更深层次的上下文建模有助于提升 Transformer 基的多模态情感分析性能，DTCN 的对比学习进一步增强跨模态表征对齐。

Abstract: Multimodal Sentiment Analysis (MSA) seeks to understand human emotions by
jointly analyzing data from multiple modalities typically text and images
offering a richer and more accurate interpretation than unimodal approaches. In
this paper, we first propose BERT-ViT-EF, a novel model that combines powerful
Transformer-based encoders BERT for textual input and ViT for visual input
through an early fusion strategy. This approach facilitates deeper cross-modal
interactions and more effective joint representation learning. To further
enhance the model's capability, we propose an extension called the Dual
Transformer Contrastive Network (DTCN), which builds upon BERT-ViT-EF. DTCN
incorporates an additional Transformer encoder layer after BERT to refine
textual context (before fusion) and employs contrastive learning to align text
and image representations, fostering robust multimodal feature learning.
Empirical results on two widely used MSA benchmarks MVSA-Single and TumEmo
demonstrate the effectiveness of our approach. DTCN achieves best accuracy
(78.4%) and F1-score (78.3%) on TumEmo, and delivers competitive performance on
MVSA-Single, with 76.6% accuracy and 75.9% F1-score. These improvements
highlight the benefits of early fusion and deeper contextual modeling in
Transformer-based multimodal sentiment analysis.

</details>


### [7] [Speeding Up MACE: Low-Precision Tricks for Equivarient Force Fields](https://arxiv.org/abs/2510.23621)
*Alexandre Benoit*

Main category: cs.LG

TL;DR: 通过对 MACE 的端到端与分块分析，评估不同后端与精度策略对速度与物理 fidelity 的影响，提出在不显著影响观测的前提下的大幅加速方案。


<details>
  <summary>Details</summary>
Motivation: 解决 SO(3)-等变模型在成本与精度之间的权衡，验证低精度执行和 GPU 内核优化是否能带来实质性加速。

Method: 端到端和分块剖面、对比 e3nn 与 cuEquivariance、测试 FP64/FP32/BF16/FP16（FP32 累加）在推理、NVT、NPT 与 toy 训练上的表现，确保可重复性。

Result: cuEquivariance 推理延迟约下降 3×；将线性层 cast 到 BF16/FP16 在 FP32 模型上再增速约 4×；NVT/NPT 能量和热力观测在变动范围内；半精度权重训练降低力 RMSE；混合 e3nn 与 cuEq 导致表示不匹配；融合内核和混合精度推理在不显著影响 MD 的前提下显著提速；默认策略：cuEquivariance + FP32，线性层启用 BF16/FP16，FP32 累加，训练保持 FP32；未来可在 Ampere/Hopper 上获得更多提升。

Conclusion: 综合策略能在保持物理 fidelities 的前提下显著提升 MACE 为基础的力场计算效率，给出可执行的默认配置并指向硬件与内核层面的进一步优化方向。

Abstract: Machine-learning force fields can deliver accurate molecular dynamics (MD) at
high computational cost. For SO(3)-equivariant models such as MACE, there is
little systematic evidence on whether reduced-precision arithmetic and
GPU-optimized kernels can cut this cost without harming physical fidelity. This
thesis aims to make MACE cheaper and faster while preserving accuracy by
identifying computational bottlenecks and evaluating low-precision execution
policies. We profile MACE end-to-end and per block, compare the e3nn and NVIDIA
cuEquivariance backends, and assess FP64/FP32/BF16/FP16 settings (with FP32
accumulation) for inference, short NVT and long NPT water simulations, and toy
training runs under reproducible, steady-state timing. cuEquivariance reduces
inference latency by about $3\times$. Casting only linear layers to BF16/FP16
within an FP32 model yields roughly 4x additional speedups, while energies and
thermodynamic observables in NVT/NPT MD remain within run-to-run variability.
Half-precision weights during training degrade force RMSE. Mixing e3nn and cuEq
modules without explicit adapters causes representation mismatches. Fused
equivariant kernels and mixed-precision inference can substantially accelerate
state-of-the-art force fields with negligible impact on downstream MD. A
practical policy is to use cuEquivariance with FP32 by default and enable
BF16/FP16 for linear layers (keeping FP32 accumulations) for maximum
throughput, while training remains in FP32. Further gains are expected on
Ampere/Hopper GPUs (TF32/BF16) and from kernel-level FP16/BF16 paths and
pipeline fusion.

</details>


### [8] [EddyFormer: Accelerated Neural Simulations of Three-Dimensional Turbulence at Scale](https://arxiv.org/abs/2510.24173)
*Yiheng Du,Aditi S. Krishnapriyan*

Main category: cs.LG

TL;DR: 提出 EddyFormer，将 Transformer 与谱-元素方法相结合的 SEM 架构用于大尺度湍流仿真，在 256^3 分辨率下达到接近 DNS 的精度并实现约 30 倍加速，具备域外泛化能力与在 The Well 基准上的优越性，解决了先前 ML 模型的收敛与泛化问题。


<details>
  <summary>Details</summary>
Motivation: 直接数值仿真(DNS)在大尺度湍流中计算成本极高，催生数据驱动替代方案。但现有 ML 模型往往泛化性差、难以在新域/复杂条件下收敛。需要一种兼具准确性和可扩展性的方法来近似大尺度湍流行为。

Method: 提出 EddyFormer，为基于 Transformer 的谱-元素(SEM) 架构。引入 SEM tokens，将流场分解为网格尺度(grid-scale)和亚网格尺度(subgrid-scale)分量，从而同时捕捉局部与全局特征。训练于三维各向同性湍流水数据集，在 256^3 分辨率下实现 DNS 级别精度并获得约 30× 加速；对未见域扩展至原训练域的 4× 更大规模时仍保持物理不变量指标的精度。

Result: 在 unseen domains 与 The Well 基准测试中，EddyFormer 能保持能谱、相关函数、结构函数等物理量的一致性与准确性，显示出良好的域外泛化以及对多物理条件的稳健重现，且解决了先前 ML 模型难以收敛的情况。

Conclusion: 将谱方法的高精度与 Transformer 的全局建模能力相结合的 SEM 架构，证明了基于 ML 的湍流近似在大尺度与域泛化方面的潜力，提示未来在高效湍流仿真与多域条件下的应用前景。

Abstract: Computationally resolving turbulence remains a central challenge in fluid
dynamics due to its multi-scale interactions. Fully resolving large-scale
turbulence through direct numerical simulation (DNS) is computationally
prohibitive, motivating data-driven machine learning alternatives. In this
work, we propose EddyFormer, a Transformer-based spectral-element (SEM)
architecture for large-scale turbulence simulation that combines the accuracy
of spectral methods with the scalability of the attention mechanism. We
introduce an SEM tokenization that decomposes the flow into grid-scale and
subgrid-scale components, enabling capture of both local and global features.
We create a new three-dimensional isotropic turbulence dataset and train
EddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x
speedup over DNS. When applied to unseen domains up to 4x larger than in
training, EddyFormer preserves accuracy on physics-invariant metrics-energy
spectra, correlation functions, and structure functions-showing domain
generalization. On The Well benchmark suite of diverse turbulent flows,
EddyFormer resolves cases where prior ML models fail to converge, accurately
reproducing complex dynamics across a wide range of physical conditions.

</details>


### [9] [Adversarially-Aware Architecture Design for Robust Medical AI Systems](https://arxiv.org/abs/2510.23622)
*Alyssa Gerhart,Balaji Iyangar*

Main category: cs.LG

TL;DR: 对医疗保健中的对抗性攻击进行实证研究，利用皮肤病数据集测试攻击与防御的效果，发现尽管对抗训练等防御能降低攻击成功率，但往往以牺牲干净数据性能为代价，呼吁在技术、伦理与政策层面综合提升AI的鲁棒性与公平性。


<details>
  <summary>Details</summary>
Motivation: 确保医疗AI系统在对抗性攻击下的安全性和可靠性，特别是保护弱势群体的患者安全，降低误诊与治疗延误的风险。

Method: 对皮肤科数据集进行实证实验，设计威胁模型并进行基准测试，评估模型在对抗样本上的表现；测试包括对抗训练和蒸馏等防御策略及其对清洁数据性能的影响。

Result: 攻击在多个实验设置中显著降低分类准确性；防御措施能降低攻击成功率，但通常需要在保持对清洁数据的性能方面权衡取舍，防御的有效性并非全局性。

Conclusion: 呼吁结合技术、伦理与政策层面的综合策略，构建对抗性鲁棒且兼顾公平性的医疗AI系统。

Abstract: Adversarial attacks pose a severe risk to AI systems used in healthcare,
capable of misleading models into dangerous misclassifications that can delay
treatments or cause misdiagnoses. These attacks, often imperceptible to human
perception, threaten patient safety, particularly in underserved populations.
Our study explores these vulnerabilities through empirical experimentation on a
dermatological dataset, where adversarial methods significantly reduce
classification accuracy. Through detailed threat modeling, experimental
benchmarking, and model evaluation, we demonstrate both the severity of the
threat and the partial success of defenses like adversarial training and
distillation. Our results show that while defenses reduce attack success rates,
they must be balanced against model performance on clean data. We conclude with
a call for integrated technical, ethical, and policy-based approaches to build
more resilient, equitable AI in healthcare.

</details>


### [10] [DiNo and RanBu: Lightweight Predictions from Shallow Random Forests](https://arxiv.org/abs/2510.23624)
*Tiago Mendonça dos Santos,Rafael Izbicki,Luís Gustavo Esteves*

Main category: cs.LG

TL;DR: 提出两种浅森林 DiNo 与 RanBu，通过训练后仅使用距离/近似近邻权重实现高效预测；相对于完整深度随机森林，在多数据集上达到同等或更高准确性，显著降低训练与推理时间，最高节省约95%。同时支持分位数回归，且实现为开源包。


<details>
  <summary>Details</summary>
Motivation: 随机森林在表格数据上表现好，但需要大量深树导致推理延迟和内存需求。需要在保持精度的同时降低计算成本，方便在延迟敏感或资源受限的场景部署。

Method: DiNo：基于最近公共祖先（MRCA）的共聚距离来衡量观测对之间的距离；RanBu：对Breiman近邻度量（proximity）做核平滑。两者都在森林训练完成后工作，不再生成新树，仅通过带宽参数h进行轻量矩阵向量运算；不需要额外训练。上传实现在R/C++。

Result: 在三个合成基准和25个公开数据集上，RanBu在高噪声场景下与全深森林相当甚至优于其准确性，且训练+推理时间可降低高达95%；DiNo在低噪声下实现最佳偏差-方差权衡，成本适中；两者均可扩展到分位数回归，保持显著的速度提升。

Conclusion: 提供一种高效的近似随机森林替代方案，适用于结构化表格数据，兼顾准确性与速度；开源实现便于复现与应用；未来工作包括扩展到其他模态。

Abstract: Random Forest ensembles are a strong baseline for tabular prediction tasks,
but their reliance on hundreds of deep trees often results in high inference
latency and memory demands, limiting deployment in latency-sensitive or
resource-constrained environments. We introduce DiNo (Distance with Nodes) and
RanBu (Random Bushes), two shallow-forest methods that convert a small set of
depth-limited trees into efficient, distance-weighted predictors. DiNo measures
cophenetic distances via the most recent common ancestor of observation pairs,
while RanBu applies kernel smoothing to Breiman's classical proximity measure.
Both approaches operate entirely after forest training: no additional trees are
grown, and tuning of the single bandwidth parameter $h$ requires only
lightweight matrix-vector operations. Across three synthetic benchmarks and 25
public datasets, RanBu matches or exceeds the accuracy of full-depth random
forests-particularly in high-noise settings-while reducing training plus
inference time by up to 95\%. DiNo achieves the best bias-variance trade-off in
low-noise regimes at a modest computational cost. Both methods extend directly
to quantile regression, maintaining accuracy with substantial speed gains. The
implementation is available as an open-source R/C++ package at
https://github.com/tiagomendonca/dirf. We focus on structured tabular random
samples (i.i.d.), leaving extensions to other modalities for future work.

</details>


### [11] [From Detection to Discovery: A Closed-Loop Approach for Simultaneous and Continuous Medical Knowledge Expansion and Depression Detection on Social Media](https://arxiv.org/abs/2510.23626)
*Shuang Geng,Wenli Zhang,Jiaheng Xie,Rui Wang,Sudha Ram*

Main category: cs.LG

TL;DR: 提出一个闭环的LLM-知识图谱框架，用于社媒UGC的抑郁检测，LLM同时进行实体提取，知识图谱对实体进行表示与加权来提升预测，并在专家监督下进行知识扩展与迭代学习，实现模型与领域知识的共同进化，提升预测准确性与医学理解，并具备扩展到其他动态风险监测场景的潜力。


<details>
  <summary>Details</summary>
Motivation: 弥合预测模型与领域知识的断层；在抑郁预测中，除了使用医学知识提升准确性外，还应通过预测过程来扩展知识，以实现持续的知识进化。

Method: 在知识感知阶段，LLM联合进行抑郁检测和实体抽取，知识图表示并对实体加权以提升预测。知识改进阶段，LLM提取的新实体、关系和实体类型在专家监督下被并入知识图谱，形成持续的知识更新循环。

Result: 在大规模UGC数据上，框架提升了预测准确性与医学理解。专家评估证实发现的临床相关症状、共病和社会触发因素，与现有文献互补。

Conclusion: 提出预测-学习与学习-预测相互强化的过程，展示计算模型和领域知识的协同演化，为面向动态风险监控的自适应、数据驱动知识系统奠定基础，可推广至其他领域。

Abstract: Social media user-generated content (UGC) provides real-time, self-reported
indicators of mental health conditions such as depression, offering a valuable
source for predictive analytics. While prior studies integrate medical
knowledge to improve prediction accuracy, they overlook the opportunity to
simultaneously expand such knowledge through predictive processes. We develop a
Closed-Loop Large Language Model (LLM)-Knowledge Graph framework that
integrates prediction and knowledge expansion in an iterative learning cycle.
In the knowledge-aware depression detection phase, the LLM jointly performs
depression detection and entity extraction, while the knowledge graph
represents and weights these entities to refine prediction performance. In the
knowledge refinement and expansion phase, new entities, relationships, and
entity types extracted by the LLM are incorporated into the knowledge graph
under expert supervision, enabling continual knowledge evolution. Using
large-scale UGC, the framework enhances both predictive accuracy and medical
understanding. Expert evaluations confirmed the discovery of clinically
meaningful symptoms, comorbidities, and social triggers complementary to
existing literature. We conceptualize and operationalize
prediction-through-learning and learning-through-prediction as mutually
reinforcing processes, advancing both methodological and theoretical
understanding in predictive analytics. The framework demonstrates the
co-evolution of computational models and domain knowledge, offering a
foundation for adaptive, data-driven knowledge systems applicable to other
dynamic risk monitoring contexts.

</details>


### [12] [Chain of Execution Supervision Promotes General Reasoning in Large Language Models](https://arxiv.org/abs/2510.23629)
*Nuo Chen,Zehua Li,Keqin Bao,Junyang Lin,Dayiheng Liu*

Main category: cs.LG

TL;DR: TracePile 构建了一个 2.6M 样本的数据集，将代码执行过程转化为显式、逐步的 Chain of Execution（CoE）推理，以提升大语言模型的推理能力。覆盖数学、经典算法、程序竞赛等领域，并通过变量追踪与代码改写增强逻辑粒度和代码多样性。


<details>
  <summary>Details</summary>
Motivation: 现有基于代码的训练往往把推理过程隐藏在代码实现噪声中，导致直接学习的推理能力受限，需要显式的、可解析的推理轨迹来提升鲁棒性与泛化。

Method: 构造 260 万样本的 TracePile，将代码执行转化为 CoE；引入变量追踪问题和代码改写以丰富逻辑粒度与代码多样性；在数学、经典算法、竞赛等子域进行覆盖。对四个基础模型进行 continue-pretraining、pretraining 后的指令微调，以及两阶段微调，评估在 20 项基准上的表现。

Result: 在四个基础模型和 20 个基准上均有提升，尤其 LLaMA3.1-8B 在九个数学数据集上平均提升 7.1%，并在 LiveCodeBench、CRUX、MMLU 等任务上于两阶段微调表现出显著收益。

Conclusion: 将执行轨迹作为显式化的推理信号融入大规模代码语料，能显著提升跨领域的推理与代码理解能力，证明 CoE 是提升编码和数学推理任务表现的有效训练信号。

Abstract: Building robust and general reasoning ability is a central goal in the
development of large language models (LLMs). Recent efforts increasingly turn
to code as a rich training source, given its inherent logical structure and
diverse reasoning paradigms such as divide-and-conquer, topological ordering,
and enumeration. However, reasoning in code is often expressed implicitly and
entangled with syntactic or implementation noise, making direct training on raw
code suboptimal.To address this, we introduce TracePile, a large-scale corpus
of 2.6 million samples that transforms code execution into explicit,
step-by-step chain-of-thought-style rationales, which we call Chain of
Execution (CoE). The corpus spans domains including mathematics, classical
algorithms and algorithmic competition, and is enriched with variable-tracing
questions and code rewritings to enhance logical granularity and code
diversity. We evaluate TracePile using three training setups:
continue-pretraining, instruction tuning after pretraining, and two-stage
finetuning. Experiments across four base models (LLaMA 3, LLaMA 3.1, Qwen-2.5,
and Qwen-2.5 Coder) and 20 benchmarks covering math, code, logic, and
algorithms demonstrate consistent improvements. Notably, TracePile boosts
LLaMA3.1-8B by 7.1\% on average across nine math datasets and delivers clear
gains on LiveCodeBench, CRUX, and MMLU under two-stage fine-tuning.

</details>


### [13] [Generalized Linear Bandits with Limited Adaptivity](https://arxiv.org/abs/2404.06831)
*Ayush Sawarni,Nirjhar Das,Siddharth Barman,Gaurav Sinha*

Main category: cs.LG

TL;DR: 在受限自适应条件下，研究广义线性情境带宽问题，提出两种算法 B-GLinCB 与 RS-GLinCB，覆盖两类预算型设定，达到接近最优的 sqrt(T) 量级遗憾，并消除了对非线性参数 κ 的依赖。


<details>
  <summary>Details</summary>
Motivation: 现实场景中策略更新次数有限，需在固定预算内完成学习与决策；在广义线性情境带宽问题下，如何在有限自适性条件下实现接近最优的遗憾界，并移除对 κ 的依赖。

Method: 提出两种算法：B-GLinCB（在预算 M 下需提前选定轮次更新策略）和 RS-GLinCB（在过程中自适应进行更新，更新次数为 ~ log^2 T）。两者分别针对两种受限自适性设定给出理论遗憾界，并通过新颖分析框架移除 κ 对遗憾的影响。

Result: B-GLinCB 当 M = Ω(log log T) 且臂特征向量随机生成时，遗憾为  O(√T)；RS-GLinCB 仅需 ~O(log^2 T) 次更新，即可在对手性（对抗性）特征向量下实现  O(√T) 的遗憾。并且两种情形均消除了对手性参数 κ 的依赖。

Conclusion: 提出的去 κ 依赖的方法对广义线性情境带宽问题具有独立意义，显示在受限自适应下实现接近最优遗憾的可行性，可能扩展到更广的情境中。

Abstract: We study the generalized linear contextual bandit problem within the
constraints of limited adaptivity. In this paper, we present two algorithms,
$\texttt{B-GLinCB}$ and $\texttt{RS-GLinCB}$, that address, respectively, two
prevalent limited adaptivity settings. Given a budget $M$ on the number of
policy updates, in the first setting, the algorithm needs to decide upfront $M$
rounds at which it will update its policy, while in the second setting it can
adaptively perform $M$ policy updates during its course. For the first setting,
we design an algorithm $\texttt{B-GLinCB}$, that incurs $\tilde{O}(\sqrt{T})$
regret when $M = \Omega( \log{\log T} )$ and the arm feature vectors are
generated stochastically. For the second setting, we design an algorithm
$\texttt{RS-GLinCB}$ that updates its policy $\tilde{O}(\log^2 T)$ times and
achieves a regret of $\tilde{O}(\sqrt{T})$ even when the arm feature vectors
are adversarially generated. Notably, in these bounds, we manage to eliminate
the dependence on a key instance dependent parameter $\kappa$, that captures
non-linearity of the underlying reward model. Our novel approach for removing
this dependence for generalized linear contextual bandits might be of
independent interest.

</details>


### [14] [Preference Learning with Response Time: Robust Losses and Guarantees](https://arxiv.org/abs/2505.22820)
*Ayush Sawarni,Sahasrajit Sarmasarkar,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: 将响应时间信息并入偏好学习，通过EZ模型和Neyman-orthogonal损失提升奖励模型的样本效率；对线性奖励函数实现理论上的oracle收敛率，并将结果扩展到非参数情形，实验验证在图像偏好任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 二元偏好数据虽广泛使用，但忽略了用户决策过程中的时间信息。响应时间潜在地反映偏好强度，若能有效利用可显著提升奖励模型的学习效率，进而提升基础模型的对齐效果。

Method: 将Evidence Accumulation Drift Diffusion (EZ) 模型引入偏好学习，将响应时间与二元选择数据结合；构造Neyman-orthogonal损失以实现对奖励模型的无偏估计并达到理论最优收敛率；对线性奖励函数给出理论分析，证明传统方法的误差随奖励幅度指数增长，而本方法将其降至多项式级别；并推广到非参数奖励函数空间，给出收敛性质；在图像偏好数据集上进行广泛实验以验证理论与方法。

Result: 理论上证明了在引入响应时间后，奖励模型学习可实现oracle级别的收敛率；在线性奖励函数场景中，减小了误差的增长速度，将其从指数级别转为多项式级别；对非参数情形也给出收敛保证；实验结果支持理论结论，尤其在图像偏好任务中表现出更高的样本效率。

Conclusion: 该研究系统地将响应时间信息融入偏好学习，显著提升 reward model 的样本效率和鲁棒性，为对齐大规模模型提供了理论与实践的双重支撑。未来工作可进一步评估对更复杂任务的适用性，以及对不同响应时间噪声分布的鲁棒性。

Abstract: This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [15] [Beyond Pairwise: Empowering LLM Alignment With Ranked Choice Modeling](https://arxiv.org/abs/2510.23631)
*Yuxuan Tang,Yifan Feng*

Main category: cs.LG

TL;DR: 提出 RCPO 框架：将排名选择偏好整合到 LLM 对齐中，通过最大似然估计结合排名/效用模型，兼容多种形式的反馈并可扩展到现有的成对偏好方法。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐多依赖成对偏好，缺乏对多项选择排名等更丰富人类反馈的利用。通过引入排名选择模型，可从更丰富的反馈中学习，提升对齐效果。

Method: RCPO 将偏好优化与 ranked choice modeling 通过最大似然统一起来，支持效用模型和排名模型；可涵盖 DPO、SimPO 等现有方法；以两种 ranked choice 模型（Multinomial Logit、Mallows-RMJ）为例在 Llama-3-8B-Instruct 和 Gemma-2-9B-it 上的 AlpacaEval 2、Arena-Hard 进行实验。

Result: 在指定模型与基准上，RCPO 持续优于竞争 baselines，显示直接利用排序偏好数据和恰当的选择模型能提升对齐效果。

Conclusion: RCPO 提供一个灵活、可扩展的基础框架，便于在 LLM 训练中引入（排名）选择模型和更丰富的反馈格式，未来可扩展到更多选择模型和反馈形式。

Abstract: Alignment of large language models (LLMs) has predominantly relied on
pairwise preference optimization, where annotators select the better of two
responses to a prompt. While simple, this approach overlooks the opportunity to
learn from richer forms of human feedback, such as multiwise comparisons and
top-$k$ rankings. We propose Ranked Choice Preference Optimization (RCPO), a
unified framework that bridges preference optimization with (ranked) choice
modeling via maximum likelihood estimation. The framework is flexible,
supporting both utility-based and rank-based choice models. It subsumes several
existing pairwise methods (e.g., DPO, SimPO), while providing principled
training objectives for richer feedback formats. We instantiate this framework
with two representative ranked choice models (Multinomial Logit and
Mallows-RMJ). Empirical studies on Llama-3-8B-Instruct and Gemma-2-9B-it across
AlpacaEval 2 and Arena-Hard benchmarks show that RCPO consistently outperforms
competitive baselines. RCPO shows how directly leveraging ranked preference
data, combined with the right choice models, yields more effective alignment.
It offers a versatile and extensible foundation for incorporating (ranked)
choice modeling into LLM training.

</details>


### [16] [LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression](https://arxiv.org/abs/2510.23632)
*Guozhong Li,Muhannad Alhumaidi,Spiros Skiadopoulos,Panos Kalnis*

Main category: cs.LG

TL;DR: 提出 LLMCOMP，一种基于解码器端大语言模型的高保真科学数据有损压缩框架，通过量化、Z-order 排序、覆盖引导采样、以及自回归 transformer 建模，压缩阶段仅存储排名索引和回退修正即可在严格误差界限下实现更高压缩率，实验在多组再分析数据集上比现有压缩器提升最多约 30%。


<details>
  <summary>Details</summary>
Motivation: 应对海量高分辨率时空数据的无损/有损压缩需求，现有方法在严格误差边界下的性能仍有提升空间，且解码端大模型对复杂序列数据建模的能力尚未被充分探索用于数据压缩。

Method: 将三维场量化为离散 token，使用 Z-order 曲线保持局部性；采用覆盖引导采样提高训练效率；用带有时空嵌入的自回归 transformer 对 token 转移建模；在压缩阶段进行 top-k 预测，仅存储 rank 索引与回退修正以确保严格的误差界限。

Result: 在多组再分析数据集上的实验表明，LLMCOMP 在严格误差边界下实现比最先进的压缩器更高的压缩比，提升幅度可达约 30%。

Conclusion: 证明解码器端大语言模型可作为通用的高保真科学数据压缩器，具有广泛的应用潜力，并提示未来在高维时空数据的建模与压缩方面的深入探索。

Abstract: The rapid growth of high-resolution scientific simulations and observation
systems is generating massive spatiotemporal datasets, making efficient,
error-bounded compression increasingly important. Meanwhile, decoder-only large
language models (LLMs) have demonstrated remarkable capabilities in modeling
complex sequential data. In this paper, we propose LLMCOMP, a novel lossy
compression paradigm that leverages decoder-only large LLMs to model scientific
data. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via
Z-order curves to preserve locality, and applies coverage-guided sampling to
enhance training efficiency. An autoregressive transformer is then trained with
spatial-temporal embeddings to model token transitions. During compression, the
model performs top-k prediction, storing only rank indices and fallback
corrections to ensure strict error bounds. Experiments on multiple reanalysis
datasets show that LLMCOMP consistently outperforms state-of-the-art
compressors, achieving up to 30% higher compression ratios under strict error
bounds. These results highlight the potential of LLMs as general-purpose
compressors for high-fidelity scientific data.

</details>


### [17] [Noise is All You Need: Solving Linear Inverse Problems by Noise Combination Sampling with Diffusion Models](https://arxiv.org/abs/2510.23633)
*Xun Su,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: 提出 Noise Combination Sampling (NCS)，通过从噪声子空间合成最优噪声向量以近似观测分数，从而在扩散模型中无缝嵌入观测信息，尤其在少步生成时提升性能与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 高斯扩散模型在零-shot逆问题中能结合观测信息，但若过度融合会扰乱生成过程，融合不足则无法体现逆问题约束，需要一个不依赖逐步超参的嵌入策略。

Method: 在标准扩散过程中替换噪声项，利用来自噪声子空间的噪声向量组合近似观测分数，形成一种新的采样策略以嵌入条件信息；无需逐步超参调优，兼容广泛的逆问题求解任务（如图像压缩等）。

Result: 在生成步数较小的情形下，方法实现优于传统方法，计算开销几乎不增加，同时显著提升鲁棒性与稳定性。

Conclusion: 该方法为逆问题求解提供了一种通用且高效的噪声层嵌入策略，特别在低步数场景具备明显优势，具备广泛的适用性。

Abstract: Pretrained diffusion models have demonstrated strong capabilities in
zero-shot inverse problem solving by incorporating observation information into
the generation process of the diffusion models. However, this presents an
inherent dilemma: excessive integration can disrupt the generative process,
while insufficient integration fails to emphasize the constraints imposed by
the inverse problem. To address this, we propose \emph{Noise Combination
Sampling}, a novel method that synthesizes an optimal noise vector from a noise
subspace to approximate the measurement score, replacing the noise term in the
standard Denoising Diffusion Probabilistic Models process. This enables
conditional information to be naturally embedded into the generation process
without reliance on step-wise hyperparameter tuning. Our method can be applied
to a wide range of inverse problem solvers, including image compression, and,
particularly when the number of generation steps $T$ is small, achieves
superior performance with negligible computational overhead, significantly
improving robustness and stability.

</details>


### [18] [Debiasing Reward Models by Representation Learning with Guarantees](https://arxiv.org/abs/2510.23751)
*Ignavier Ng,Patrick Blöbaum,Siddharth Bhandari,Kun Zhang,Shiva Kasiviswanathan*

Main category: cs.LG

TL;DR: 提出一种通过变分推断识别非伪变量以去偏的奖励模型训练框架，缓解强化学习来自人类反馈（RLHF）中的伪相关性问题。


<details>
  <summary>Details</summary>
Motivation: 当前对齐方法（如 RLHF）易被伪相关性利用，导致奖励模型偏离真实的人类偏好。需要一个理论与实践结合的框架，识别并利用真正的、非伪的潜在变量来提升对齐鲁棒性。

Method: 对数据生成过程进行形式化建模，认为观测数据来自伪变量和非伪变量两类潜在因素。提出非伪变量在数据上可被识别的结论，并提出基于变分推断的方法来恢复这些变量，进而用于训练奖励模型。

Result: 在合成数据和真实数据集上，所提方法能够有效减轻伪相关性并提升奖励模型的鲁棒性。

Conclusion: 理论上揭示了在存在伪变量时仍可识别非伪变量的条件，并通过实证方法将这一信号转化为更鲁棒的偏好对齐模型。

Abstract: Recent alignment techniques, such as reinforcement learning from human
feedback, have been widely adopted to align large language models with human
preferences by learning and leveraging reward models. In practice, these models
often exploit spurious correlations, involving, e.g., response length,
discrimination, sycophancy, and conceptual bias, which is a problem that has
received increasing attention. In this work, we propose a principled framework
that mitigates these biases in reward models while preserving the underlying
factors that reflect intended preferences. We first provide a formulation of
the data-generating process, assuming that the observed data (e.g., text) is
generated from both spurious and non-spurious latent variables. We show that,
interestingly, these non-spurious latent variables can be theoretically
identified from data, regardless of whether a surrogate for the spurious latent
variables is available. This further inspires a practical method that uses
variational inference to recover these variables and leverages them to train
reward models. Experiments on synthetic and real-world datasets demonstrate
that our method effectively mitigates spurious correlation issues and yields
more robust reward models.

</details>


### [19] [Monotone and Separable Set Functions: Characterizations and Neural Models](https://arxiv.org/abs/2510.23634)
*Soutrik Sarangi,Yonatan Sverdlov,Nadav Dym,Abir De*

Main category: cs.LG

TL;DR: 提出并研究了 MAS（Monotone and Separating）集合函数以在集合到向量的映射中保持集合包含关系；给出有限与无限基底的结果、weakly MAS 的定义及其 Holder 连续性、以及构造通用单调模型的办法并通过实验验证其有效性，代码公开。


<details>
  <summary>Details</summary>
Motivation: 在集合包含问题中，要求从集合S映射到向量F(S)的顺序结构与集合的自然包含关系一致；即 S ⊆ T 当且仅当 F(S) ≤ F(T) 。

Method: 理论分析：给出实现 MAS 所需的向量维数的上下界，研究无限基底下的不存在性；提出 weakly MAS 作为放宽属性，结合 Holder 连续性稳定性；设计可通用的单调模型并证明其可近似所有单调集合函数；在多种集合包含任务上进行实验，比较带有集合包含偏置的模型与标准集合模型；公开实现代码。

Result: 在有限基底上存在 MAS 函数；在无限基底下不存在完全 MAS，但存在弱 MAS 并具 Holder 连续性；MAS 可以用于构造天然单调的通用模型并近似所有单调集合函数；实验显示引入 MAS 偏置的模型优于未引入偏置的基线；代码已在 GitHub 提供。

Conclusion: MAS 框架为集合表示提供了一个系统的单调性与分离性的归纳偏置，明确了理论边界与可行性，并给出可操作的模型与实验证据；未来工作可聚焦高效实现、扩展到更广泛的集合操作及对弱 MAS 的进一步分析。

Abstract: Motivated by applications for set containment problems, we consider the
following fundamental problem: can we design set-to-vector functions so that
the natural partial order on sets is preserved, namely $S\subseteq T \text{ if
and only if } F(S)\leq F(T) $. We call functions satisfying this property
Monotone and Separating (MAS) set functions. % We establish lower and upper
bounds for the vector dimension necessary to obtain MAS functions, as a
function of the cardinality of the multisets and the underlying ground set. In
the important case of an infinite ground set, we show that MAS functions do not
exist, but provide a model called our which provably enjoys a relaxed MAS
property we name "weakly MAS" and is stable in the sense of Holder continuity.
We also show that MAS functions can be used to construct universal models that
are monotone by construction and can approximate all monotone set functions.
Experimentally, we consider a variety of set containment tasks. The experiments
show the benefit of using our our model, in comparison with standard set models
which do not incorporate set containment as an inductive bias. Our code is
available in https://github.com/yonatansverdlov/Monotone-Embedding.

</details>


### [20] [A Physics-informed Multi-resolution Neural Operator](https://arxiv.org/abs/2510.23810)
*Sumanta Roy,Bahador Bahmani,Ioannis G. Kevrekidis,Michael D. Shields*

Main category: cs.LG

TL;DR: 在数据稀缺且网格离散度不一致的情形下，提出一种数据无关的物理信息算子学习方法。通过将输入函数投影到潜在嵌入空间，使用多层感知机近似算子，并在物理空间通过有限差分法强约束PDE，能在多分辨率数据上进行鲁棒推断。


<details>
  <summary>Details</summary>
Motivation: 解决高保真训练数据难以获得以及输入函数在不同样本中离散化差异较大的问题，提升算子学习的适用性和鲁棒性。

Method: 扩展RINO为完全数据无关的框架：把任意足够细的离散输入投影到用预训练基函数构成的潜在向量空间；用一个MLP来近似与PDE相关的算子，输入为潜在代码和时空坐标以输出解；在物理空间通过有限差分求解器对PDE进行约束。

Result: 在多分辨率数据的数值示例中对方法进行验证，输入在不同网格上抽样（粗分辨率和细分辨率）时，方法仍展现出良好性能，证明了对离散化不一致的鲁棒性和数据无关学习能力。

Conclusion: 该工作实现了数据稀缺场景下的物理信息算子学习的可行性，将RINO扩展到数据无关版本，并通过潜在嵌入与MLP结合来高效近似算子，且通过PDE约束确保解的物理一致性，适用于工程中的多分辨率输入场景。

Abstract: The predictive accuracy of operator learning frameworks depends on the
quality and quantity of available training data (input-output function pairs),
often requiring substantial amounts of high-fidelity data, which can be
challenging to obtain in some real-world engineering applications. These
datasets may be unevenly discretized from one realization to another, with the
grid resolution varying across samples. In this study, we introduce a
physics-informed operator learning approach by extending the Resolution
Independent Neural Operator (RINO) framework to a fully data-free setup,
addressing both challenges simultaneously. Here, the arbitrarily (but
sufficiently finely) discretized input functions are projected onto a latent
embedding space (i.e., a vector space of finite dimensions), using pre-trained
basis functions. The operator associated with the underlying partial
differential equations (PDEs) is then approximated by a simple multi-layer
perceptron (MLP), which takes as input a latent code along with spatiotemporal
coordinates to produce the solution in the physical space. The PDEs are
enforced via a finite difference solver in the physical space. The validation
and performance of the proposed method are benchmarked on several numerical
examples with multi-resolution data, where input functions are sampled at
varying resolutions, including both coarse and fine discretizations.

</details>


### [21] [Help the machine to help you: an evaluation in the wild of egocentric data cleaning via skeptical learning](https://arxiv.org/abs/2510.23635)
*Andrea Bontempelli,Matteo Busso,Leonardo Javier Malcotti,Fausto Giunchiglia*

Main category: cs.LG

TL;DR: 在真实用户条件下评估 Skeptical Learning（SKEL）的有效性，发现可在减轻标注工作量的同时提升数据质量，但需权衡用户参与成本。


<details>
  <summary>Details</summary>
Motivation: 弥补以往仅比较离线主动标注与被动数据的评估未获得最终用户确认的不足，提升注释准确性与用户参与的一致性。

Method: 在四周内让大学生使用 iLog 移动应用进行注释，结合用户提供的/来自传感器的被动数据，对比 SKEL 估计的注释标签与用户最终确认的标签，评估准确性与用户工作量。

Result: 结果显示在平衡用户努力与数据质量方面存在挑战；SKEL 能减少注释努力并提高数据质量，但其效益受用户参与程度和场景复杂性影响。

Conclusion: 在实际应用中，考虑引入用户驱动的标签确认机制可增强注释质量，同时需设计以降低用户成本的交互与自动化辅助。

Abstract: Any digital personal assistant, whether used to support task performance,
answer questions, or manage work and daily life, including fitness schedules,
requires high-quality annotations to function properly. However, user
annotations, whether actively produced or inferred from context (e.g., data
from smartphone sensors), are often subject to errors and noise. Previous
research on Skeptical Learning (SKEL) addressed the issue of noisy labels by
comparing offline active annotations with passive data, allowing for an
evaluation of annotation accuracy. However, this evaluation did not include
confirmation from end-users, the best judges of their own context. In this
study, we evaluate SKEL's performance in real-world conditions with actual
users who can refine the input labels based on their current perspectives and
needs. The study involves university students using the iLog mobile application
on their devices over a period of four weeks. The results highlight the
challenges of finding the right balance between user effort and data quality,
as well as the potential benefits of using SKEL, which include reduced
annotation effort and improved quality of collected data.

</details>


### [22] [Optimal Arm Elimination Algorithms for Combinatorial Bandits](https://arxiv.org/abs/2510.23992)
*Yuxiao Wen,Yanjun Han,Zhengyuan Zhou*

Main category: cs.LG

TL;DR: 提出三集合（confirmed/active/eliminated）消除框架，结合显式探索，适用于带图反馈的组合式MAB和组合线性上下文带，获得近似最优后悔界并匹配下界；在UCB缺乏显式探索时可能失效。


<details>
  <summary>Details</summary>
Motivation: 在组合式带臂问题中，需要同时处理多臂选择和高效的逐步淘汰。现有的基于UCB的扩展在消除策略上常常不充分，缺乏足够的显式探索以保证强理论保证，因此需要新的消除框架以实现近最优后悔界。

Method: 提出将臂分为confirmed、active、eliminated三集合的消除方案，并通过显式探索来更新这三集合。算法适用于两种设置：带一般图反馈的组合式多臂带（CMAB）和组合线性上下文带（linear contextual bandit）。给出近似最优的后悔界，并提供匹配下界，指出在某些情形下UCB类方法因显式探索不足而失效。

Result: 在两种设置下实现近乎最优的后悔界，理论上显示所提方法优于或等同于UCB类方法，且给出与之匹配的下界，确立了理论极限。

Conclusion: 三集合消除与显式探索的策略具有广泛适用性，能在组合式带臂的核心设置中实现近似最优性能，并提供了理论极限的证据。

Abstract: Combinatorial bandits extend the classical bandit framework to settings where
the learner selects multiple arms in each round, motivated by applications such
as online recommendation and assortment optimization. While extensions of upper
confidence bound (UCB) algorithms arise naturally in this context, adapting arm
elimination methods has proved more challenging. We introduce a novel
elimination scheme that partitions arms into three categories (confirmed,
active, and eliminated), and incorporates explicit exploration to update these
sets. We demonstrate the efficacy of our algorithm in two settings: the
combinatorial multi-armed bandit with general graph feedback, and the
combinatorial linear contextual bandit. In both cases, our approach achieves
near-optimal regret, whereas UCB-based methods can provably fail due to
insufficient explicit exploration. Matching lower bounds are also provided.

</details>


### [23] [Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation](https://arxiv.org/abs/2510.23636)
*Thaweerath Phisannupawong,Joshua Julian Damanik,Han-Lim Choi*

Main category: cs.LG

TL;DR: 利用轻量级大语言模型的多模态框架，将轨迹表示与文本航行信息整合用于实时航班延误预测，达到亚分钟级误差并具备良好扩展性。


<details>
  <summary>Details</summary>
Motivation: 航班延误对空中交通管理造成显著影响，需结合轨迹数据和文本信息（天气、航行公告等）提升预测准确性与时效性，帮助网路性能优化和空管决策。

Method: 提出基于轻量级LLM的多模态延误预测框架；将轨迹数据转化为语言模态并与飞行信息、天气、NOTAM等文本信息结合，进行跨模态信息对齐与推理；支持实时更新以融合新信息。

Result: 实验结果显示在源源不断的信息上下文中实现亚分钟级预测误差，能够有效利用延迟来源相关的上下文信息；具备实用性和可扩展性适用于真实运营。

Conclusion: 将语言理解与轨迹信息的跨模态适配相结合，可以提升延误预测的准确性与实时性；该框架具备落地潜力和对现实空中交通管理的实用性。

Abstract: Flight delay prediction has become a key focus in air traffic management, as
delays highlight inefficiencies that impact overall network performance. This
paper presents a lightweight large language model-based multimodal flight delay
prediction, formulated from the perspective of air traffic controllers
monitoring aircraft delay after entering the terminal area. The approach
integrates trajectory representations with textual aeronautical information,
including flight information, weather reports, and aerodrome notices, by
adapting trajectory data into the language modality to capture airspace
conditions. Experimental results show that the model consistently achieves
sub-minute prediction error by effectively leveraging contextual information
related to the sources of delay. The framework demonstrates that linguistic
understanding, when combined with cross-modality adaptation of trajectory
information, enhances delay prediction. Moreover, the approach shows
practicality and scalability for real-world operations, supporting real-time
updates that refine predictions upon receiving new operational information.

</details>


### [24] [Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning](https://arxiv.org/abs/2510.24356)
*Suman Sanyal*

Main category: cs.LG

TL;DR: 提出 Perception Learning (PeL) 的范式，通过任务无关的感知接口优化、与下游决策解耦，关注可验证的感知属性和不变性。


<details>
  <summary>Details</summary>
Motivation: 解决感知与决策耦合带来的不稳定性与信息丢失问题，追求鲁棒、信息丰富且几何受控的表示。

Method: 将感知接口 f_1: X f0Z 与决策接口 g_1: Z f0Y 分离，使用无标签的感知信号进行优化，提出感知属性的形式定义与不变量，证明对保留充分不变量的更新与贝叶斯任务风险梯度正交，同时提出一组任务无关的评估指标。

Result: 给出理论框架、分解方法及不变量的独立性证明，提出用于评估感知质量的指标集合；未给出具体数据集上的实验。

Conclusion: PeL 提供感知与决策的系统分离及感知质量认证的评估工具，为鲁棒感知学习提供理论支撑与实验指标。

Abstract: We introduce Perception Learning (PeL), a paradigm that optimizes an agent's
sensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnostic
signals, decoupled from downstream decision learning
$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-free
perceptual properties, such as stability to nuisances, informativeness without
collapse, and controlled geometry, assessed via objective
representation-invariant metrics. We formalize the separation of perception and
decision, define perceptual properties independent of objectives or
reparameterizations, and prove that PeL updates preserving sufficient
invariants are orthogonal to Bayes task-risk gradients. Additionally, we
provide a suite of task-agnostic evaluation metrics to certify perceptual
quality.

</details>


### [25] [Combining Textual and Structural Information for Premise Selection in Lean](https://arxiv.org/abs/2510.23637)
*Job Petrovčič,David Eliecer Narvaez Denis,Ljupčo Todorovski*

Main category: cs.LG

TL;DR: Graph-augmented premise selection using dense embeddings and a heterogeneous dependency graph improves Lean premise retrieval by >25% over the ReProver baseline on LeanDojo.


<details>
  <summary>Details</summary>
Motivation: Premise selection is a key bottleneck in scaling theorem proving for large formal libraries. Existing language-based methods treat premises in isolation and ignore relational dependencies; leveraging the graph of dependencies could improve accuracy.

Method: Combine dense text embeddings of Lean formalizations with a graph neural network operating on a heterogeneous dependency graph that encodes state–premise and premise–premise relations.

Result: On the LeanDojo Benchmark, the proposed method outperforms the ReProver language-based baseline by over 25% across standard retrieval metrics.

Conclusion: Incorporating relational information via graph structure enhances premise selection, demonstrating that graph-augmented representations can significantly improve scaling of theorem proving in large formal corpora.

Abstract: Premise selection is a key bottleneck for scaling theorem proving in large
formal libraries. Yet existing language-based methods often treat premises in
isolation, ignoring the web of dependencies that connects them. We present a
graph-augmented approach that combines dense text embeddings of Lean
formalizations with graph neural networks over a heterogeneous dependency graph
capturing both state--premise and premise--premise relations. On the LeanDojo
Benchmark, our method outperforms the ReProver language-based baseline by over
25% across standard retrieval metrics. These results demonstrate the power of
relational information for more effective premise selection.

</details>


### [26] [Eigenfunction Extraction for Ordered Representation Learning](https://arxiv.org/abs/2510.24672)
*Burak Varıcı,Che-Ping Tsai,Ritabrata Ray,Nicholas M. Boffi,Pradeep Ravikumar*

Main category: cs.LG

TL;DR: 提出一个模块化框架，用以从上下文核中提取有序、可辨识的本征函数，克服现有对比/非对比学习仅恢复核前几个本征函数的局限；结合低秩近似与 Rayleigh商优化实现本征函数提取，并在合成核与真实图像数据上验证，所得到的本征值可作为特征重要性分数，支持自适应维度的效率-精度权衡。


<details>
  <summary>Details</summary>
Motivation: 现有目标（对比/非对比学习）隐式进行上下文核的谱分解，但仅恢复核的前几本征函数的线性子空间；完整谱分解有助于理解特征排序与重要性，因此需要可辨识、可排序的本征函数，并且需与上下文核兼容且具备可扩展性。

Method: 提出一个通用、模块化的框架，设计可满足关键需求的组件，使之兼容上下文核且可扩展；并将低秩近似与 Rayleigh商优化这两大范式对齐到该框架，以实现本征函数的提取。

Result: 在合成核上进行了验证，并在真实图像数据集上展示，所恢复的本征值作为有效的特征重要性分数，可用于通过自适应维度表示实现的效率-精度权衡的特征选择。

Conclusion: 该框架实现了有序、可辨识本征函数的提取，为上下文相关表示学习中的特征排序与维度自适应提供了理论与实践基础。

Abstract: Recent advances in representation learning reveal that widely used
objectives, such as contrastive and non-contrastive, implicitly perform
spectral decomposition of a contextual kernel, induced by the relationship
between inputs and their contexts. Yet, these methods recover only the linear
span of top eigenfunctions of the kernel, whereas exact spectral decomposition
is essential for understanding feature ordering and importance. In this work,
we propose a general framework to extract ordered and identifiable
eigenfunctions, based on modular building blocks designed to satisfy key
desiderata, including compatibility with the contextual kernel and scalability
to modern settings. We then show how two main methodological paradigms,
low-rank approximation and Rayleigh quotient optimization, align with this
framework for eigenfunction extraction. Finally, we validate our approach on
synthetic kernels and demonstrate on real-world image datasets that the
recovered eigenvalues act as effective importance scores for feature selection,
enabling principled efficiency-accuracy tradeoffs via adaptive-dimensional
representations.

</details>


### [27] [Integrating Genomics into Multimodal EHR Foundation Models](https://arxiv.org/abs/2510.23639)
*Jonathan Amar,Edward Liu,Alessandra Breschi,Liangliang Zhang,Pouya Kheradpour,Sylvia Li,Lisa Soleymani Lehmann,Alessandro Giulianelli,Matt Edwards,Yugang Jia,David Nola,Raghav Mani,Pankaj Vats,Jesse Tetreault,T. J. Chen,Cory Y. McLean*

Main category: cs.LG

TL;DR: 提出一种在电子健康记录（EHR）基础上融合多组学数据的基础模型，以多模态方式将多遗传风险评分（PRS）作为核心数据模态，与EHR数据共同学习以提升疾病预测与个性化治疗的潜力，尤其在AoU数据上对2型糖尿病等疾病的预测表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有EHR驱动的预测模型多依赖临床记录，忽略个体的遗传 predisposition，限制预测能力、个性化与公平性。本研究尝试通过将基因组信息（PRS）整合到EHR基础模型中，构建更全面的健康画像并提升跨模态推断能力。

Method: 构建一个结合临床EHR数据与多模态基因组信息的EHR基础模型，利用All of Us研究计划的多样化数据进行训练与评估，借鉴生成式AI在基础模型的进展来增强预测能力与可解释性；探索迁移学习以实现自定义分类任务，提升架构的适用性与效率。

Result: 在AoU数据上评估，模型对多种疾病的发病预测具有价值，尤其对2型糖尿病的预测表现突出，并揭示PRS与EHR数据之间的相互作用。研究还展示了对自定义分类任务的迁移学习能力，体现架构的通用性与效率。

Conclusion: 该方法为疾病预测、前瞻性健康管理、风险分层和个性化治疗策略提供新洞见，为在真实世界证据中实现更个性化、更加公平且可操作的医疗保健奠定基础。

Abstract: This paper introduces an innovative Electronic Health Record (EHR) foundation
model that integrates Polygenic Risk Scores (PRS) as a foundational data
modality, moving beyond traditional EHR-only approaches to build more holistic
health profiles. Leveraging the extensive and diverse data from the All of Us
(AoU) Research Program, this multimodal framework aims to learn complex
relationships between clinical data and genetic predispositions. The
methodology extends advancements in generative AI to the EHR foundation model
space, enhancing predictive capabilities and interpretability. Evaluation on
AoU data demonstrates the model's predictive value for the onset of various
conditions, particularly Type 2 Diabetes (T2D), and illustrates the interplay
between PRS and EHR data. The work also explores transfer learning for custom
classification tasks, showcasing the architecture's versatility and efficiency.
This approach is pivotal for unlocking new insights into disease prediction,
proactive health management, risk stratification, and personalized treatment
strategies, laying the groundwork for more personalized, equitable, and
actionable real-world evidence generation in healthcare.

</details>


### [28] [Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning](https://arxiv.org/abs/2510.23640)
*Zihao Jing,Yan Sun,Yan Yi Li,Sugitha Janarthanan,Alana Deng,Pingzhao Hu*

Main category: cs.LG

TL;DR: MuMo 提出一种面向分子多模态表征的结构化融合框架，通过结构化融合管线（SFP）将二维拓扑与三维几何统一为稳定的结构先验，并通过渐进注入（PI）将该先验不对称地注入序列流以避免模态坍塌，在State Space骨干上实现长程依赖和鲁棒信息传播。实验在29个TDC/MoleculeNet基准任务上平均提升2.7%，在22个任务中排名第一，LD50任务提升27%。代码可用。


<details>
  <summary>Details</summary>
Motivation: 多模态分子建模常受3D构象不稳定性与模态坍塌影响，亟需一个稳定的融合策略以兼顾2D拓扑与3D几何信息，并避免直接简单拼接造成的模态干扰。

Method: 引入结构化融合管线（SFP）将2D拓扑与3D几何合并成统一的结构先验；使用渐进注入（PI）以不对称方式将上述先验注入到序列流中，既保留各模态的特性又实现跨模态信息互补；基于State Space骨干实现长程依赖与鲁棒信息传播。

Result: 在29项基准任务（TDC和MoleculeNet）上，MuMo相较最佳基线平均提升2.7%，22项任务排名第一，LD50任务提升27%，显示对3D构象噪声的鲁棒性及多模态融合的有效性。

Conclusion: MuMo证明了通过结构化融合和渐进注入的设计能够提升分子多模态表示的鲁棒性与泛化能力，代码已公开。

Abstract: Multimodal molecular models often suffer from 3D conformer unreliability and
modality collapse, limiting their robustness and generalization. We propose
MuMo, a structured multimodal fusion framework that addresses these challenges
in molecular representation through two key strategies. To reduce the
instability of conformer-dependent fusion, we design a Structured Fusion
Pipeline (SFP) that combines 2D topology and 3D geometry into a unified and
stable structural prior. To mitigate modality collapse caused by naive fusion,
we introduce a Progressive Injection (PI) mechanism that asymmetrically
integrates this prior into the sequence stream, preserving modality-specific
modeling while enabling cross-modal enrichment. Built on a state space
backbone, MuMo supports long-range dependency modeling and robust information
propagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) and
MoleculeNet, MuMo achieves an average improvement of 2.7% over the
best-performing baseline on each task, ranking first on 22 of them, including a
27% improvement on the LD50 task. These results validate its robustness to 3D
conformer noise and the effectiveness of multimodal fusion in molecular
representation. The code is available at: github.com/selmiss/MuMo.

</details>


### [29] [Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging](https://arxiv.org/abs/2510.23641)
*Aaron Wang,Zihan Zhao,Subash Katel,Vivekanand Gyanchand Sahu,Elham E Khoda,Abhijith Gandrakota,Jennifer Ngadiuba,Richard Cavanaugh,Javier Duarte*

Main category: cs.LG

TL;DR: SAL-T 以线性注意力实现高效的物理感知变换器，在粒子分布的区域间进行注意力分配，并加入卷积捕捉局部相关性。实现了接近全注意力的分类性能，同时显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 解决高能物理中大数据吞吐环境下变压器的二次复杂度与高资源需求问题；在保持线性注意力的同时，融入物理先验的区域划分与局部相关性建模，以提升粒子碰撞数据的分类能力。

Method: 提出 SAL-T：对 linformer 进行物理启发改进；基于动量/角度等运动学特征对粒子进行空间分区，在区域之间计算注意力；并使用卷积层捕捉局部相关性（受喷射物理启发）。在 jet 分类任务中优于标准 linformer，并且在不牺牲太多精度的前提下达到接近全注意力的表现，同时资源占用更少、推理延迟更低；在 ModelNet10 数据集上也验证了线性注意力趋势。

Result: 在 Jet 分类任务中，SAL-T 的性能优于标准 linformer，且接近全注意力变换器；总体上实现了显著的资源与延迟优势，且在通用点云数据集 ModelNet10 上也显示出相似的趋势。

Conclusion: SAL-T 提供一种高效、可扩展的注意力机制，结合物理启发的分区和局部卷积，在高吞吐环境下实现与全注意力相当的分类效果，同时显著降低计算成本与延迟，适用于高能物理和点云任务的实际部署。

Abstract: Transformers are very effective in capturing both global and local
correlations within high-energy particle collisions, but they present
deployment challenges in high-data-throughput environments, such as the CERN
LHC. The quadratic complexity of transformer models demands substantial
resources and increases latency during inference. In order to address these
issues, we introduce the Spatially Aware Linear Transformer (SAL-T), a
physics-inspired enhancement of the linformer architecture that maintains
linear attention. Our method incorporates spatially aware partitioning of
particles based on kinematic features, thereby computing attention between
regions of physical significance. Additionally, we employ convolutional layers
to capture local correlations, informed by insights from jet physics. In
addition to outperforming the standard linformer in jet classification tasks,
SAL-T also achieves classification results comparable to full-attention
transformers, while using considerably fewer resources with lower latency
during inference. Experiments on a generic point cloud classification dataset
(ModelNet10) further confirm this trend. Our code is available at
https://github.com/aaronw5/SAL-T4HEP.

</details>


### [30] [Efficient Low Rank Attention for Long-Context Inference in Large Language Models](https://arxiv.org/abs/2510.23649)
*Tenghui Li,Guoxu Zhou,Xuyang Zhao,Yuning Qiu,Qibin Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As the length of input text grows, the key-value (KV) cache in LLMs imposes
prohibitive GPU memory costs and limits long-context inference on resource
constrained devices. Existing approaches, such as KV quantization and pruning,
reduce memory usage but suffer from numerical precision loss or suboptimal
retention of key-value pairs. We introduce Low Rank Query and Key attention
(LRQK), a two-stage framework that jointly decomposes the full-precision query
and key matrices into compact rank-\(r\) factors during the prefill stage, and
then uses these low-dimensional projections to compute proxy attention scores
in \(\mathcal{O}(lr)\) time at each decode step. By selecting only the
top-\(k\) tokens and a small fixed set of recent tokens, LRQK employs a mixed
GPU-CPU cache with a hit-and-miss mechanism that transfers only missing
full-precision KV pairs, thereby preserving exact attention outputs while
reducing CPU-GPU data movement. Extensive experiments on the RULER and
LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK
matches or surpasses leading sparse-attention methods in long context settings,
while delivering significant memory savings with minimal loss in accuracy. Our
code is available at https://github.com/tenghuilee/LRQK.

</details>


### [31] [Beyond Hidden-Layer Manipulation: Semantically-Aware Logit Interventions for Debiasing LLMs](https://arxiv.org/abs/2510.23650)
*Wei Xia*

Main category: cs.LG

TL;DR: 提出静态与动态两种零-shot logits 层去偏方法；动态在不显著损失流畅性的前提下将偏见降低高达70%，logits 干预优于隐藏层去偏，语义感知的 logits 干预在对齐的 LLMs 上稳定有效。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型中的偏见问题，寻找无需微调的零-shot去偏策略，提高去偏的稳定性与适用性。

Method: 提出 Static 与 Dynamic 两种零-shot logits-layer 去偏方法。Dynamic 着眼于在降低偏见的同时控制对文本语义和流畅性的影响；Logits intervention 作为核心干预，效果优于隐藏层去偏；引入语义感知机制以提升干预的稳健性。

Result: 实验表明 Dynamic 可将偏见降低约70%，且流畅性损失很小；logits 干预优于隐藏层方法；语义感知的 logits 干预在对齐的 LLMs 上表现出稳定且有效的去偏效果。

Conclusion: 基于 logits 层的零-shot 去偏方法，尤其是语义感知的 logits 干预，能够在不进行显式微调的前提下实现稳定且有效的去偏；动态方法在偏见降低与语言质量之间达到良好折中。

Abstract: We proposed Static and Dynamic -- two zero-shot logits-layer debiasing
methods. Dynamic reduces bias by up to 70% with minimal fluency loss. Logits
intervention outperforms hidden-layer approaches. We show semantic-aware logits
intervention is stable and effective for debiasing aligned LLMs.

</details>


### [32] [The Structural Scalpel: Automated Contiguous Layer Pruning for Large Language Models](https://arxiv.org/abs/2510.23652)
*Yao Lu,Yuqi Li,Wenbin Xie,Shanqing Yu,Qi Xuan,Zhaowei Zhu,Shiping Wen*

Main category: cs.LG

TL;DR: 提出了一种连续层 pruning 框架 CLP，通过可微分凹门控自动识别最佳连续层段以 pruning，并通过截断端点微调策略仅对相邻层进行微调以恢复性能；在 LLaMA2/3、Qwen 的 7B-70B 规模上显著优于现有方法，在 20% pruning 下对 LLaMA3-70B 的性能保留约 95.34%，可与量化结合进一步压缩。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型的计算成本高，资源受限设备部署困难。现有层 pruning 往往依赖手工设计的度量来逐层 pruning，忽略层之间的依赖关系，导致信息流中断和性能下降。需要考虑层之间的依赖并实现高效、可微分的剪裁策略。

Method: 提出两大创新：1) 可微分的凹门控（concave gate）算法，基于梯度优化自动识别最优的连续层段进行 pruning；2) 截断端点微调策略，通过仅微调被剪裁段相邻的层来有效恢复模型性能。

Result: 在多种模型架构与尺寸（7B–70B，包含 LLaMA2/3、Qwen）上进行广泛实验，CLP 显著优于现有基线。在 20% pruning 下，CLP 在 LLaMA3-70B 上达到约 95.34% 的性能保留，相对于基线提升约 4.29%–30.52% 的性能。并且可与量化无缝结合，进一步压缩且性能损失较小。

Conclusion: CLP 为大模型的连续层 pruning 提供了有效且可扩展的解决方案，能在较高的 pruning 率下保持接近原始性能，并具备与量化耦合的潜力，适合边缘设备部署的实际应用。

Abstract: Although large language models (LLMs) have achieved revolutionary
breakthroughs in many fields, their large model size and high computational
cost pose significant challenges for practical deployment on
resource-constrained edge devices. To this end, layer pruning has been proposed
to reduce the computational overhead by directly removing redundant layers.
However, existing layer pruning methods typically rely on hand-crafted metrics
to evaluate and remove individual layers, while ignoring the dependencies
between layers. This can disrupt the model's information flow and severely
degrade performance. To address these issues, we propose CLP, a novel
continuous layer pruning framework that introduces two key innovations: a
differentiable concave gate algorithm that automatically identifies the best
continuous layer segments for pruning via gradient-based optimization; and a
cutoff endpoint tuning strategy that effectively restores model performance by
fine-tuning only the layers adjacent to the pruned segments. Extensive
experiments across multiple model architectures (including LLaMA2, LLaMA3 and
Qwen) and sizes (from $7$B to $70$B parameters) show that CLP significantly
outperforms existing state-of-the-art baselines. For example, at a pruning rate
of $20\%$, CLP achieves an average performance retention of $95.34\%$ on
LLaMA3-70B, outperforming baselines by $4.29\%$-$30.52\%$. Furthermore, CLP can
be seamlessly combined with quantization to further compress the model with
only a slight performance loss.

</details>


### [33] [A machine learning framework integrating seed traits and plasma parameters for predicting germination uplift in crops](https://arxiv.org/abs/2510.23657)
*Saklain Niam,Tashfiqur Rahman,Md. Amjad Patwary,Mukarram Hossain*

Main category: cs.LG

TL;DR: 建立了首个机器学习框架来预测冷等离子体（CP）诱导的种子发芽提升，覆盖大豆、大麦、向日葵、萝卜和番茄在介电击穿放电（DBD）等离子体下的发芽改变量。 Extrа Trees（ET）模型表现最佳，R^2 ~0.919–0.925，MAE ~2.62–1.23（按子项），RMSE ~3.21；在特征简化后R^2提升至0.925。研究揭示 hormetic 响应：7–15 kV、200–500 s 区间最优，超出20 kV或暴露时间过长则抑制发芽；放电功率≥100 W且低暴露时间有利于发芽速率。不同物种与品种预测差异较大，萝卜和大豆预测较稳健，向日葵波动较大；并将框架嵌入 MLflow，形成精准农业中的CP种子发芽优化决策工具。


<details>
  <summary>Details</summary>
Motivation: 解决冷等离子体处理下种子-等离子体-环境相互作用导致的发芽预测困难，亟需一个可跨物种、跨参数的预测框架来优化CP处理以提升产量与稳定性。

Method: 比较多种模型（GB、XGB、ET及混合方法），以ET表现最佳；覆盖大豆、萝卜、糖麦、向日葵、番茄等作物及若干品种；进行特征降维和模型参数调优；对发芽 uplift 进行量化预测，并分析 hormetic 特征和放电功率对结果的支配作用；结果以MAE、RMSE、R^2等指标评估，并部署在 MLflow。

Result: ET模型在原始数据集上R^2=0.919、RMSE=3.21、MAE=2.62，特征降维后R^2提升至0.925；按物种/品种：萝卜MAE=1.46、大豆MAE=2.05、向日葵MAE=3.80；品种中，Williams/Mae=1.23、Sari=1.33，Arian=2.86、Nyírési fekete=3.74；揭示放电功率是主导因素，発芽速率在≥100 W且暴露时间较短时达到最大值；框架实现可用于精准农业中的CP发芽优化决策。

Conclusion: 本研究提供一个跨物种的、基于机器学习的CP发芽预测与优化工具，揭示 hormetic 响应和放电功率的重要性，并通过 MLflow 实现可部署的决策支持，以推动CP在精准农业中的应用。

Abstract: Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet
outcomes remain difficult to predict due to complex seed--plasma--environment
interactions. This study introduces the first machine learning framework to
forecast germination uplift in soybean, barley, sunflower, radish, and tomato
under dielectric barrier discharge (DBD) plasma. Among the models tested (GB,
XGB, ET, and hybrids), Extra Trees (ET) performed best (R\textsuperscript{2} =
0.919; RMSE = 3.21; MAE = 2.62), improving to R\textsuperscript{2} = 0.925
after feature reduction. Engineering analysis revealed a hormetic response:
negligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for
200--500 s, and reduced germination beyond 20 kV or prolonged exposures.
Discharge power was also a dominant factor, with germination rate maximizing at
$\geq$100 W with low exposure time. Species and cultivar-level predictions
showed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high
consistency, while sunflower remained slightly higher variable (MAE = 3.80).
Among cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,
while Arian (2.86) and Ny\'{\i}rs\'{e}gi fekete (3.74) were comparatively
poorly captured. This framework was also embedded into MLflow, providing a
decision-support tool for optimizing CP seed germination in precision
agriculture.

</details>


### [34] [Quantum Machine Learning for Image Classification: A Hybrid Model of Residual Network with Quantum Support Vector Machine](https://arxiv.org/abs/2510.23659)
*Md. Farhan Shahriyar,Gazi Tanbhir,Abdullah Md Raihan Chy*

Main category: cs.LG

TL;DR: 提出基于 ResNet-50 特征 + PCA 降维，再用 QSVM（含 ZZ、Z、Pauli-X 等特征映射）进行分类的混合量子-经典模型，用于土豆病害识别，Z 映射的 QSVM 取得 99.23% 的高准确率，优于 SVM 与 RF。


<details>
  <summary>Details</summary>
Motivation: 解决高维图像特征带来的计算与分类挑战，探索量子映射在提升分类性能和效率方面的潜力。

Method: 过程是：用 ResNet-50 提取 RGB 图像特征；用 PCA 降维；将降维后的特征输入到 QSVM，比较 ZZ、Z、Pauli-X 等不同量子特征映射；以 SVM 和随机森林为对比，采用五折分层交叉验证。

Result: Z 映射 QSVM 获得最高准确率 99.23%，优于 SVM 和 RF；其他映射也有表现，但未达该水平。

Conclusion: 结果表明量子-经典混合模型在图像分类、尤其是植物病害识别任务上具有潜在优势，呈现将量子计算融入经典深度学习的可行性。

Abstract: Recently, there has been growing attention on combining quantum machine
learning (QML) with classical deep learning approaches, as computational
techniques are key to improving the performance of image classification tasks.
This study presents a hybrid approach that uses ResNet-50 (Residual Network)
for feature extraction and Quantum Support Vector Machines (QSVM) for
classification in the context of potato disease detection. Classical machine
learning as well as deep learning models often struggle with high-dimensional
and complex datasets, necessitating advanced techniques like quantum computing
to improve classification efficiency. In our research, we use ResNet-50 to
extract deep feature representations from RGB images of potato diseases. These
features are then subjected to dimensionality reduction using Principal
Component Analysis (PCA). The resulting features are processed through QSVM
models which apply various quantum feature maps such as ZZ, Z, and Pauli-X to
transform classical data into quantum states. To assess the model performance,
we compared it with classical machine learning algorithms such as Support
Vector Machine (SVM) and Random Forest (RF) using five-fold stratified
cross-validation for comprehensive evaluation. The experimental results
demonstrate that the Z-feature map-based QSVM outperforms classical models,
achieving an accuracy of 99.23 percent, surpassing both SVM and RF models. This
research highlights the advantages of integrating quantum computing into image
classification and provides a potential disease detection solution through
hybrid quantum-classical modeling.

</details>


### [35] [AI-Driven Carbon Monitoring: Transformer-Based Reconstruction of Atmospheric CO2 in Canadian Poultry Regions](https://arxiv.org/abs/2510.23663)
*Padmanabhan Jagannathan Prajesh,Kaliaperumal Ragunath,Miriam Gordon,Bruce Rathgeber,Suresh Neethirajan*

Main category: cs.LG

TL;DR: A spatiotemporal transformer with wavelet representations (ST-ViWT) reconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 over southern Canada, emphasizing poultry regions, achieving high accuracy and robust validation with TCCON.


<details>
  <summary>Details</summary>
Motivation: Accurate, gap-filled, uncertainty-quantified XCO2 maps over agricultural landscapes are crucial for emissions accounting, policy assessment, and targeted mitigation, particularly in poultry-dense regions with sparse satellite observations.

Method: Integrates wavelet time-frequency representations with transformer attention, fusing meteorology, vegetation indices, topography, and land cover. Trains and evaluates on 2024 OCO-2 data; outputs continuous XCO2 fields with quantified uncertainties.

Result: ST-ViWT achieves R2 = 0.984, RMSE = 0.468 ppm on 2024 OCO-2 data; 92.3% of gap-filled predictions within ±1 ppm. Independent TCCON validation shows bias -0.14 ppm, r = 0.928, reproducing late-summer drawdown. Spatially, poultry regions show r = 0.43 between facility density and XCO2; high-density areas have larger amplitudes (≈9.57 ppm) and more summer variability.

Conclusion: ST-ViWT provides seamless, uncertainty-aware CO2 surfaces at 0.25° resolution, enabling year-round coverage with sparse observations, enabling integration with inventories and precision livestock platforms for improved emission benchmarking and policy-relevant mitigation.

Abstract: Accurate mapping of column-averaged CO2 (XCO2) over agricultural landscapes
is essential for guiding emission mitigation strategies. We present a
Spatiotemporal Vision Transformer with Wavelets (ST-ViWT) framework that
reconstructs continuous, uncertainty-quantified XCO2 fields from OCO-2 across
southern Canada, emphasizing poultry-intensive regions. The model fuses wavelet
time-frequency representations with transformer attention over meteorology,
vegetation indices, topography, and land cover. On 2024 OCO-2 data, ST-ViWT
attains R2 = 0.984 and RMSE = 0.468 ppm; 92.3 percent of gap-filled predictions
lie within +/-1 ppm. Independent validation with TCCON shows robust
generalization (bias = -0.14 ppm; r = 0.928), including faithful reproduction
of the late-summer drawdown. Spatial analysis across 14 poultry regions reveals
a moderate positive association between facility density and XCO2 (r = 0.43);
high-density areas exhibit larger seasonal amplitudes (9.57 ppm) and enhanced
summer variability. Compared with conventional interpolation and standard
machine-learning baselines, ST-ViWT yields seamless 0.25 degree CO2 surfaces
with explicit uncertainties, enabling year-round coverage despite sparse
observations. The approach supports integration of satellite constraints with
national inventories and precision livestock platforms to benchmark emissions,
refine region-specific factors, and verify interventions. Importantly,
transformer-based Earth observation enables scalable, transparent, spatially
explicit carbon accounting, hotspot prioritization, and policy-relevant
mitigation assessment.

</details>


### [36] [Transformers from Compressed Representations](https://arxiv.org/abs/2510.23665)
*Juan C. Leon Alcazar,Mattia Soldan,Mohammad Saatialsoruji,Alejandro Pardo,Hani Itani,Juan Camilo Perez,Bernard Ghanem*

Main category: cs.LG

TL;DR: TEMPEST 使用压缩文件的字节流结构作为 tokenization/编码策略，直接从压缩数据流中学习语义表征，降低 token 数量和资源消耗，达到与 state-of-the-art 相近的准确度同时提高效率。


<details>
  <summary>Details</summary>
Motivation: 在数据高效存储和传输场景中，压缩文件潜在的表示学习空间尚未充分挖掘。通过利用压缩数据流的结构设计 tokenization 和编码，使 Transformers 能直接从压缩数据中学习语义表征，避免解码或原始字节层处理的开销。

Method: 提出 TEMPEST，通过针对压缩文件的字节流结构设计 tokenization/编码策略，并使标准 Transformer 能直接处理压缩数据流以学习语义表征，显著减少用于语义分类的 token 数量。

Result: 在多数据集、编码方案和模态上进行广泛实验，TEMPEST 的准确率达到与最先进方法相当，同时在内存和计算方面实现效率提升。

Conclusion: 通过利用压缩数据流的本征结构，TEMPEST 为基于 Transformer 的压缩数据表示学习提供了更高效的途径，能在保持竞争力的准确性的同时降低资源需求。

Abstract: Compressed file formats are the corner stone of efficient data storage and
transmission, yet their potential for representation learning remains largely
underexplored. We introduce TEMPEST (TransformErs froM comPressed
rEpreSenTations), a method that exploits the inherent byte-stream structure of
compressed files to design an effective tokenization and encoding strategy. By
leveraging this compact encoding, a standard transformer can directly learn
semantic representations from compressed data streams, bypassing the need for
raw byte-level processing or full media decoding. Our proposal substantially
reduces the number of tokens required for semantic classification, thereby
lowering both computational complexity and memory usage. Through extensive
experiments across diverse datasets, coding schemes, and modalities, we show
that TEMPEST achieves accuracy competitive wit the state-of-the-art while
delivering efficiency gains in memory and compute.

</details>


### [37] [Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization](https://arxiv.org/abs/2510.23667)
*Amin Heyrani Nobari,Lyle Regenwetter,Cyril Picard,Ligong Han,Faez Ahmed*

Main category: cs.LG

TL;DR: 提出 OAT（Optimize Any Topology），一种基础模型框架，能够直接给出任意长宽比、分辨率、体积分数、载荷和固定条件下的最小成本拓扑布局，实现分辨率无关、快速的拓扑优化，并附带 OpenTO 数据集。


<details>
  <summary>Details</summary>
Motivation: 结构拓扑优化在计算上成本高，现有深度学习方法受限于固定网格、有限边界条件和事后优化，难以在广泛情境中部署。因此需要一个通用、快速、可处理不同分辨率和条件的框架来提升实际应用和研究。

Method: 结合一个对分辨率与形状无关的自编码器、一个隐式神场解码器，以及一个条件潜扩散模型，该模型在 OpenTO 数据集上训练，OpenTO 包含 2.2 百万优化结构和覆盖 200 万组边界条件的配置。系统实现对从 64×64 到 256×256 的多分辨率、长宽比最高 10:1 的推理，并提供大规模数据集以推动生成模型在逆设计中的研究。

Result: 在四项公开基准和两项未知场景上，OAT 相较于最佳先前模型在平均柔度指标上降低高达 90%，且在单个 GPU 上跨分辨率（64×64 至 256×256）与不同长宽比的推理时间均小于 1 秒。实现了通用、快速、分辨率无关的物理感知拓扑优化框架。

Conclusion: 确立了 OAT 作为一种通用的、快速的物理感知拓扑优化框架，并提供了大规模数据集以促进逆设计中的生成建模研究；代码与数据可在 GitHub 获取。

Abstract: Structural topology optimization (TO) is central to engineering design but
remains computationally intensive due to complex physics and hard constraints.
Existing deep-learning methods are limited to fixed square grids, a few
hand-coded boundary conditions, and post-hoc optimization, preventing general
deployment. We introduce Optimize Any Topology (OAT), a foundation-model
framework that directly predicts minimum-compliance layouts for arbitrary
aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines
a resolution- and shape-agnostic autoencoder with an implicit neural-field
decoder and a conditional latent-diffusion model trained on OpenTO, a new
corpus of 2.2 million optimized structures covering 2 million unique
boundary-condition configurations. On four public benchmarks and two
challenging unseen tests, OAT lowers mean compliance up to 90% relative to the
best prior models and delivers sub-1 second inference on a single GPU across
resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These
results establish OAT as a general, fast, and resolution-free framework for
physics-aware topology optimization and provide a large-scale dataset to spur
further research in generative modeling for inverse design. Code & data can be
found at https://github.com/ahnobari/OptimizeAnyTopology.

</details>


### [38] [Traffic flow forecasting, STL decomposition, Hybrid model, LSTM, ARIMA, XGBoost, Intelligent transportation systems](https://arxiv.org/abs/2510.23668)
*Fujiang Yuan,Yangrui Fan,Xiaohuan Bing,Zhen Tian,Chunhong Yuan,Yankang Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate traffic flow forecasting is essential for intelligent transportation
systems and urban traffic management. However, single model approaches often
fail to capture the complex, nonlinear, and multi scale temporal patterns in
traffic flow data. This study proposes a decomposition driven hybrid framework
that integrates Seasonal Trend decomposition using Loess (STL) with three
complementary predictive models. STL first decomposes the original time series
into trend, seasonal, and residual components. Then, a Long Short Term Memory
(LSTM) network models long term trends, an Autoregressive Integrated Moving
Average (ARIMA) model captures seasonal periodicity, and an Extreme Gradient
Boosting (XGBoost) algorithm predicts nonlinear residual fluctuations. The
final forecast is obtained through multiplicative integration of the sub model
predictions. Using 998 traffic flow records from a New York City intersection
between November and December 2015, results show that the LSTM ARIMA XGBoost
hybrid model significantly outperforms standalone models including LSTM, ARIMA,
and XGBoost across MAE, RMSE, and R squared metrics. The decomposition strategy
effectively isolates temporal characteristics, allowing each model to
specialize, thereby improving prediction accuracy, interpretability, and
robustness.

</details>


### [39] [DBLoss: Decomposition-based Loss Function for Time Series Forecasting](https://arxiv.org/abs/2510.23672)
*Xiangfei Qiu,Xingjian Wu,Hanyin Cheng,Xvyuan Liu,Chenjuan Guo,Jilin Hu,Bin Yang*

Main category: cs.LG

TL;DR: 提出一种基于分解的损失函数 DBLoss，通过在预测区间内对季节性与趋势进行分解并分别计算损失，再加权整合，提升时间序列预测的效果。


<details>
  <summary>Details</summary>
Motivation: MSE 在预测区间内往往无法准确捕捉季节性和趋势；现有分解模块在前向传播中难以对齐损失与分解目标，因此需要更贴合预测区间特征的损失函数。

Method: 使用指数移动平均在预测区间内对时间序列进行季节和趋势分解，分别计算两部分的损失，然后按权重合并，形成通用的损失 DBLoss；可与任意深度学习预测模型结合。

Result: 大量实证结果表明，与SOTA模型相比，DBLoss在多组真实世界数据集上显著提高了预测性能，提出了时间序列损失设计的新视角。

Conclusion: DBLoss 提供了一种简洁有效的思路，将分解思想直接融入损失函数，具有良好的泛化性和可应用性，能够推动未来损失设计的发展。

Abstract: Time series forecasting holds significant value in various domains such as
economics, traffic, energy, and AIOps, as accurate predictions facilitate
informed decision-making. However, the existing Mean Squared Error (MSE) loss
function sometimes fails to accurately capture the seasonality or trend within
the forecasting horizon, even when decomposition modules are used in the
forward propagation to model the trend and seasonality separately. To address
these challenges, we propose a simple yet effective Decomposition-Based Loss
function called DBLoss. This method uses exponential moving averages to
decompose the time series into seasonal and trend components within the
forecasting horizon, and then calculates the loss for each of these components
separately, followed by weighting them. As a general loss function, DBLoss can
be combined with any deep learning forecasting model. Extensive experiments
demonstrate that DBLoss significantly improves the performance of
state-of-the-art models across diverse real-world datasets and provides a new
perspective on the design of time series loss functions.

</details>


### [40] [Beyond Prompt Engineering: Neuro-Symbolic-Causal Architecture for Robust Multi-Objective AI Agents](https://arxiv.org/abs/2510.23682)
*Gokturk Aytug Akarlar*

Main category: cs.LG

TL;DR: Chimera combines an LLM strategist, a formally verified symbolic constraint engine, and a causal inference module to create a robust autonomous agent. In 52-week realistic e-commerce simulations, Chimera outperforms LLM-only and LLM+symbolic baselines, achieving higher profits and brand trust, and showing zero constraint violations via TLA+. The study argues architectural design, not mere prompt engineering, drives reliability in production settings.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents in high-stakes domains exhibit catastrophic brittleness and sensitivity to prompt framing. There is a need for architectural safeguards that ensure reliability, safety, and interpretability beyond prompt engineering.

Method: The study performs 52-week simulations in a realistic e-commerce setting, testing organizational biases toward volume vs. margin optimization. Baselines: (i) LLM-only agents; (ii) LLM with symbolic constraints. Chimera enables counterfactual reasoning and ensures constraints; benchmarks measure financial performance (profit, revenue, margins), volume, brand trust, and constraint safety. TLA+ formal verification verifies constraint adherence across scenarios; results include detailed economic and trust metrics.

Result: LLM-only agents exhibit catastrophic failures: up to -$99K in volume scenarios or -48.6% trust in margin scenarios. Adding symbolic constraints mitigates disasters but yields only 43–87% of Chimera’s profit. Chimera achieves the highest returns: around $1.52M and $1.96M in the reported metrics, with some cases exceeding $2.2M, and improves brand trust by +1.8% to +10.8% (in some cases up to +20.86%). Chimera demonstrates prompt-agnostic robustness. TLA+ formal verification reports zero constraint violations across all tested scenarios.

Conclusion: Architectural design, integrating learning, symbolic reasoning, and causal counterfactuals, is critical for reliable autonomous agents in production environments. Prompt engineering alone is insufficient. The work provides open-source implementations and live demos to support reproducibility.

Abstract: Large language models show promise as autonomous decision-making agents, yet
their deployment in high-stakes domains remains fraught with risk. Without
architectural safeguards, LLM agents exhibit catastrophic brittleness:
identical capabilities produce wildly different outcomes depending solely on
prompt framing. We present Chimera, a neuro-symbolic-causal architecture that
integrates three complementary components - an LLM strategist, a formally
verified symbolic constraint engine, and a causal inference module for
counterfactual reasoning. We benchmark Chimera against baseline architectures
(LLM-only, LLM with symbolic constraints) across 52-week simulations in a
realistic e-commerce environment featuring price elasticity, trust dynamics,
and seasonal demand. Under organizational biases toward either volume or margin
optimization, LLM-only agents fail catastrophically (total loss of \$99K in
volume scenarios) or destroy brand trust (-48.6% in margin scenarios). Adding
symbolic constraints prevents disasters but achieves only 43-87% of Chimera's
profit. Chimera consistently delivers the highest returns (\$1.52M and \$1.96M
respectively, some cases +\$2.2M) while improving brand trust (+1.8% and
+10.8%, some cases +20.86%), demonstrating prompt-agnostic robustness. Our TLA+
formal verification proves zero constraint violations across all scenarios.
These results establish that architectural design not prompt engineering
determines the reliability of autonomous agents in production environments. We
provide open-source implementations and interactive demonstrations for
reproducibility.

</details>


### [41] [Parallel BiLSTM-Transformer networks for forecasting chaotic dynamics](https://arxiv.org/abs/2510.23685)
*Junwen Ma,Mingyu Ge,Yisen Wang,Yong Zhang,Weicheng Fu*

Main category: cs.LG

TL;DR: 提出一个并行的 Transformer + BiLSTM 混合框架来预测混沌时间序列。双分支分别捕捉全局依赖与局部时间特征，在特征融合层融合，使用时间延迟嵌入对 Lorenz 系统的自主演化预测和未观测变量推断任务进行评估，结果优于单分支模型。


<details>
  <summary>Details</summary>
Motivation: 混沌系统对初始条件高度敏感，现有模型难以同时捕捉局部特征和全局依赖，亟需一种能同时兼顾局部与全局信息的预测框架，以提高长期预测和状态重建的准确性。

Method: 设计双分支并行架构：Transformer 分支负责捕捉远程全局依赖，BiLSTM 分支侧重提取局部时间特征；在特征融合层进行表示融合。以时滞嵌入的状态向量作为输入，对 Lorenz 系统进行两类任务评估：自治演化预测（递归外推轨迹）和未观测变量推断（从部分观测的时滞嵌入推断未观测态变量）。

Result: 实验证明该混合框架在两个任务上均优于单分支模型，表现出对混沌系统预测的鲁棒性和有效性。

Conclusion: 通过互补表示的融合，该工作实现了对混沌时间序列的更准确预测与状态重建，验证了并行 Transformer 与 BiLSTM 的协同作用在处理高维、非线性、强依赖的时间序列中的潜力。

Abstract: The nonlinear nature of chaotic systems results in extreme sensitivity to
initial conditions and highly intricate dynamical behaviors, posing fundamental
challenges for accurately predicting their evolution. To overcome the
limitation that conventional approaches fail to capture both local features and
global dependencies in chaotic time series simultaneously, this study proposes
a parallel predictive framework integrating Transformer and Bidirectional Long
Short-Term Memory (BiLSTM) networks. The hybrid model employs a dual-branch
architecture, where the Transformer branch mainly captures long-range
dependencies while the BiLSTM branch focuses on extracting local temporal
features. The complementary representations from the two branches are fused in
a dedicated feature-fusion layer to enhance predictive accuracy. As
illustrating examples, the model's performance is systematically evaluated on
two representative tasks in the Lorenz system. The first is autonomous
evolution prediction, in which the model recursively extrapolates system
trajectories from the time-delay embeddings of the state vector to evaluate
long-term tracking accuracy and stability. The second is inference of
unmeasured variable, where the model reconstructs the unobserved states from
the time-delay embeddings of partial observations to assess its
state-completion capability. The results consistently indicate that the
proposed hybrid framework outperforms both single-branch architectures across
tasks, demonstrating its robustness and effectiveness in chaotic system
prediction.

</details>


### [42] [On the Societal Impact of Machine Learning](https://arxiv.org/abs/2510.23693)
*Joachim Baumann*

Main category: cs.LG

TL;DR: 本论文/博士论文聚焦机器学习的社会影响，提出在ML系统中更恰当地衡量公平性、系统地分解系统以预测偏差动态，并设计有效干预以在保持系统效用的同时减少算法歧视，并讨论未来挑战与研究方向（包括生成式AI）


<details>
  <summary>Details</summary>
Motivation: 由于ML系统越来越多地影响关键决策，数据驱动的系统往往在缺乏明确公平性考量时可能产生歧视性影响，因此需要改进公平性衡量、理解偏差的演化过程，并提供可执行的干预策略来降低歧视并维持系统性能

Method: 提出用于公平性测量的改进框架、对ML系统进行系统性的解构以预测偏差动态，以及设计可落地的干预措施以降低歧视并维持系统效用

Result: 贡献在于提供更合适的公平性衡量、对偏差动态的系统性分解能力，以及能够降低算法歧视的有效干预，同时保持系统效用

Conclusion: 总结指出当前面临的挑战与未来研究方向，强调在生成式人工智能等日益融入社会背景下，ML的社会影响需与更广泛的社会价值观保持一致，提出研究框架的基础性作用

Abstract: This PhD thesis investigates the societal impact of machine learning (ML). ML
increasingly informs consequential decisions and recommendations, significantly
affecting many aspects of our lives. As these data-driven systems are often
developed without explicit fairness considerations, they carry the risk of
discriminatory effects. The contributions in this thesis enable more
appropriate measurement of fairness in ML systems, systematic decomposition of
ML systems to anticipate bias dynamics, and effective interventions that reduce
algorithmic discrimination while maintaining system utility. I conclude by
discussing ongoing challenges and future research directions as ML systems,
including generative artificial intelligence, become increasingly integrated
into society. This work offers a foundation for ensuring that ML's societal
impact aligns with broader social values.

</details>


### [43] [MUStReason: A Benchmark for Diagnosing Pragmatic Reasoning in Video-LMs for Multimodal Sarcasm Detection](https://arxiv.org/abs/2510.23727)
*Anisha Saha,Varsha Suresh,Timothy Hospedales,Vera Demberg*

Main category: cs.LG

TL;DR: 提出 MUStReason 诊断性基准用于多模态视频语言模型的讽刺检测，结合模态相关线索标注及推理步骤；并提出 PragCoT 框架以引导模型关注隐含意图，区分感知与推理。


<details>
  <summary>Details</summary>
Motivation: 讽刺检测需要跨模态线索和语言层面的推理，现有多模态模型在识别相关线索和进行 pragmatically 推理方面存在不足；需要可解释的诊断性基准来评估和改进模型推理能力。

Method: 构建 MUStReason，给出模态特定相关线索的注释和潜在推理步骤，用于诊断 sarcasm；评估 VideoLMs 的讽刺分类性能，并对生成的推理进行量化与质性分析（将问题分解为感知与推理）；提出 PragCoT 框架，引导 VideoLMs 关注隐含意图而非字面意思。

Result: 提供了MUStReason 基准及其注释与推理步骤的示例，展示对 VideoLMs 的诊断性评估能力；对生成的推理进行感知与推理分离的定量与定性分析；演示 PragCoT 如何改善对隐含意图的关注。具体性能增益需在实验部分给出。

Conclusion: 指出要提升讽刺检测的多模态推理能力，需要可解释的诊断基准和引导性推理框架；MUStReason 与 PragCoT 为评估和改进 VideoLMs 处理讽刺语义的能力提供新工具。

Abstract: Sarcasm is a specific type of irony which involves discerning what is said
from what is meant. Detecting sarcasm depends not only on the literal content
of an utterance but also on non-verbal cues such as speaker's tonality, facial
expressions and conversational context. However, current multimodal models
struggle with complex tasks like sarcasm detection, which require identifying
relevant cues across modalities and pragmatically reasoning over them to infer
the speaker's intention. To explore these limitations in VideoLMs, we introduce
MUStReason, a diagnostic benchmark enriched with annotations of
modality-specific relevant cues and underlying reasoning steps to identify
sarcastic intent. In addition to benchmarking sarcasm classification
performance in VideoLMs, using MUStReason we quantitatively and qualitatively
evaluate the generated reasoning by disentangling the problem into perception
and reasoning, we propose PragCoT, a framework that steers VideoLMs to focus on
implied intentions over literal meaning, a property core to detecting sarcasm.

</details>


### [44] [Revealing the Potential of Learnable Perturbation Ensemble Forecast Model for Tropical Cyclone Prediction](https://arxiv.org/abs/2510.23794)
*Jun Liu,Tao Zhou,Jiarui Li,Xiaohui Zhong,Peng Zhang,Jie Feng,Lei Chen,Hao Li*

Main category: cs.LG

TL;DR: FuXi-ENS introduces a learnable perturbation scheme for ensemble generation and surpasses ECMWF-ENS in several TC forecasting aspects, especially track accuracy and dynamical representation, while still underestimating intensity.


<details>
  <summary>Details</summary>
Motivation: Enhance tropical cyclone ensemble forecasting by better capturing atmospheric nonlinearity and reducing computational costs through learnable perturbations, addressing uncertainty in TC tracks, intensity, and associated dynamics.

Method: Systematic comparison between FuXi-ENS and ECMWF-ENS using all 90 global tropical cyclones from 2018; evaluation across TC-related physical variables, track and intensity forecasts, and associated dynamical/thermodynamical fields; analysis of moisture turbulent energy distributions and their relation to TC warm core to assess how well each system captures large-scale circulation.

Result: FuXi-ENS provides advantages in predicting TC-related physical variables and yields more accurate track forecasts with reduced ensemble spread; it underestimates intensity relative to observations; dynamical analyses show FuXi-ENS better captures large-scale circulation and concentrates moisture turbulent energy near the TC warm core, while ECMWF-ENS shows broader dispersion.

Conclusion: Learnable perturbations can improve AI-based ensemble forecasts of tropical cyclones and other extreme weather events, offering insights for advancing ensemble prediction and potentially reducing computational costs while improving skill in key forecast aspects.

Abstract: Tropical cyclones (TCs) are highly destructive and inherently uncertain
weather systems. Ensemble forecasting helps quantify these uncertainties, yet
traditional systems are constrained by high computational costs and limited
capability to fully represent atmospheric nonlinearity. FuXi-ENS introduces a
learnable perturbation scheme for ensemble generation, representing a novel
AI-based forecasting paradigm. Here, we systematically compare FuXi-ENS with
ECMWF-ENS using all 90 global TCs in 2018, examining their performance in
TC-related physical variables, track and intensity forecasts, and the
associated dynamical and thermodynamical fields. FuXi-ENS demonstrates clear
advantages in predicting TC-related physical variables, and achieves more
accurate track forecasts with reduced ensemble spread, though it still
underestimates intensity relative to observations. Further dynamical and
thermodynamical analyses reveal that FuXi-ENS better captures large-scale
circulation, with moisture turbulent energy more tightly concentrated around
the TC warm core, whereas ECMWF-ENS exhibits a more dispersed distribution.
These findings highlight the potential of learnable perturbations to improve TC
forecasting skill and provide valuable insights for advancing AI-based ensemble
prediction of extreme weather events that have significant societal impacts.

</details>


### [45] [Learning Interpretable Features in Audio Latent Spaces via Sparse Autoencoders](https://arxiv.org/abs/2510.23802)
*Nathan Paek,Yongyi Zang,Qihui Yang,Randal Leistikow*

Main category: cs.LG

TL;DR: 将稀疏自编码器用于音频生成模型潜在空间的可解释映射，通过线性映射将SAE特征映射到离散声学属性，实现对音高、振幅、音色的可控分析，覆盖连续与离散潜在空间。


<details>
  <summary>Details</summary>
Motivation: 音频数据的密集性质和语义难以保留在潜在表示中，需要一种方法来解码、解释以及操控音频生成过程中的声学特征。

Method: 在音频自编码器潜在向量上训练稀疏自编码器(SAE)，再学习从SAE特征到离散化声学属性（pitch、amplitude、timbre）的线性映射；在DiffRhythm-VAE、EnCodec、WavTokenizer等连续/离散潜在空间上验证；分析DiffRhythm模型中声学属性的演变。

Result: 实现可控操纵与分析，能够观察生成过程中的pitch、timbre、loudness随时间的演变；在多种潜在空间上验证了该框架的可行性。

Conclusion: 该框架目前在音频模态上得到验证，但具有良好扩展性，未来可扩展至对其他模态潜在空间的可解释分析。

Abstract: While sparse autoencoders (SAEs) successfully extract interpretable features
from language models, applying them to audio generation faces unique
challenges: audio's dense nature requires compression that obscures semantic
meaning, and automatic feature characterization remains limited. We propose a
framework for interpreting audio generative models by mapping their latent
representations to human-interpretable acoustic concepts. We train SAEs on
audio autoencoder latents, then learn linear mappings from SAE features to
discretized acoustic properties (pitch, amplitude, and timbre). This enables
both controllable manipulation and analysis of the AI music generation process,
revealing how acoustic properties emerge during synthesis. We validate our
approach on continuous (DiffRhythm-VAE) and discrete (EnCodec, WavTokenizer)
audio latent spaces, and analyze DiffRhythm, a state-of-the-art text-to-music
model, to demonstrate how pitch, timbre, and loudness evolve throughout
generation. While our work is only done on audio modality, our framework can be
extended to interpretable analysis of visual latent space generation models.

</details>


### [46] [How do simple rotations affect the implicit bias of Adam?](https://arxiv.org/abs/2510.23804)
*Adela DePavia,Vasileios Charisopoulos,Rebecca Willett*

Main category: cs.LG

TL;DR: Adam 及自适应梯度方法的“丰富性偏差”在正交变换下不稳定，旋转会使 Adam 失去对 Bayes 边界的偏好，甚至比梯度下降更劣；通过一种正交变换的重参数化方法，可以使一阶方法对数据旋转等变性，从而恢复 Adam 的偏好。


<details>
  <summary>Details</summary>
Motivation: 理解自适应优化方法（如 Adam、Adagrad）对模型泛化的影响，特别是在特征空间的正交旋转下，为何 Adam 的优势可能消失或反转，以及如何设计方法使其对数据变换保持鲁棒。

Method: 对 Adam 在正交变换下的行为进行理论与经验分析，说明其坐标预条件化导致对数据旋转敏感，从而可能从丰富边界偏好转变为线性边界；引入并应用一种最近提出的重参数化方法，将优化目标进行正交变换，从而使任一一阶方法对数据旋转具有等变性，并通过实验验证其对恢复 Adam 的边界偏好有效。

Result: 在数据分布的轻微旋转下，Adam 的丰富性偏好可能被削弱，甚至产生更劣于 GD 的线性边界；而重参数化的方法能够赋予一阶优化方法对数据旋转的等变性，显著降低旋转带来的负效应，并在实践中恢复对丰富边界的偏好。

Conclusion: 正交重参数化为一阶优化方法提供了对数据旋转的不变性，能在一定程度上维持或恢复 Adam 的泛化优势；该思路对设计对数据变换具有鲁棒性的一阶优化算法具有指导意义。

Abstract: Adaptive gradient methods such as Adam and Adagrad are widely used in machine
learning, yet their effect on the generalization of learned models -- relative
to methods like gradient descent -- remains poorly understood. Prior work on
binary classification suggests that Adam exhibits a ``richness bias,'' which
can help it learn nonlinear decision boundaries closer to the Bayes-optimal
decision boundary relative to gradient descent. However, the coordinate-wise
preconditioning scheme employed by Adam renders the overall method sensitive to
orthogonal transformations of feature space. We show that this sensitivity can
manifest as a reversal of Adam's competitive advantage: even small rotations of
the underlying data distribution can make Adam forfeit its richness bias and
converge to a linear decision boundary that is farther from the Bayes-optimal
decision boundary than the one learned by gradient descent. To alleviate this
issue, we show that a recently proposed reparameterization method -- which
applies an orthogonal transformation to the optimization objective -- endows
any first-order method with equivariance to data rotations, and we empirically
demonstrate its ability to restore Adam's bias towards rich decision
boundaries.

</details>


### [47] [Combining SHAP and Causal Analysis for Interpretable Fault Detection in Industrial Processes](https://arxiv.org/abs/2510.23817)
*Pedro Cortes dos Santos,Matheus Becali Rocha,Renato A Krohling*

Main category: cs.LG

TL;DR: 通过SHAP解释与多算法DAG因果分析，在Tennessee Eastman Process上提升故障检测的准确性并给出可操作的因果洞见。


<details>
  <summary>Details</summary>
Motivation: 解决工业过程故障检测中可解释性不足和操作性有限的问题；将可解释性与因果分析结合以揭示故障传播机制。

Method: 使用SHAP提升特征解释性，将高维复杂问题转化为透明形式；再结合多算法生成的有向无环图进行因果分析，揭示故障传播的关键机制。

Result: SHAP与因果分析呈现一致结论，核心要素如冷却与分离系统被确认为对故障发展关键的驱动因素；提升检测准确性并提供可操作的故障起源洞见。

Conclusion: 将预测能力与因果理解结合，构筑更鲁棒的工业过程监控工具，填补该领域在可解释性与因果分析结合方面的研究空白。

Abstract: Industrial processes generate complex data that challenge fault detection
systems, often yielding opaque or underwhelming results despite advanced
machine learning techniques. This study tackles such difficulties using the
Tennessee Eastman Process, a well-established benchmark known for its intricate
dynamics, to develop an innovative fault detection framework. Initial attempts
with standard models revealed limitations in both performance and
interpretability, prompting a shift toward a more tractable approach. By
employing SHAP (SHapley Additive exPlanations), we transform the problem into a
more manageable and transparent form, pinpointing the most critical process
features driving fault predictions. This reduction in complexity unlocks the
ability to apply causal analysis through Directed Acyclic Graphs, generated by
multiple algorithms, to uncover the underlying mechanisms of fault propagation.
The resulting causal structures align strikingly with SHAP findings,
consistently highlighting key process elements-like cooling and separation
systems-as pivotal to fault development. Together, these methods not only
enhance detection accuracy but also provide operators with clear, actionable
insights into fault origins, a synergy that, to our knowledge, has not been
previously explored in this context. This dual approach bridges predictive
power with causal understanding, offering a robust tool for monitoring complex
manufacturing environments and paving the way for smarter, more interpretable
fault detection in industrial systems.

</details>


### [48] [ScaLoRA: Optimally Scaled Low-Rank Adaptation for Efficient High-Rank Fine-Tuning](https://arxiv.org/abs/2510.23818)
*Yilang Zhang,Xiaodong Yang,Yiwei Cai,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 通过累积连续低秩增量形成的高秩权重更新来提升 LoRA 的效果，同时给出解析的缩放系数来接近全微调的效果，显著提升在大语言模型上的收敛速度和性能。


<details>
  <summary>Details</summary>
Motivation: LoRA 将权重更新局限于低秩子空间，可能限制模型表现并减慢收敛。本工作提出一种在保持低秩更新带来的效率的同时，逐步累积高秩更新以更接近全微调。

Method: 在每次更新中识别使损失函数最小化的最优低秩矩阵，并通过对原始低秩矩阵的列进行缩放来组成一个高秩的等价更新，以实现无重启的高效优化。缩放系数可解析地求出。

Result: 在参数规模达 12B 的大语言模型上，与现有 LoRA 变体相比，该方法在多项任务上表现出稳定的性能提升和更快的收敛，包括自然语言理解、常识推理和数学题解等。

Conclusion: 提出的高秩累积方法在保持高效的同时提升性能与收敛速率，理论上给出缩放系数的解析解，对跨任务的鲁棒性良好。

Abstract: As large language models (LLMs) continue to scale in size, the computational
overhead has become a major bottleneck for task-specific fine-tuning. While
low-rank adaptation (LoRA) effectively curtails this cost by confining the
weight updates to a low-dimensional subspace, such a restriction can hinder
effectiveness and slow convergence. This contribution deals with these
limitations by accumulating progressively a high-rank weight update from
consecutive low-rank increments. Specifically, the per update optimal low-rank
matrix is identified to minimize the loss function and closely approximate full
fine-tuning. To endow efficient and seamless optimization without restarting,
this optimal choice is formed by appropriately scaling the columns of the
original low-rank matrix. Rigorous performance guarantees reveal that the
optimal scaling can be found analytically. Extensive numerical tests with
popular LLMs scaling up to 12 billion parameters demonstrate a consistent
performance gain and fast convergence relative to state-of-the-art LoRA
variants on diverse tasks including natural language understanding, commonsense
reasoning, and mathematical problem solving.

</details>


### [49] [A PDE-Informed Latent Diffusion Model for 2-m Temperature Downscaling](https://arxiv.org/abs/2510.23866)
*Paul Rosu,Muchang Bahng,Erick Jiang,Rico Zhu,Vahid Tarokh*

Main category: cs.LG

TL;DR: Physics-conditioned latent diffusion model for dynamical downscaling of atmospheric data, focusing on high-res 2-m temperature fields. Adds a PDE-based loss (finite-difference advection-diffusion) in pixel space to enforce physical consistency; base diffusion with residual against UNet; code available on GitHub.


<details>
  <summary>Details</summary>
Motivation: Need physically coherent high-resolution downscaled atmospheric fields. Standard diffusion models may produce plausible fields but can violate basic physics; imposing a PDE-based training objective aims to improve physical plausibility and consistency with advection-diffusion dynamics.

Method: Extend a pre-existing diffusion architecture with a residual formulation relative to a reference UNet. Train with an additional PDE loss term computed in full-resolution pixel space by decoding latent representations and applying a finite-difference approximation to an effective advection-diffusion balance. Assess whether this PDE loss yields further regularization beyond standard diffusion training.

Result: Empirical observations show that conventional diffusion training already yields low PDE residuals; fine-tuning with the PDE loss can further regularize the model and enhance physical plausibility of generated fields.

Conclusion: Incorporating a PDE-informed loss during fine-tuning can modestly improve physical consistency of downscaled atmospheric fields, especially when the diffusion model already adheres closely to physical constraints; the approach is implemented with full-resolution PDE residual computation and is available in the public codebase (GitHub).

Abstract: This work presents a physics-conditioned latent diffusion model tailored for
dynamical downscaling of atmospheric data, with a focus on reconstructing
high-resolution 2-m temperature fields. Building upon a pre-existing diffusion
architecture and employing a residual formulation against a reference UNet, we
integrate a partial differential equation (PDE) loss term into the model's
training objective. The PDE loss is computed in the full resolution (pixel)
space by decoding the latent representation and is designed to enforce physical
consistency through a finite-difference approximation of an effective
advection-diffusion balance. Empirical observations indicate that conventional
diffusion training already yields low PDE residuals, and we investigate how
fine-tuning with this additional loss further regularizes the model and
enhances the physical plausibility of the generated fields. The entirety of our
codebase is available on Github, for future reference and development.

</details>


### [50] [GIFT: Group-relative Implicit Fine Tuning Integrates GRPO with DPO and UNA](https://arxiv.org/abs/2510.23868)
*Zhichao Wang*

Main category: cs.LG

TL;DR: GIFT 通过正则化隐式与显式奖励，将非凸优化转化为对齐损失（均方误差），在策略端实现高效、稳定的对齐，优于DPO/UNA/GRPO，且在数学任务上表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法要么充分利用显式奖励、要么利用隐式奖励，但常导致非凸优化、离线训练易过拟合、超参数多且难以探索。需要一个统一、高效、可在策略端探索的框架来同时利用隐式与显式奖励。

Method: 核心思路包含三部分：1) 将GRPO的在线多答案生成与归一化引入到训练过程；2) 采用DPO的隐式奖励形式；3) 结合UNA的隐式-显式奖励对齐原则。通过对隐式与显式奖励进行归一化，消除了一个导致难以使用隐式奖励的难题项；将目标转化为规范化奖励的均方误差（MSE），使优化从非凸转为凸、可导。保持on-policy，提升探索能力。

Result: 经验上，在数学基准上展现出更强的推理与对齐能力，训练收敛更快、过拟合显著减少、超参数更少且泛化更好，同时计算开销较低。

Conclusion: GIFT 提供一个将隐式与显式奖励统一对齐的策略端框架，相较离线方法在稳定性、效率和鲁棒性方面具有优势，且对复杂任务具较好扩展性。

Abstract: I propose \textbf{G}roup-relative \textbf{I}mplicit \textbf{F}ine
\textbf{T}uning (GIFT), a novel reinforcement learning framework for aligning
LLMs. Instead of directly maximizing cumulative rewards like PPO or GRPO, GIFT
minimizes the discrepancy between implicit and explicit reward models. It
combines three key ideas: (1) the online multi-response generation and
normalization of GRPO, (2) the implicit reward formulation of DPO, and (3) the
implicit-explicit reward alignment principle of UNA. By jointly normalizing the
implicit and explicit rewards, GIFT eliminates an otherwise intractable term
that prevents effective use of implicit rewards. This normalization transforms
the complex reward maximization objective into a simple mean squared error
(MSE) loss between the normalized reward functions, converting a non-convex
optimization problem into a convex, stable, and analytically differentiable
formulation. Unlike offline methods such as DPO and UNA, GIFT remains on-policy
and thus retains exploration capability. Compared to GRPO, it requires fewer
hyperparameters, converges faster, and generalizes better with significantly
reduced training overfitting. Empirically, GIFT achieves superior reasoning and
alignment performance on mathematical benchmarks while remaining
computationally efficient.

</details>


### [51] [RS-ORT: A Reduced-Space Branch-and-Bound Algorithm for Optimal Regression Trees](https://arxiv.org/abs/2510.23901)
*Cristobal Heredia,Pedro Chumpitaz-Flores,Kaixun Hua*

Main category: cs.LG

TL;DR: 提出 RS-ORT：在回归树学习中，使用只在树结构变量上分支的优化-分支定界算法，提供对百万级数据的可扩展性和全局最优性保证，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于混合整数规划的回归树学习要么仅适用于二进制特征，要么在连续且规模庞大的数据下计算困难。对连续特征进行简单二值化会损失全局最优性并产生过深的树。

Method: 将最优回归树训练重新表述为两阶段优化问题，提出 Reduced-Space Optimal Regression Trees (RS-ORT)，这是一个专门用于分支定界的算法，仅在树结构变量上分支。通过闭式叶子预测、经验阈值离散化、深度-1 子树解析等 bound-tightening 技巧，以及可分解的上下界策略，加速训练，并实现节点级的并行化。

Result: 在包含二进制与连续特征的多项回归基准上，RS-ORT 在训练与测试性能上优于最先进的方法。对于高达 2,000,000 条样本、含连续特征的数据集，四小时内可获得有保证的训练性能、结构更简单且泛化性更强的树。

Conclusion: RS-ORT 提供一个收敛性独立于样本规模的优化框架，具备良好可扩展性、并行性和实用性，能在大规模回归问题上实现全局最优性近似且保持良好泛化。

Abstract: Mixed-integer programming (MIP) has emerged as a powerful framework for
learning optimal decision trees. Yet, existing MIP approaches for regression
tasks are either limited to purely binary features or become computationally
intractable when continuous, large-scale data are involved. Naively binarizing
continuous features sacrifices global optimality and often yields needlessly
deep trees. We recast the optimal regression-tree training as a two-stage
optimization problem and propose Reduced-Space Optimal Regression Trees
(RS-ORT) - a specialized branch-and-bound (BB) algorithm that branches
exclusively on tree-structural variables. This design guarantees the
algorithm's convergence and its independence from the number of training
samples. Leveraging the model's structure, we introduce several bound
tightening techniques - closed-form leaf prediction, empirical threshold
discretization, and exact depth-1 subtree parsing - that combine with
decomposable upper and lower bounding strategies to accelerate the training.
The BB node-wise decomposition enables trivial parallel execution, further
alleviating the computational intractability even for million-size datasets.
Based on the empirical studies on several regression benchmarks containing both
binary and continuous features, RS-ORT also delivers superior training and
testing performance than state-of-the-art methods. Notably, on datasets with up
to 2,000,000 samples with continuous features, RS-ORT can obtain guaranteed
training performance with a simpler tree structure and a better generalization
ability in four hours.

</details>


### [52] [Key and Value Weights Are Probably All You Need: On the Necessity of the Query, Key, Value weight Triplet in Decoder-Only Transformers](https://arxiv.org/abs/2510.23912)
*Marko Karbevski,Antonij Mijoski*

Main category: cs.LG

TL;DR: 对 Query-Key-Value 三元组中的 Query 权重进行削减的理论与实证研究，提出 Query 权重冗余并在小型 GPT-3 结构上验证，实现>8% 的非嵌入/lm-head 参数减少，性能保持接近基线。


<details>
  <summary>Details</summary>
Motivation: 在自注意力机制中，Query、Key、Value 三元组是核心组成。若能将 Query 权重简化或冗余化，将显著提升模型参数效率及训练/推理成本。该工作尝试在理论层面揭示 Query 权重的冗余性，并在小型 GPT-3 架构上进行实证验证以评估可行性。

Method: 给出简化假设下的理论分析，证明 Query 权重具有冗余性并可通过等效参数化进行替代；在完整复杂度的 GPT-3 小型架构（包含层归一化、跳跃连接、权重衰减）上从头训练，比较减参数模型与标准基线在验证损失上的差异。

Result: 理论上证明了 Query 权重的冗余性；在 GPT-3 小型架构的实证中，减少后的模型在验证损失上与标准基线相近，且非嵌入/语言模型头参数减少超过 8%。

Conclusion: 该现象在理论与小型实证层面得到支持，提示在更大规模模型中进一步研究 Query 权重冗余性，以推动注意力机制的参数高效化与可扩展性。

Abstract: The Query, Key, Value weight triplet is a building block of current attention
mechanisms in state-of-the-art LLMs. We theoretically investigate whether this
triplet can be reduced, proving under simplifying assumptions that the Query
weights are redundant, thereby reducing the number of non-embedding/lm-head
parameters by over 8%. We validate the theory on full-complexity GPT-3 small
architectures (with layer normalization, skip connections, and weight decay)
trained from scratch, demonstrating that the reduced model achieves comparable
validation loss to standard baselines. These findings motivate the
investigation of the Query weight redundancy at scale.

</details>


### [53] [Geometry-Inspired Unified Framework for Discounted and Average Reward MDPs](https://arxiv.org/abs/2510.23914)
*Arsenii Mustafin,Xinyi Sheng,Dominik Baumann*

Main category: cs.LG

TL;DR: 将平均奖励与折现奖励两种MDP分析统一起来，扩展折现情形的几何解释到平均奖励，从而在唯一且遍历的最优策略条件下，证明值迭代具有几何收敛率。


<details>
  <summary>Details</summary>
Motivation: 解决长期研究中对MDP两类奖励设置的分析分歧，提供一个统一的几何框架并将折现结果推广至平均奖励情形，提升对值迭代收敛性的理解。

Method: 将对折现奖励的几何解释扩展至平均奖励的MDP，构建统一框架；在存在唯一且遍历的最优策略下，给出值迭代的几何收敛性证明。

Result: 得到一个统一的几何框架，将平均奖励的分析与折现奖励的分析统一起来，并在特定条件下证明值迭代的几何收敛速率。

Conclusion: 实现了平均奖励与折现奖励两种MDP分析的统一，并证明在唯一且遍历的最优策略下，值迭代具几何收敛。

Abstract: The theoretical analysis of Markov Decision Processes (MDPs) is commonly
split into two cases - the average-reward case and the discounted-reward case -
which, while sharing similarities, are typically analyzed separately. In this
work, we extend a recently introduced geometric interpretation of MDPs for the
discounted-reward case to the average-reward case, thereby unifying both. This
allows us to extend a major result known for the discounted-reward case to the
average-reward case: under a unique and ergodic optimal policy, the Value
Iteration algorithm achieves a geometric convergence rate.

</details>


### [54] [Differential Privacy: Gradient Leakage Attacks in Federated Learning Environments](https://arxiv.org/abs/2510.23931)
*Miguel Fernandez-de-Retana,Unai Zulaika,Rubén Sánchez-Corcuera,Aitor Almeida*

Main category: cs.LG

TL;DR: DP-SGD 能在一定程度上缓解梯度泄漏攻击，但会牺牲模型效用；PDP-SGD 在分类任务上保持较好性能，但对重建攻击无效。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，保护训练数据隐私，防止从共享的梯度中恢复私人信息；需要对差分隐私机制在现实分布式场景中的有效性进行实证评估。

Method: 对多种计算机视觉模型，在不同隐私等级下进行简单分类任务的训练；在模拟的FL环境中截获梯度并尝试进行私有数据重建，比较 DP-SGD 与 PDP-SGD 的防护效果。

Result: DP-SGD 能显著缓解梯度泄漏攻击，但对模型性能有中等程度的成本；PDP-SGD 维持了较强的分类性能，但对重建攻击并无实质防护效果。

Conclusion: 应超越理论保证，对隐私机制进行经验性评估；在分布式学习场景中，信息泄露可能成为一个关键威胁，因此需要选取合适的隐私保护策略。

Abstract: Federated Learning (FL) allows for the training of Machine Learning models in
a collaborative manner without the need to share sensitive data. However, it
remains vulnerable to Gradient Leakage Attacks (GLAs), which can reveal private
information from the shared model updates. In this work, we investigate the
effectiveness of Differential Privacy (DP) mechanisms - specifically, DP-SGD
and a variant based on explicit regularization (PDP-SGD) - as defenses against
GLAs. To this end, we evaluate the performance of several computer vision
models trained under varying privacy levels on a simple classification task,
and then analyze the quality of private data reconstructions obtained from the
intercepted gradients in a simulated FL environment. Our results demonstrate
that DP-SGD significantly mitigates the risk of gradient leakage attacks,
albeit with a moderate trade-off in model utility. In contrast, PDP-SGD
maintains strong classification performance but proves ineffective as a
practical defense against reconstruction attacks. These findings highlight the
importance of empirically evaluating privacy mechanisms beyond their
theoretical guarantees, particularly in distributed learning scenarios where
information leakage may represent an unassumable critical threat to data
security and privacy.

</details>


### [55] [Modeling Biological Multifunctionality with Echo State Networks](https://arxiv.org/abs/2510.23940)
*Anastasia-Maria Leventi-Peetz,Jörg-Volker Peetz,Kai Weber,Nikolaos Zacharis*

Main category: cs.LG

TL;DR: 提出了一个三维三组分反应扩散模型，与FitzHugh–Nagumo类似，用以捕捉生物系统的时空动力学，并通过用时间序列数据训练的Echo State Network（ESN）来重现系统行为，结果显示数据驱动的多功能ESN在生物动力学建模中具有可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 解决需要同时考虑非线性局部动力学与扩散传播的生物电生理过程的时空行为，探索数据驱动模型在复杂生物动力学中的应用潜力。

Method: 建立三维多组分反应扩散方程，数值求解生成时间序列数据；以这些数据训练并评估一个Echo State Network（ESN），以重现实验系统的动力学。

Result: ESN成功再现实验系统的动力学，能够捕捉关键的时空模式和动态行为，证明数据驱动的多功能ESN在生物动力学模拟中的可行性与有效性。

Conclusion: 数据驱动的ESN方法适用于模拟和预测复杂生物动力学系统，未来可扩展到更复杂的模型、增强泛化能力，以及提高对噪声和非平稳数据的鲁棒性等。

Abstract: In this work, a three-dimensional multicomponent reaction-diffusion model has
been developed, combining excitable-system dynamics with diffusion processes
and sharing conceptual features with the FitzHugh-Nagumo model. Designed to
capture the spatiotemporal behavior of biological systems, particularly
electrophysiological processes, the model was solved numerically to generate
time-series data. These data were subsequently used to train and evaluate an
Echo State Network (ESN), which successfully reproduced the system's dynamic
behavior. The results demonstrate that simulating biological dynamics using
data-driven, multifunctional ESN models is both feasible and effective.

</details>


### [56] [ChessQA: Evaluating Large Language Models for Chess Understanding](https://arxiv.org/abs/2510.23948)
*Qianfeng Wen,Zhenwei Tang,Ashton Anderson*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Chess provides an ideal testbed for evaluating the reasoning, modeling, and
abstraction capabilities of large language models (LLMs), as it has
well-defined structure and objective ground truth while admitting a wide
spectrum of skill levels. However, existing evaluations of LLM ability in chess
are ad hoc and narrow in scope, making it difficult to accurately measure LLM
chess understanding and how it varies with scale, post-training methodologies,
or architecture choices. We present ChessQA, a comprehensive benchmark that
assesses LLM chess understanding across five task categories (Structural,
Motifs, Short Tactics, Position Judgment, and Semantic), which approximately
correspond to the ascending abstractions that players master as they accumulate
chess knowledge, from understanding basic rules and learning tactical motifs to
correctly calculating tactics, evaluating positions, and semantically
describing high-level concepts. In this way, ChessQA captures a more
comprehensive picture of chess ability and understanding, going significantly
beyond the simple move quality evaluations done previously, and offers a
controlled, consistent setting for diagnosis and comparison. Furthermore,
ChessQA is inherently dynamic, with prompts, answer keys, and construction
scripts that can evolve as models improve. Evaluating a range of contemporary
LLMs, we find persistent weaknesses across all five categories and provide
results and error analyses by category. We will release the code, periodically
refreshed datasets, and a public leaderboard to support further research.

</details>


### [57] [A Pragmatic Way to Measure Chain-of-Thought Monitorability](https://arxiv.org/abs/2510.23966)
*Scott Emmons,Roland S. Zimmermann,David K. Elson,Rohin Shah*

Main category: cs.LG

TL;DR: 提出通过可读性（legibility）与覆盖度（coverage）两个指标来评估 Chain-of-Thought 的可监控性，借助一个 autorater 提示让任意强大语言模型计算这两项指标，并在前沿模型上进行验证；同时公开完整的 autorater 提示以供开发者追踪设计决策对监控性的影响；强调这是对抗性测试的补充。


<details>
  <summary>Details</summary>
Motivation: 在训练实践或模型架构变更可能削弱 CoT 的监控性背景下，需量化并保持可追踪的推理过程，以确保人类能理解并复现输出。

Method: 构建 autorater 提示，通过让LLM评估 CoT 的 legibility 与 coverage；先用合成降级进行 sanity-check；再在若干前沿模型和挑战性基准上应用评估并报告结果；同时公开完整的 autorater 提示。

Result: 实验显示相关模型具有较高的 monitorability；提供了一套可重复的评估工具，帮助开发者追踪设计决策对 monitorability 的影响。

Conclusion: 该度量应作为监控性测试的补充，与对抗性压力测试共同使用，以评估默认监控性，而非替代对抗性稳健性测试。

Abstract: While Chain-of-Thought (CoT) monitoring offers a unique opportunity for AI
safety, this opportunity could be lost through shifts in training practices or
model architecture. To help preserve monitorability, we propose a pragmatic way
to measure two components of it: legibility (whether the reasoning can be
followed by a human) and coverage (whether the CoT contains all the reasoning
needed for a human to also produce the final output). We implement these
metrics with an autorater prompt that enables any capable LLM to compute the
legibility and coverage of existing CoTs. After sanity-checking our prompted
autorater with synthetic CoT degradations, we apply it to several frontier
models on challenging benchmarks, finding that they exhibit high
monitorability. We present these metrics, including our complete autorater
prompt, as a tool for developers to track how design decisions impact
monitorability. While the exact prompt we share is still a preliminary version
under ongoing development, we are sharing it now in the hopes that others in
the community will find it useful. Our method helps measure the default
monitorability of CoT - it should be seen as a complement, not a replacement,
for the adversarial stress-testing needed to test robustness against
deliberately evasive models.

</details>


### [58] [An efficient probabilistic hardware architecture for diffusion-like models](https://arxiv.org/abs/2510.23972)
*Andraž Jelinčič,Owen Lockwood,Akhil Garlapati,Guillaume Verdon,Trevor McCourt*

Main category: cs.LG

TL;DR: 提出一种全晶体管概率性计算机，在硬件层面实现强大的去噪模型，系统级分析表明在简单图像基准上可与GPU性能相当，能耗约降低1万倍。


<details>
  <summary>Details</summary>
Motivation: 当前概率性AI快速发展催生专用随机计算机的需求，但现有方案往往受限于建模技术与异质、不可扩展的硬件。需要一种可扩展且高效的硬件实现，以在硬件层面支持强大去噪模型的概率推断。

Method: 设计并提出一种全晶体管概率性计算机，将去噪模型直接在硬件中实现；进行系统级分析评估性能与能耗，并与GPU进行对比。

Result: 系统级分析表明，基于该体系结构的器件在一个简单的图像基准上可达到与GPU相当的性能，同时能耗约为GPU的1e4倍。

Conclusion: 该工作展示了通过全晶体管概率性硬件实现高效推断的潜力，能够显著降低能耗，然而还需要在更复杂任务与大规模数据集上进行扩展与验证。

Abstract: The proliferation of probabilistic AI has promoted proposals for specialized
stochastic computers. Despite promising efficiency gains, these proposals have
failed to gain traction because they rely on fundamentally limited modeling
techniques and exotic, unscalable hardware. In this work, we address these
shortcomings by proposing an all-transistor probabilistic computer that
implements powerful denoising models at the hardware level. A system-level
analysis indicates that devices based on our architecture could achieve
performance parity with GPUs on a simple image benchmark using approximately
10,000 times less energy.

</details>


### [59] [Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.23974)
*Byeonghu Na,Minsang Park,Gyuwon Sim,Donghyeok Shin,HeeSun Bae,Mina Kang,Se Jung Kwon,Wanmo Kang,Il-Chul Moon*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Text-to-image diffusion models rely on text embeddings from a pre-trained
text encoder, but these embeddings remain fixed across all diffusion timesteps,
limiting their adaptability to the generative process. We propose Diffusion
Adaptive Text Embedding (DATE), which dynamically updates text embeddings at
each diffusion timestep based on intermediate perturbed data. We formulate an
optimization problem and derive an update rule that refines the text embeddings
at each sampling step to improve alignment and preference between the mean
predicted image and the text. This allows DATE to dynamically adapts the text
conditions to the reverse-diffused images throughout diffusion sampling without
requiring additional model training. Through theoretical analysis and empirical
results, we show that DATE maintains the generative capability of the model
while providing superior text-image alignment over fixed text embeddings across
various tasks, including multi-concept generation and text-guided image
editing. Our code is available at https://github.com/aailab-kaist/DATE.

</details>


### [60] [HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing](https://arxiv.org/abs/2510.23980)
*Guojing Cong,Tom Potok,Hamed Poursiami,Maryam Parsa*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present a novel algorithm, \hdgc, that marries graph convolution with
binding and bundling operations in hyperdimensional computing for transductive
graph learning. For prediction accuracy \hdgc outperforms major and popular
graph neural network implementations as well as state-of-the-art
hyperdimensional computing implementations for a collection of homophilic
graphs and heterophilic graphs. Compared with the most accurate learning
methodologies we have tested, on the same target GPU platform, \hdgc is on
average 9561.0 and 144.5 times faster than \gcnii, a graph neural network
implementation and HDGL, a hyperdimensional computing implementation,
respectively. As the majority of the learning operates on binary vectors, we
expect outstanding energy performance of \hdgc on neuromorphic and emerging
process-in-memory devices.

</details>


### [61] [Predicting Barge Tow Size on Inland Waterways Using Vessel Trajectory Derived Features: Proof of Concept](https://arxiv.org/abs/2510.23994)
*Geoffery Agorku,Sarah Hernandez,Hayley Hames,Cade Wagner*

Main category: cs.LG

TL;DR: 将AIS数据用于估计内陆水道翻船数量的端到端方法。通过卫星影像标注和AIS轨迹匹配，构建30个AIS特征，使用RFE筛选，比较六种回归模型，Poisson回归器表现最好，MAE=1.92，且12个特征具备较好预测性。未来工作包括在不同河道的迁移性验证，方法对港口管理和航运时间调度具有应用潜力。


<details>
  <summary>Details</summary>
Motivation: 解决内陆水道中驳船总数的实时、准确估计问题。由于驳船非自推动且现有监测系统受限，利用AIS流数据与机器学习来推断总数以提升海事态势感知。

Method: 从卫星影像中对Lower Mississippi River的驳船实例进行人工标注，并与AIS船舶轨迹进行时空匹配。基于船舶几何、动态运动和轨迹模式的30个AIS派生特征被构建出来，并通过递归特征消除(RFE)筛选最具预测性的变量。训练并评估六种回归模型（包括集成、核方法和广义线性模型）。最终选择Poisson回归器。

Result: Poisson回归器在测试集上达到最大性MAE为1.92艘驳船，使用了12个特征。特征重要性分析显示，表征船舶机动性的指标（航向熵、速度波动、航次长度等）对驳船数量的预测最具贡献。

Conclusion: 所提方法为提升海事领域态势感知提供了一个可扩展、易于实现的解决方案，具有在锁闸调度、港口管理和货运规划等方面的潜在应用。未来工作将探索该方法在不同内陆河道的可迁移性与环境条件差异下的模型迁移能力。

Abstract: Accurate, real-time estimation of barge quantity on inland waterways remains
a critical challenge due to the non-self-propelled nature of barges and the
limitations of existing monitoring systems. This study introduces a novel
method to use Automatic Identification System (AIS) vessel tracking data to
predict the number of barges in tow using Machine Learning (ML). To train and
test the model, barge instances were manually annotated from satellite scenes
across the Lower Mississippi River. Labeled images were matched to AIS vessel
tracks using a spatiotemporal matching procedure. A comprehensive set of 30
AIS-derived features capturing vessel geometry, dynamic movement, and
trajectory patterns were created and evaluated using Recursive Feature
Elimination (RFE) to identify the most predictive variables. Six regression
models, including ensemble, kernel-based, and generalized linear approaches,
were trained and evaluated. The Poisson Regressor model yielded the best
performance, achieving a Mean Absolute Error (MAE) of 1.92 barges using 12 of
the 30 features. The feature importance analysis revealed that metrics
capturing vessel maneuverability such as course entropy, speed variability and
trip length were most predictive of barge count. The proposed approach provides
a scalable, readily implementable method for enhancing Maritime Domain
Awareness (MDA), with strong potential applications in lock scheduling, port
management, and freight planning. Future work will expand the proof of concept
presented here to explore model transferability to other inland rivers with
differing operational and environmental conditions.

</details>


### [62] [Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.24012)
*Byeonghu Na,Mina Kang,Jiseok Kwak,Minsang Park,Jiwoo Shin,SeJoon Jun,Gayoung Lee,Jin-Hwa Kim,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出STG，一种无需再训练的文本嵌入引导方法，用于提升扩散模型的生成安全性，在不显著损害语义的前提下去除不当内容。


<details>
  <summary>Details</summary>
Motivation: 当前大规模文本与图像数据集往往包含不当或带偏见的内容，可能在恶意文本提示下生成有害输出。需要一种训练外的、高效的安全控制机制。

Method: 在采样阶段对文本嵌入进行调整，依据对期望最终去噪图像的安全性评估函数来引导嵌入，从而让模型产生更安全的输出，同时尽量保持原始语义；该方法无需额外训练，且理论上将模型分布对齐到安全约束。

Result: 在多种安全场景（如裸体、暴力、艺术风格移除）下，STG优于训练基线和训练-free基线，能去除不安全内容并保留输入的核心语义。

Conclusion: STG提供了一种高效的、训练外的安全增强途径，理论与实验均支持其有效性，且对生成质量的影响最小。

Abstract: Text-to-image models have recently made significant advances in generating
realistic and semantically coherent images, driven by advanced diffusion models
and large-scale web-crawled datasets. However, these datasets often contain
inappropriate or biased content, raising concerns about the generation of
harmful outputs when provided with malicious text prompts. We propose Safe Text
embedding Guidance (STG), a training-free approach to improve the safety of
diffusion models by guiding the text embeddings during sampling. STG adjusts
the text embeddings based on a safety function evaluated on the expected final
denoised image, allowing the model to generate safer outputs without additional
training. Theoretically, we show that STG aligns the underlying model
distribution with safety constraints, thereby achieving safer outputs while
minimally affecting generation quality. Experiments on various safety
scenarios, including nudity, violence, and artist-style removal, show that STG
consistently outperforms both training-based and training-free baselines in
removing unsafe content while preserving the core semantic intent of input
prompts. Our code is available at https://github.com/aailab-kaist/STG.

</details>


### [63] [NeuroPathNet: Dynamic Path Trajectory Learning for Brain Functional Connectivity Analysis](https://arxiv.org/abs/2510.24025)
*Guo Tianqi Guo,Chen Liping,Peng Ciyuan,Guo Jingjing,Ren Jing*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding the evolution of brain functional networks over time is of
great significance for the analysis of cognitive mechanisms and the diagnosis
of neurological diseases. Existing methods often have difficulty in capturing
the temporal evolution characteristics of connections between specific
functional communities. To this end, this paper proposes a new path-level
trajectory modeling framework (NeuroPathNet) to characterize the dynamic
behavior of connection pathways between brain functional partitions. Based on
medically supported static partitioning schemes (such as Yeo and Smith ICA), we
extract the time series of connection strengths between each pair of functional
partitions and model them using a temporal neural network. We validate the
model performance on three public functional Magnetic Resonance Imaging (fMRI)
datasets, and the results show that it outperforms existing mainstream methods
in multiple indicators. This study can promote the development of dynamic graph
learning methods for brain network analysis, and provide possible clinical
applications for the diagnosis of neurological diseases.

</details>


### [64] [Efficient Global-Local Fusion Sampling for Physics-Informed Neural Networks](https://arxiv.org/abs/2510.24026)
*Jiaqi Luo,Shixin Xu,Zhouwang Yang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The accuracy of Physics-Informed Neural Networks (PINNs) critically depends
on the placement of collocation points, as the PDE loss is approximated through
sampling over the solution domain. Global sampling ensures stability by
covering the entire domain but requires many samples and is computationally
expensive, whereas local sampling improves efficiency by focusing on
high-residual regions but may neglect well-learned areas, reducing robustness.
We propose a Global-Local Fusion (GLF) Sampling Strategy that combines the
strengths of both approaches. Specifically, new collocation points are
generated by perturbing training points with Gaussian noise scaled inversely to
the residual, thereby concentrating samples in difficult regions while
preserving exploration. To further reduce computational overhead, a lightweight
linear surrogate is introduced to approximate the global residual-based
distribution, achieving similar effectiveness at a fraction of the cost.
Together, these components, residual-adaptive sampling and residual-based
approximation, preserve the stability of global methods while retaining the
efficiency of local refinement. Extensive experiments on benchmark PDEs
demonstrate that GLF consistently improves both accuracy and efficiency
compared with global and local sampling strategies. This study provides a
practical and scalable framework for enhancing the reliability and efficiency
of PINNs in solving complex and high-dimensional PDEs.

</details>


### [65] [Spatio-temporal Multivariate Time Series Forecast with Chosen Variables](https://arxiv.org/abs/2510.24027)
*Zibo Liu,Zhe Jiang,Zelin Xu,Tingsong Xiao,Yupu Zhang,Zhengkun Xiao,Haibo Wang,Shigang Chen*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Spatio-Temporal Multivariate time series Forecast (STMF) uses the time series
of $n$ spatially distributed variables in a period of recent past to forecast
their values in a period of near future. It has important applications in
spatio-temporal sensing forecast such as road traffic prediction and air
pollution prediction. Recent papers have addressed a practical problem of
missing variables in the model input, which arises in the sensing applications
where the number $m$ of sensors is far less than the number $n$ of locations to
be monitored, due to budget constraints. We observe that the state of the art
assumes that the $m$ variables (i.e., locations with sensors) in the model
input are pre-determined and the important problem of how to choose the $m$
variables in the input has never been studied. This paper fills the gap by
studying a new problem of STMF with chosen variables, which optimally selects
$m$-out-of-$n$ variables for the model input in order to maximize the forecast
accuracy. We propose a unified framework that jointly performs variable
selection and model optimization for both forecast accuracy and model
efficiency. It consists of three novel technical components: (1) masked
variable-parameter pruning, which progressively prunes less informative
variables and attention parameters through quantile-based masking; (2)
prioritized variable-parameter replay, which replays low-loss past samples to
preserve learned knowledge for model stability; (3) dynamic extrapolation
mechanism, which propagates information from variables selected for the input
to all other variables via learnable spatial embeddings and adjacency
information. Experiments on five real-world datasets show that our work
significantly outperforms the state-of-the-art baselines in both accuracy and
efficiency, demonstrating the effectiveness of joint variable selection and
model optimization.

</details>


### [66] [GraphNet: A Large-Scale Computational Graph Dataset for Tensor Compiler Research](https://arxiv.org/abs/2510.24035)
*Xinqi Li,Yiqun Liu,Shan Jiang,Enrong Zheng,Huaijin Zheng,Wenhao Dai,Haodong Deng,Dianhai Yu,Yanjun Ma*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce GraphNet, a dataset of 2.7K real-world deep learning
computational graphs with rich metadata, spanning six major task categories
across multiple deep learning frameworks. To evaluate tensor compiler
performance on these samples, we propose the benchmark metric Speedup Score
S(t), which jointly considers runtime speedup and execution correctness under
tunable tolerance levels, offering a reliable measure of general optimization
capability. Furthermore, we extend S(t) to the Error-aware Speedup Score ES(t),
which incorporates error information and helps compiler developers identify key
performance bottlenecks. In this report, we benchmark the default tensor
compilers, CINN for PaddlePaddle and TorchInductor for PyTorch, on computer
vision (CV) and natural language processing (NLP) samples to demonstrate the
practicality of GraphNet. The full construction pipeline with graph extraction
and compiler evaluation tools is available at
https://github.com/PaddlePaddle/GraphNet .

</details>


### [67] [Localized Kernel Projection Outlyingness: A Two-Stage Approach for Multi-Modal Outlier Detection](https://arxiv.org/abs/2510.24043)
*Akira Tamamori*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents Two-Stage LKPLO, a novel multi-stage outlier detection
framework that overcomes the coexisting limitations of conventional
projection-based methods: their reliance on a fixed statistical metric and
their assumption of a single data structure. Our framework uniquely synthesizes
three key concepts: (1) a generalized loss-based outlyingness measure (PLO)
that replaces the fixed metric with flexible, adaptive loss functions like our
proposed SVM-like loss; (2) a global kernel PCA stage to linearize non-linear
data structures; and (3) a subsequent local clustering stage to handle
multi-modal distributions. Comprehensive 5-fold cross-validation experiments on
10 benchmark datasets, with automated hyperparameter optimization, demonstrate
that Two-Stage LKPLO achieves state-of-the-art performance. It significantly
outperforms strong baselines on datasets with challenging structures where
existing methods fail, most notably on multi-cluster data (Optdigits) and
complex, high-dimensional data (Arrhythmia). Furthermore, an ablation study
empirically confirms that the synergistic combination of both the kernelization
and localization stages is indispensable for its superior performance. This
work contributes a powerful new tool for a significant class of outlier
detection problems and underscores the importance of hybrid, multi-stage
architectures.

</details>


### [68] [Mitigating Negative Transfer via Reducing Environmental Disagreement](https://arxiv.org/abs/2510.24044)
*Hui Sun,Zheng Xie,Hao-Yuan He,Ming Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Unsupervised Domain Adaptation~(UDA) focuses on transferring knowledge from a
labeled source domain to an unlabeled target domain, addressing the challenge
of \emph{domain shift}. Significant domain shifts hinder effective knowledge
transfer, leading to \emph{negative transfer} and deteriorating model
performance. Therefore, mitigating negative transfer is essential. This study
revisits negative transfer through the lens of causally disentangled learning,
emphasizing cross-domain discriminative disagreement on non-causal
environmental features as a critical factor. Our theoretical analysis reveals
that overreliance on non-causal environmental features as the environment
evolves can cause discriminative disagreements~(termed \emph{environmental
disagreement}), thereby resulting in negative transfer. To address this, we
propose Reducing Environmental Disagreement~(RED), which disentangles each
sample into domain-invariant causal features and domain-specific non-causal
environmental features via adversarially training domain-specific environmental
feature extractors in the opposite domains. Subsequently, RED estimates and
reduces environmental disagreement based on domain-specific non-causal
environmental features. Experimental results confirm that RED effectively
mitigates negative transfer and achieves state-of-the-art performance.

</details>


### [69] [Causal-Aware Generative Adversarial Networks with Reinforcement Learning](https://arxiv.org/abs/2510.24046)
*Tu Anh Hoang Nguyen,Dang Nguyen,Tri-Nhan Vo,Thuc Duy Le,Sunil Gupta*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The utility of tabular data for tasks ranging from model training to
large-scale data analysis is often constrained by privacy concerns or
regulatory hurdles. While existing data generation methods, particularly those
based on Generative Adversarial Networks (GANs), have shown promise, they
frequently struggle with capturing complex causal relationship, maintaining
data utility, and providing provable privacy guarantees suitable for enterprise
deployment. We introduce CA-GAN, a novel generative framework specifically
engineered to address these challenges for real-world tabular datasets. CA-GAN
utilizes a two-step approach: causal graph extraction to learn a robust,
comprehensive causal relationship in the data's manifold, followed by a custom
Conditional WGAN-GP (Wasserstein GAN with Gradient Penalty) that operates
exclusively as per the structure of nodes in the causal graph. More
importantly, the generator is trained with a new Reinforcement Learning-based
objective that aligns the causal graphs constructed from real and fake data,
ensuring the causal awareness in both training and sampling phases. We
demonstrate CA-GAN superiority over six SOTA methods across 14 tabular
datasets. Our evaluations, focused on core data engineering metrics: causal
preservation, utility preservation, and privacy preservation. Our method offers
a practical, high-performance solution for data engineers seeking to create
high-quality, privacy-compliant synthetic datasets to benchmark database
systems, accelerate software development, and facilitate secure data-driven
research.

</details>


### [70] [Low-N Protein Activity Optimization with FolDE](https://arxiv.org/abs/2510.24053)
*Jacob B. Roberts,Catherine R. Ji,Isaac Donnell,Thomas D. Young,Allison N. Pearson,Graham A. Hudson,Leah S. Keiser,Mia Wesselkamper,Peter H. Winegar,Janik Ludwig,Sarah H. Klass,Isha V. Sheth,Ezechinyere C. Ukabiala,Maria C. T. Astolfi,Benjamin Eysenbach,Jay D. Keasling*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Proteins are traditionally optimized through the costly construction and
measurement of many mutants. Active Learning-assisted Directed Evolution (ALDE)
alleviates that cost by predicting the best improvements and iteratively
testing mutants to inform predictions. However, existing ALDE methods face a
critical limitation: selecting the highest-predicted mutants in each round
yields homogeneous training data insufficient for accurate prediction models in
subsequent rounds. Here we present FolDE, an ALDE method designed to maximize
end-of-campaign success. In simulations across 20 protein targets, FolDE
discovers 23% more top 10% mutants than the best baseline ALDE method (p=0.005)
and is 55% more likely to find top 1% mutants. FolDE achieves this primarily
through naturalness-based warm-starting, which augments limited activity
measurements with protein language model outputs to improve activity
prediction. We also introduce a constant-liar batch selector, which improves
batch diversity; this is important in multi-mutation campaigns but had limited
effect in our benchmarks. The complete workflow is freely available as
open-source software, making efficient protein optimization accessible to any
laboratory.

</details>


### [71] [Information-Theoretic Discrete Diffusion](https://arxiv.org/abs/2510.24088)
*Moongyu Jeon,Sangwoo Shin,Dongjae Jeon,Albert No*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present an information-theoretic framework for discrete diffusion models
that yields principled estimators of log-likelihood using score-matching
losses. Inspired by the I-MMSE identity for the Gaussian setup, we derive
analogous results for the discrete setting. Specifically, we introduce the
Information-Minimum Denoising Score Entropy (I-MDSE) relation, which links
mutual information between data and its diffused version to the minimum
denoising score entropy (DSE) loss. We extend this theory to masked diffusion
and establish the Information-Minimum Denoising Cross-Entropy (I-MDCE)
relation, connecting cross-entropy losses to mutual information in discrete
masked processes. These results provide a time-integral decomposition of the
log-likelihood of the data in terms of optimal score-based losses, showing that
commonly used losses such as DSE and DCE are not merely variational bounds but
tight and principled estimators of log-likelihood. The I-MDCE decomposition
further enables practical extensions, including time-free formula, conditional
likelihood estimation in prompt-response tasks, and coupled Monte Carlo
estimation of likelihood ratios. Experiments on synthetic and real-world data
confirm the accuracy, variance stability, and utility of our estimators. The
code is publicly available at https://github.com/Dongjae0324/infodis.

</details>


### [72] [Learning Parameterized Skills from Demonstrations](https://arxiv.org/abs/2510.24095)
*Vedant Gupta,Haotian Fu,Calvin Luo,Yiding Jiang,George Konidaris*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present DEPS, an end-to-end algorithm for discovering parameterized skills
from expert demonstrations. Our method learns parameterized skill policies
jointly with a meta-policy that selects the appropriate discrete skill and
continuous parameters at each timestep. Using a combination of temporal
variational inference and information-theoretic regularization methods, we
address the challenge of degeneracy common in latent variable models, ensuring
that the learned skills are temporally extended, semantically meaningful, and
adaptable. We empirically show that learning parameterized skills from
multitask expert demonstrations significantly improves generalization to unseen
tasks. Our method outperforms multitask as well as skill learning baselines on
both LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers
interpretable parameterized skills, such as an object grasping skill whose
continuous arguments define the grasp location.

</details>


### [73] [Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.24120)
*Ziyu Liu,Yijing Liu,Jianfei Yuan,Minzhi Yan,Le Yue,Honghui Xiong,Yi Yang*

Main category: cs.LG

TL;DR: G2ConS通过图引导的概念选择，结合无LLM成本的概念图，降低KG构建成本并保持高效检索与问答质量。


<details>
  <summary>Details</summary>
Motivation: 解决基于检索的LLM问答在大规模文本上需要大量LLM调用以抽取实体和关系，成本高昂的问题；发现某些词语（概念）及其相关文档更重要，需要高效的概念层筛选来减少计算开销。

Method: 提出两大组成：1) chunk selection method（块选择）用于筛选 salient 文档块以降低KG构建成本；2) LLM无关的概念图用于弥补因块选择带来的知识空缺，且实现零成本增补。整体流程尽量减少LLM调用。

Result: 在多个真实数据集上，G2ConS在构建成本、检索效果和问答质量方面超过所有基线。

Conclusion: Graph-Guided Concept Selection 能显著降低构建成本并维持或提升检索与回答质量，特别适用于需要多跳推理且文档规模大的领域。

Abstract: Graph-based RAG constructs a knowledge graph (KG) from text chunks to enhance
retrieval in Large Language Model (LLM)-based question answering. It is
especially beneficial in domains such as biomedicine, law, and political
science, where effective retrieval often involves multi-hop reasoning over
proprietary documents. However, these methods demand numerous LLM calls to
extract entities and relations from text chunks, incurring prohibitive costs at
scale. Through a carefully designed ablation study, we observe that certain
words (termed concepts) and their associated documents are more important.
Based on this insight, we propose Graph-Guided Concept Selection (G2ConS). Its
core comprises a chunk selection method and an LLM-independent concept graph.
The former selects salient document chunks to reduce KG construction costs; the
latter closes knowledge gaps introduced by chunk selection at zero cost.
Evaluations on multiple real-world datasets show that G2ConS outperforms all
baselines in construction cost, retrieval effectiveness, and answering quality.

</details>


### [74] [Causal Convolutional Neural Networks as Finite Impulse Response Filters](https://arxiv.org/abs/2510.24125)
*Kiran Bacsa,Wei Liu,Xudong Jian,Huangbin Liang,Eleni Chatzi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study investigates the behavior of Causal Convolutional Neural Networks
(CNNs) with quasi-linear activation functions when applied to time-series data
characterized by multimodal frequency content. We demonstrate that, once
trained, such networks exhibit properties analogous to Finite Impulse Response
(FIR) filters, particularly when the convolutional kernels are of extended
length exceeding those typically employed in standard CNN architectures. Causal
CNNs are shown to capture spectral features both implicitly and explicitly,
offering enhanced interpretability for tasks involving dynamic systems.
Leveraging the associative property of convolution, we further show that the
entire network can be reduced to an equivalent single-layer filter resembling
an FIR filter optimized via least-squares criteria. This equivalence yields new
insights into the spectral learning behavior of CNNs trained on signals with
sparse frequency content. The approach is validated on both simulated beam
dynamics and real-world bridge vibration datasets, underlining its relevance
for modeling and identifying physical systems governed by dynamic responses.

</details>


### [75] [Fixed Point Neural Acceleration and Inverse Surrogate Model for Battery Parameter Identification](https://arxiv.org/abs/2510.24135)
*Hojin Cheon,Hyeongseok Seo,Jihun Jeon,Wooju Lee,Dohyun Jeong,Hongseok Kim*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid expansion of electric vehicles has intensified the need for
accurate and efficient diagnosis of lithium-ion batteries. Parameter
identification of electrochemical battery models is widely recognized as a
powerful method for battery health assessment. However, conventional
metaheuristic approaches suffer from high computational cost and slow
convergence, and recent machine learning methods are limited by their reliance
on constant current data, which may not be available in practice. To overcome
these challenges, we propose deep learning-based framework for parameter
identification of electrochemical battery models. The proposed framework
combines a neural surrogate model of the single particle model with electrolyte
(NeuralSPMe) and a deep learning-based fixed-point iteration method. NeuralSPMe
is trained on realistic EV load profiles to accurately predict lithium
concentration dynamics under dynamic operating conditions while a parameter
update network (PUNet) performs fixed-point iterative updates to significantly
reduce both the evaluation time per sample and the overall number of iterations
required for convergence. Experimental evaluations demonstrate that the
proposed framework accelerates the parameter identification by more than 2000
times, achieves superior sample efficiency and more than 10 times higher
accuracy compared to conventional metaheuristic algorithms, particularly under
dynamic load scenarios encountered in practical applications.

</details>


### [76] [Identifiable learning of dissipative dynamics](https://arxiv.org/abs/2510.24160)
*Aiqing Zhu,Beatrice W. Soh,Grigorios A. Pavliotis,Qianxiao Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Complex dissipative systems appear across science and engineering, from
polymers and active matter to learning algorithms. These systems operate far
from equilibrium, where energy dissipation and time irreversibility are key to
their behavior, but are difficult to quantify from data. Learning accurate and
interpretable models of such dynamics remains a major challenge: the models
must be expressive enough to describe diverse processes, yet constrained enough
to remain physically meaningful and mathematically identifiable. Here, we
introduce I-OnsagerNet, a neural framework that learns dissipative stochastic
dynamics directly from trajectories while ensuring both interpretability and
uniqueness. I-OnsagerNet extends the Onsager principle to guarantee that the
learned potential is obtained from the stationary density and that the drift
decomposes cleanly into time-reversible and time-irreversible components, as
dictated by the Helmholtz decomposition. Our approach enables us to calculate
the entropy production and to quantify irreversibility, offering a principled
way to detect and quantify deviations from equilibrium. Applications to polymer
stretching in elongational flow and to stochastic gradient Langevin dynamics
reveal new insights, including super-linear scaling of barrier heights and
sub-linear scaling of entropy production rates with the strain rate, and the
suppression of irreversibility with increasing batch size. I-OnsagerNet thus
establishes a general, data-driven framework for discovering and interpreting
non-equilibrium dynamics.

</details>


### [77] [V-SAT: Video Subtitle Annotation Tool](https://arxiv.org/abs/2510.24180)
*Arpita Kundu,Joyita Chakraborty,Anindita Desarkar,Aritra Sen,Srushti Anil Patil,Vishwanathan Raman*

Main category: cs.LG

TL;DR: 提出V-SAT，一体化字幕注释工具，通过融合LLMs、VLMs、图像处理与ASR，自动检测并纠正多类字幕质量问题，显著提升字幕质量并通过人机协作确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前流媒体及社交平台的视频字幕需求日益增长，但现有字幕生成方法多为纯语音转写或OCR提取，存在同步性差、文本错误/有害内容、格式不一致、阅读速度不恰当等问题。此外，修正工作往往需大量的后期人工操作，成本高且效率低。需一个统一框架来解决跨模态的字幕质量问题并减轻后期编辑负担。

Method: 提出V-SAT（视频字幕注释工具），利用大语言模型（LLMs）与视觉语言模型（VLMs）、图像处理和自动语音识别（ASR）的融合，结合音视频的上下文线索，自动检测并纠正字幕中的语言层面与图像层面的问题。系统包含语言与图像两类模式的纠错，且引入人工闭环验证以保障高质量输出。

Result: 在解决所有语言模式问题后，SUBER分数从9.6降至3.54；图像模式问题的F1分数约为0.80，表明在图像相关错误的检测和纠正方面具有较高准确性。

Conclusion: V-SAT提供了首个全面解决鲁棒字幕注释的统一框架，结合多模态信息与人机协作，显著提升字幕注释的整体质量和可靠性。

Abstract: The surge of audiovisual content on streaming platforms and social media has
heightened the demand for accurate and accessible subtitles. However, existing
subtitle generation methods primarily speech-based transcription or OCR-based
extraction suffer from several shortcomings, including poor synchronization,
incorrect or harmful text, inconsistent formatting, inappropriate reading
speeds, and the inability to adapt to dynamic audio-visual contexts. Current
approaches often address isolated issues, leaving post-editing as a
labor-intensive and time-consuming process. In this paper, we introduce V-SAT
(Video Subtitle Annotation Tool), a unified framework that automatically
detects and corrects a wide range of subtitle quality issues. By combining
Large Language Models(LLMs), Vision-Language Models (VLMs), Image Processing,
and Automatic Speech Recognition (ASR), V-SAT leverages contextual cues from
both audio and video. Subtitle quality improved, with the SUBER score reduced
from 9.6 to 3.54 after resolving all language mode issues and F1-scores of
~0.80 for image mode issues. Human-in-the-loop validation ensures high-quality
results, providing the first comprehensive solution for robust subtitle
annotation.

</details>


### [78] [SPEAR++: Scaling Gradient Inversion via Sparsely-Used Dictionary Learning](https://arxiv.org/abs/2510.24200)
*Alexander Bakarsky,Dimitar I. Dimitrov,Maximilian Baader,Martin Vechev*

Main category: cs.LG

TL;DR: SPEAR++ improves gradient inversion attack practicality in federated learning by using sparsely-used dictionary learning, enabling attacks on much larger batch sizes while staying robust to DP noise and FedAvg.


<details>
  <summary>Details</summary>
Motivation: 桥接理论攻击与现实场景之间的差距；评估联邦学习中的隐私脆弱性，特别是在对线性层+ReLU的梯度反演攻击方面，提升对更大批量数据的可行性。

Method: 将稀疏字典学习等先进技术引入SPEAR框架，对线性层+ReLU的梯度反演问题进行求解，使其在更大批量（b）下仍可行，同时保持对差分隐私噪声和FedAvg聚合的鲁棒性。

Result: 攻击现在可应用于约10倍于原始SPEAR的批量规模，同时保留对DP噪声和FedAvg聚合的鲁棒性，体现了方法的实用性提升。

Conclusion: SPEAR++使梯度反演攻击在联邦学习中的应用更具现实性，揭示隐私风险仍然存在，需要进一步研究防护措施与鲁棒的隐私保护机制。

Abstract: Federated Learning has seen an increased deployment in real-world scenarios
recently, as it enables the distributed training of machine learning models
without explicit data sharing between individual clients. Yet, the introduction
of the so-called gradient inversion attacks has fundamentally challenged its
privacy-preserving properties. Unfortunately, as these attacks mostly rely on
direct data optimization without any formal guarantees, the vulnerability of
real-world systems remains in dispute and requires tedious testing for each new
federated deployment. To overcome these issues, recently the SPEAR attack was
introduced, which is based on a theoretical analysis of the gradients of linear
layers with ReLU activations. While SPEAR is an important theoretical
breakthrough, the attack's practicality was severely limited by its exponential
runtime in the batch size b. In this work, we fill this gap by applying
State-of-the-Art techniques from Sparsely-Used Dictionary Learning to make the
problem of gradient inversion on linear layers with ReLU activations tractable.
Our experiments demonstrate that our new attack, SPEAR++, retains all desirable
properties of SPEAR, such as robustness to DP noise and FedAvg aggregation,
while being applicable to 10x bigger batch sizes.

</details>


### [79] [Unlocking Out-of-Distribution Generalization in Dynamics through Physics-Guided Augmentation](https://arxiv.org/abs/2510.24216)
*Fan Xu,Hao Wu,Kun Wang,Nan Wang,Qingsong Wen,Xian Wu,Wei Gong,Xibin Zhao*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In dynamical system modeling, traditional numerical methods are limited by
high computational costs, while modern data-driven approaches struggle with
data scarcity and distribution shifts. To address these fundamental
limitations, we first propose SPARK, a physics-guided quantitative augmentation
plugin. Specifically, SPARK utilizes a reconstruction autoencoder to integrate
physical parameters into a physics-rich discrete state dictionary. This state
dictionary then acts as a structured dictionary of physical states, enabling
the creation of new, physically-plausible training samples via principled
interpolation in the latent space. Further, for downstream prediction, these
augmented representations are seamlessly integrated with a Fourier-enhanced
Graph ODE, a combination designed to robustly model the enriched data
distribution while capturing long-term temporal dependencies. Extensive
experiments on diverse benchmarks demonstrate that SPARK significantly
outperforms state-of-the-art baselines, particularly in challenging
out-of-distribution scenarios and data-scarce regimes, proving the efficacy of
our physics-guided augmentation paradigm.

</details>


### [80] [Closing Gaps: An Imputation Analysis of ICU Vital Signs](https://arxiv.org/abs/2510.24217)
*Alisher Turubayev,Anna Shopova,Fabian Lange,Mahmut Kamalak,Paul Mattes,Victoria Ayvasky,Bert Arnrich,Bjarne Pfitzner,Robin P. van de Water*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As more Intensive Care Unit (ICU) data becomes available, the interest in
developing clinical prediction models to improve healthcare protocols
increases. However, the lack of data quality still hinders clinical prediction
using Machine Learning (ML). Many vital sign measurements, such as heart rate,
contain sizeable missing segments, leaving gaps in the data that could
negatively impact prediction performance. Previous works have introduced
numerous time-series imputation techniques. Nevertheless, more comprehensive
work is needed to compare a representative set of methods for imputing ICU
vital signs and determine the best practice. In reality, ad-hoc imputation
techniques that could decrease prediction accuracy, like zero imputation, are
still used. In this work, we compare established imputation techniques to guide
researchers in improving the performance of clinical prediction models by
selecting the most accurate imputation technique. We introduce an extensible
and reusable benchmark with currently 15 imputation and 4 amputation methods,
created for benchmarking on major ICU datasets. We hope to provide a
comparative basis and facilitate further ML development to bring more models
into clinical practice.

</details>


### [81] [PRIVET: Privacy Metric Based on Extreme Value Theory](https://arxiv.org/abs/2510.24233)
*Antoine Szatkownik,Aurélien Decelle,Beatriz Seoane,Nicolas Bereux,Léo Planche,Guillaume Charpiat,Burak Yelmen,Flora Jay,Cyril Furtlehner*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep generative models are often trained on sensitive data, such as genetic
sequences, health data, or more broadly, any copyrighted, licensed or protected
content. This raises critical concerns around privacy-preserving synthetic
data, and more specifically around privacy leakage, an issue closely tied to
overfitting. Existing methods almost exclusively rely on global criteria to
estimate the risk of privacy failure associated to a model, offering only
quantitative non interpretable insights. The absence of rigorous evaluation
methods for data privacy at the sample-level may hinder the practical
deployment of synthetic data in real-world applications. Using extreme value
statistics on nearest-neighbor distances, we propose PRIVET, a generic
sample-based, modality-agnostic algorithm that assigns an individual privacy
leak score to each synthetic sample. We empirically demonstrate that PRIVET
reliably detects instances of memorization and privacy leakage across diverse
data modalities, including settings with very high dimensionality, limited
sample sizes such as genetic data and even under underfitting regimes. We
compare our method to existing approaches under controlled settings and show
its advantage in providing both dataset level and sample level assessments
through qualitative and quantitative outputs. Additionally, our analysis
reveals limitations in existing computer vision embeddings to yield
perceptually meaningful distances when identifying near-duplicate samples.

</details>


### [82] [Sparse Optimistic Information Directed Sampling](https://arxiv.org/abs/2510.24234)
*Ludovic Schwartz,Hamish Flynn,Gergely Neu*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Many high-dimensional online decision-making problems can be modeled as
stochastic sparse linear bandits. Most existing algorithms are designed to
achieve optimal worst-case regret in either the data-rich regime, where
polynomial depen- dence on the ambient dimension is unavoidable, or the
data-poor regime, where dimension-independence is possible at the cost of worse
dependence on the num- ber of rounds. In contrast, the sparse Information
Directed Sampling (IDS) algo- rithm satisfies a Bayesian regret bound that has
the optimal rate in both regimes simultaneously. In this work, we explore the
use of Sparse Optimistic Informa- tion Directed Sampling (SOIDS) to achieve the
same adaptivity in the worst-case setting, without Bayesian assumptions.
Through a novel analysis that enables the use of a time-dependent learning
rate, we show that SOIDS can optimally balance information and regret. Our
results extend the theoretical guarantees of IDS, pro- viding the first
algorithm that simultaneously achieves optimal worst-case regret in both the
data-rich and data-poor regimes. We empirically demonstrate the good
performance of SOIDS.

</details>


### [83] [PaTaRM: Bridging Pairwise and Pointwise Signals via Preference-Aware Task-Adaptive Reward Modeling](https://arxiv.org/abs/2510.24235)
*Ai Jian,Jingqing Ruan,Xing Ma,Dailin Li,QianLin Zhou,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reward models (RMs) are central to reinforcement learning from human feedback
(RLHF), providing the critical supervision signals that align large language
models (LLMs) with human preferences. While generative reward models (GRMs)
offer greater interpretability than traditional scalar RMs, current training
paradigms remain limited. Pair-wise methods rely on binary good-versus-bad
labels, which cause mismatches for point-wise inference and necessitate complex
pairing strategies for effective application in RLHF. On the other hand,
point-wise methods require more elaborate absolute labeling with rubric-driven
criteria, resulting in poor adaptability and high annotation costs. In this
work, we propose the Preference-Aware Task-Adaptive Reward Model (PaTaRM), a
unified framework that integrates a preference-aware reward (PAR) mechanism
with dynamic rubric adaptation. PaTaRM leverages relative preference
information from pairwise data to construct robust point-wise training signals,
eliminating the need for explicit point-wise labels. Simultaneously, it employs
a task-adaptive rubric system that flexibly generates evaluation criteria for
both global task consistency and instance-specific fine-grained reasoning. This
design enables efficient, generalizable, and interpretable reward modeling for
RLHF. Extensive experiments show that PaTaRM achieves an average relative
improvement of 4.7% on RewardBench and RMBench across Qwen3-8B and Qwen3-14B
models. Furthermore, PaTaRM boosts downstream RLHF performance, with an average
improvement of 13.6% across IFEval and InFoBench benchmarks, confirming its
effectiveness and robustness. Our code is available at
https://github.com/JaneEyre0530/PaTaRM.

</details>


### [84] [Temporal Knowledge Graph Hyperedge Forecasting: Exploring Entity-to-Category Link Prediction](https://arxiv.org/abs/2510.24240)
*Edward Markai,Sina Molavipour*

Main category: cs.LG

TL;DR: 在 TLogic 规则框架上引入实体类别并探讨数据驱动的类别生成与聚合策略，从而在保持可解释性的同时提升时态知识图谱的预测性能。


<details>
  <summary>Details</summary>
Motivation: 时态知识图谱的预测潜力巨大，但现有大多方法是黑箱，难以解释。需要一个可解释的规则基方法来对预测过程透明化，并在类别信息缺失时仍保持性能。

Method: 提出扩展的规则格式，引入实体类别作为关键组件以限制规则应用；在类别未知时，使用数据驱动的LLM-基方法生成类别；比较并分析用于聚合检索实体分数的不同聚合策略。

Result: 所提出的方法在保持可解释性的同时达到高准确性，规则框架对预测具有良好透明性，类别生成和聚合策略的探索为实际应用提供有效选择。

Conclusion: 通过将类别信息融入规则框架并结合数据驱动的类别生成与聚合策略，获得可解释且高效的时态知识图谱预测；方法具有潜在的实际应用价值。

Abstract: Temporal Knowledge Graphs have emerged as a powerful way of not only modeling
static relationships between entities but also the dynamics of how relations
evolve over time. As these informational structures can be used to store
information from a real-world setting, such as a news flow, predicting future
graph components to a certain extent equates predicting real-world events. Most
of the research in this field focuses on embedding-based methods, often
leveraging convolutional neural net architectures. These solutions act as black
boxes, limiting insight. In this paper, we explore an extension to an
established rule-based framework, TLogic, that yields a high accuracy in
combination with explainable predictions. This offers transparency and allows
the end-user to critically evaluate the rules applied at the end of the
prediction stage. The new rule format incorporates entity category as a key
component with the purpose of limiting rule application only to relevant
entities. When categories are unknown for building the graph, we propose a
data-driven method to generate them with an LLM-based approach. Additionally,
we investigate the choice of aggregation method for scores of retrieved
entities when performing category prediction.

</details>


### [85] [SALS: Sparse Attention in Latent Space for KV cache Compression](https://arxiv.org/abs/2510.24273)
*Junlin Mu,Hantao Huang,Jihang Zhang,Minghui Yu,Tao Wang,Yidong Li*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models capable of handling extended contexts are in high
demand, yet their inference remains challenging due to substantial Key-Value
cache size and high memory bandwidth requirements. Previous research has
demonstrated that KV cache exhibits low-rank characteristics within the hidden
dimension, suggesting the potential for effective compression. However, due to
the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive
low-rank compression suffers severe accuracy degradation or creates a new speed
bottleneck, as the low-rank cache must first be reconstructed in order to apply
RoPE. In this paper, we introduce two key insights: first, the application of
RoPE to the key vectors increases their variance, which in turn results in a
higher rank; second, after the key vectors are transformed into the latent
space, they largely maintain their representation across most layers. Based on
these insights, we propose the Sparse Attention in Latent Space framework. SALS
projects the KV cache into a compact latent space via low-rank projection, and
performs sparse token selection using RoPE-free query-key interactions in this
space. By reconstructing only a small subset of important tokens, it avoids the
overhead of full KV cache reconstruction. We comprehensively evaluate SALS on
various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and
additionally verify its scalability on the RULER-128k benchmark with
LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA
performance by maintaining competitive accuracy. Under different settings, SALS
achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention
operator compared to FlashAttention2 on the 4K sequence. For the end-to-end
throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared
to GPT-fast on 4k and 32K sequences, respectively.

</details>


### [86] [EDC: Equation Discovery for Classification](https://arxiv.org/abs/2510.24310)
*Guus Toussaint,Arno Knobbe*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Equation Discovery techniques have shown considerable success in regression
tasks, where they are used to discover concise and interpretable models
(\textit{Symbolic Regression}). In this paper, we propose a new ED-based binary
classification framework. Our proposed method EDC finds analytical functions of
manageable size that specify the location and shape of the decision boundary.
In extensive experiments on artificial and real-life data, we demonstrate how
EDC is able to discover both the structure of the target equation as well as
the value of its parameters, outperforming the current state-of-the-art
ED-based classification methods in binary classification and achieving
performance comparable to the state of the art in binary classification. We
suggest a grammar of modest complexity that appears to work well on the tested
datasets but argue that the exact grammar -- and thus the complexity of the
models -- is configurable, and especially domain-specific expressions can be
included in the pattern language, where that is required. The presented grammar
consists of a series of summands (additive terms) that include linear,
quadratic and exponential terms, as well as products of two features (producing
hyperbolic curves ideal for capturing XOR-like dependencies). The experiments
demonstrate that this grammar allows fairly flexible decision boundaries while
not so rich to cause overfitting.

</details>


### [87] [Transformers can do Bayesian Clustering](https://arxiv.org/abs/2510.24318)
*Prajit Bhaskaran,Tom Viering*

Main category: cs.LG

TL;DR: Cluster-PFN是一个基于Transformer的无监督贝叶斯聚类模型，扩展PFN以处理缺失数据，能在合成GMM先验上训练，估计簇数与簇分配的后验，速度远超VI和传统准则。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯聚类在实际大规模数据上的计算成本以及缺失数据带来的不确定性问题；现有方法在规模、复杂先验或缺失数据处理方面存在瓶颈。

Method: 在Prior-Data Fitted Networks( PFN )框架上引入Transformer，使用从有限高斯混合模型(GMM)先验生成的合成数据进行训练，学习输出簇数与簇分配的后验分布，支持带缺失数据的复杂先验。

Result: 簇数估计比AIC、BIC和变分推断(VI)等手工模型选择更准确；聚类质量与VI相当且在速度上大幅领先；在高缺失真实基因组数据上，优于基于插补的基线，显示出良好鲁棒性与可扩展性。

Conclusion: Cluster-PFN提供了一个可扩展、灵活的贝叶斯聚类框架，能在含缺失数据的现实数据集上实现高效而可靠的聚类，且对复杂先验具备良好适应性。

Abstract: Bayesian clustering accounts for uncertainty but is computationally demanding
at scale. Furthermore, real-world datasets often contain missing values, and
simple imputation ignores the associated uncertainty, resulting in suboptimal
results. We present Cluster-PFN, a Transformer-based model that extends
Prior-Data Fitted Networks (PFNs) to unsupervised Bayesian clustering. Trained
entirely on synthetic datasets generated from a finite Gaussian Mixture Model
(GMM) prior, Cluster-PFN learns to estimate the posterior distribution over
both the number of clusters and the cluster assignments. Our method estimates
the number of clusters more accurately than handcrafted model selection
procedures such as AIC, BIC and Variational Inference (VI), and achieves
clustering quality competitive with VI while being orders of magnitude faster.
Cluster-PFN can be trained on complex priors that include missing data,
outperforming imputation-based baselines on real-world genomic datasets, at
high missingness. These results show that the Cluster-PFN can provide scalable
and flexible Bayesian clustering.

</details>


### [88] [What do vision-language models see in the context? Investigating multimodal in-context learning](https://arxiv.org/abs/2510.24331)
*Gabriel O. dos Santos,Esther Colombini,Sandra Avila*

Main category: cs.LG

TL;DR: 系统性研究VLM的ICL，揭示训练数据、提示设计与训练策略对多模态ICL的影响，以及现有模型在视觉–文本整合中的局限。


<details>
  <summary>Details</summary>
Motivation: 尽管ICL在LLMs广泛研究，但在VLM中如何从多模态演示中学习仍不清楚；需要评估不同架构、训练策略和提示设计对VLM ICL的影响，以及分析注意力模式的变化。

Method: 在七个VLM、四种架构、三项图像字幕数据集上进行ICL评估；比较不同提示设计、训练策略（如imag-text交错训练与指令调优）的影响；分析随示例数量增加的注意力分布，以了解多模态整合能力的变化。

Result: IMAGe-Text 交错训练提升ICL但未显著实现视觉–文本的有效整合；指令调优提高指令遵循但可能降低对ICL的依赖，显示对指令对齐与上下文自适应之间的权衡；注意力分析表明VLM多集中于文本线索，未充分利用视觉信息，显现当前多模态整合能力的局限。

Conclusion: 当前VLM在ICL上的能力有限，需在数据、提示设计和训练目标上进一步优化以提升对多模态上下文的学习与适应性。

Abstract: In-context learning (ICL) enables Large Language Models (LLMs) to learn tasks
from demonstration examples without parameter updates. Although it has been
extensively studied in LLMs, its effectiveness in Vision-Language Models (VLMs)
remains underexplored. In this work, we present a systematic study of ICL in
VLMs, evaluating seven models spanning four architectures on three image
captioning benchmarks. We analyze how prompt design, architectural choices, and
training strategies influence multimodal ICL. To our knowledge, we are the
first to analyze how attention patterns in VLMs vary with an increasing number
of in-context demonstrations. Our results reveal that training on imag-text
interleaved data enhances ICL performance but does not imply effective
integration of visual and textual information from demonstration examples. In
contrast, instruction tuning improves instruction-following but can reduce
reliance on in-context demonstrations, suggesting a trade-off between
instruction alignment and in-context adaptation. Attention analyses further
show that current VLMs primarily focus on textual cues and fail to leverage
visual information, suggesting a limited capacity for multimodal integration.
These findings highlight key limitations in the ICL abilities of current VLMs
and provide insights for enhancing their ability to learn from multimodal
in-context examples.

</details>


### [89] [Filtering instances and rejecting predictions to obtain reliable models in healthcare](https://arxiv.org/abs/2510.24368)
*Maria Gabriela Valeriano,David Kohan Marzagão,Alfredo Montelongo,Carlos Roberto Veiga Kiffer,Natan Katz,Ana Carolina Lorena*

Main category: cs.LG

TL;DR: 提出一个两步数据驱动方法：用实例难度过滤训练数据并在推理阶段进行置信度拒绝，以提升医疗数据上的模型可靠性，同时兼顾预测性能和拒绝率。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域，预测的可靠性和对不确定性的处理至关重要；现有模型往往对低置信样本给出预测，需要更注重数据质量和不确定性管理。

Method: 第一步在训练阶段使用实例难度（IH）筛选并清理困难/有问题的样本；第二步在推断阶段引入基于置信度的拒绝机制，只保留置信度高的预测，同时以替代基线比较（影响值、不确定性）评估方法。

Result: 在三个真实医疗数据集上，IH过滤结合置信度拒绝的方法提升了模型的可靠性，同时保持较高的实例保留比例；与影响值、不确定性等基线相比，方法更有效于提升性能并控制拒绝率。

Conclusion: 将IH过滤与置信度拒绝相结合，能有效提升在安全关键应用中的可部署性和鲁棒性。

Abstract: Machine Learning (ML) models are widely used in high-stakes domains such as
healthcare, where the reliability of predictions is critical. However, these
models often fail to account for uncertainty, providing predictions even with
low confidence. This work proposes a novel two-step data-centric approach to
enhance the performance of ML models by improving data quality and filtering
low-confidence predictions. The first step involves leveraging Instance
Hardness (IH) to filter problematic instances during training, thereby refining
the dataset. The second step introduces a confidence-based rejection mechanism
during inference, ensuring that only reliable predictions are retained. We
evaluate our approach using three real-world healthcare datasets, demonstrating
its effectiveness at improving model reliability while balancing predictive
performance and rejection rate. Additionally, we use alternative criteria -
influence values for filtering and uncertainty for rejection - as baselines to
evaluate the efficiency of the proposed method. The results demonstrate that
integrating IH filtering with confidence-based rejection effectively enhances
model performance while preserving a large proportion of instances. This
approach provides a practical method for deploying ML systems in
safety-critical applications.

</details>


### [90] [A Comprehensive Evaluation Framework for Synthetic Trip Data Generation in Public Transport](https://arxiv.org/abs/2510.24375)
*Yuanyuan Wu,Zhenlin Qin,Zhenliang Ma*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Synthetic data offers a promising solution to the privacy and accessibility
challenges of using smart card data in public transport research. Despite rapid
progress in generative modeling, there is limited attention to comprehensive
evaluation, leaving unclear how reliable, safe, and useful synthetic data truly
are. Existing evaluations remain fragmented, typically limited to
population-level representativeness or record-level privacy, without
considering group-level variations or task-specific utility. To address this
gap, we propose a Representativeness-Privacy-Utility (RPU) framework that
systematically evaluates synthetic trip data across three complementary
dimensions and three hierarchical levels (record, group, population). The
framework integrates a consistent set of metrics to quantify similarity,
disclosure risk, and practical usefulness, enabling transparent and balanced
assessment of synthetic data quality. We apply the framework to benchmark
twelve representative generation methods, spanning conventional statistical
models, deep generative networks, and privacy-enhanced variants. Results show
that synthetic data do not inherently guarantee privacy and there is no
"one-size-fits-all" model, the trade-off between privacy and
representativeness/utility is obvious. Conditional Tabular generative
adversarial network (CTGAN) provide the most balanced trade-off and is
suggested for practical applications. The RPU framework provides a systematic
and reproducible basis for researchers and practitioners to compare synthetic
data generation techniques and select appropriate methods in public transport
applications.

</details>


### [91] [Methodology for Comparing Machine Learning Algorithms for Survival Analysis](https://arxiv.org/abs/2510.24473)
*Lucas Buk Cardoso,Simone Aldrey Angelo,Yasmin Pacheco Gil Bonilha,Fernando Maia,Adeylson Guimarães Ribeiro,Maria Paula Curado,Gisele Aparecida Fernandes,Vanderlei Cunha Parro,Flávio Almeida de Magalhães Cipparrone,Alexandre Dias Porto Chiavegatto Filho,Tatiana Natasha Toporcov*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study presents a comparative methodological analysis of six machine
learning models for survival analysis (MLSA). Using data from nearly 45,000
colorectal cancer patients in the Hospital-Based Cancer Registries of S\~ao
Paulo, we evaluated Random Survival Forest (RSF), Gradient Boosting for
Survival Analysis (GBSA), Survival SVM (SSVM), XGBoost-Cox (XGB-Cox),
XGBoost-AFT (XGB-AFT), and LightGBM (LGBM), capable of predicting survival
considering censored data. Hyperparameter optimization was performed with
different samplers, and model performance was assessed using the Concordance
Index (C-Index), C-Index IPCW, time-dependent AUC, and Integrated Brier Score
(IBS). Survival curves produced by the models were compared with predictions
from classification algorithms, and predictor interpretation was conducted
using SHAP and permutation importance. XGB-AFT achieved the best performance
(C-Index = 0.7618; IPCW = 0.7532), followed by GBSA and RSF. The results
highlight the potential and applicability of MLSA to improve survival
prediction and support decision making.

</details>


### [92] [Sample-efficient and Scalable Exploration in Continuous-Time RL](https://arxiv.org/abs/2510.24482)
*Klemens Iten,Lenart Treven,Bhavya Sukhija,Florian Dörfler,Andreas Krause*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning algorithms are typically designed for discrete-time
dynamics, even though the underlying real-world control systems are often
continuous in time. In this paper, we study the problem of continuous-time
reinforcement learning, where the unknown system dynamics are represented using
nonlinear ordinary differential equations (ODEs). We leverage probabilistic
models, such as Gaussian processes and Bayesian neural networks, to learn an
uncertainty-aware model of the underlying ODE. Our algorithm, COMBRL, greedily
maximizes a weighted sum of the extrinsic reward and model epistemic
uncertainty. This yields a scalable and sample-efficient approach to
continuous-time model-based RL. We show that COMBRL achieves sublinear regret
in the reward-driven setting, and in the unsupervised RL setting (i.e., without
extrinsic rewards), we provide a sample complexity bound. In our experiments,
we evaluate COMBRL in both standard and unsupervised RL settings and
demonstrate that it scales better, is more sample-efficient than prior methods,
and outperforms baselines across several deep RL tasks.

</details>


### [93] [MIMIC-Sepsis: A Curated Benchmark for Modeling and Learning from Sepsis Trajectories in the ICU](https://arxiv.org/abs/2510.24500)
*Yong Huang,Zhongqi Yang,Amir Rahmani*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Sepsis is a leading cause of mortality in intensive care units (ICUs), yet
existing research often relies on outdated datasets, non-reproducible
preprocessing pipelines, and limited coverage of clinical interventions. We
introduce MIMIC-Sepsis, a curated cohort and benchmark framework derived from
the MIMIC-IV database, designed to support reproducible modeling of sepsis
trajectories. Our cohort includes 35,239 ICU patients with time-aligned
clinical variables and standardized treatment data, including vasopressors,
fluids, mechanical ventilation and antibiotics. We describe a transparent
preprocessing pipeline-based on Sepsis-3 criteria, structured imputation
strategies, and treatment inclusion-and release it alongside benchmark tasks
focused on early mortality prediction, length-of-stay estimation, and shock
onset classification. Empirical results demonstrate that incorporating
treatment variables substantially improves model performance, particularly for
Transformer-based architectures. MIMIC-Sepsis serves as a robust platform for
evaluating predictive and sequential models in critical care research.

</details>


### [94] [Local Performance vs. Out-of-Distribution Generalization: An Empirical Analysis of Personalized Federated Learning in Heterogeneous Data Environments](https://arxiv.org/abs/2510.24503)
*Mortesa Hussaini,Jan Theiß,Anthony Stein*

Main category: cs.LG

TL;DR: 提出一种以个体化更新为核心的联邦学习方法FLIU，扩展FedAvg，通过在单轮内对不同阶段进行评估来提升对异质数据环境的本地与泛化性能；在MNIST/CIFAR-10的IID、非IID及Dirichlet分布压力测试下进行实验对比。


<details>
  <summary>Details</summary>
Motivation: 在异质数据环境中，本地模型容易收敛到局部最优，FedAvg聚合可能与全局目标不一致，导致客户端漂移。需要同时关注局部性能与对分布外样本的泛化能力，并通过个体化更新减少漂移。

Method: 提出Federated Learning with Individualized Updates (FLIU)，在FedAvg基础上增加一个简单的个体化步骤并引入自适应个性化因子；同时在单轮内对模型更新的不同阶段进行评估分析。

Result: 在MNIST、CIFAR-10上分别在IID、非IID及Dirichlet分布等压力场下进行对比实验，展示了对不同阶段评估框架的价值，以及FLIU在异质性环境中的对局部和泛化性能的影响（摘要未给出具体数值）。

Conclusion: 本研究强调同时评估本地表现与对分布外样本的泛化能力，提出在单轮中对不同阶段进行评估的方法。FLIU提供了一种简单可实现的个体化扩展，在数据异质性环境中有望提升适配性与鲁棒性。

Abstract: In the context of Federated Learning with heterogeneous data environments,
local models tend to converge to their own local model optima during local
training steps, deviating from the overall data distributions. Aggregation of
these local updates, e.g., with FedAvg, often does not align with the global
model optimum (client drift), resulting in an update that is suboptimal for
most clients. Personalized Federated Learning approaches address this challenge
by exclusively focusing on the average local performances of clients' models on
their own data distribution. Generalization to out-of-distribution samples,
which is a substantial benefit of FedAvg and represents a significant component
of robustness, appears to be inadequately incorporated into the assessment and
evaluation processes. This study involves a thorough evaluation of Federated
Learning approaches, encompassing both their local performance and their
generalization capabilities. Therefore, we examine different stages within a
single communication round to enable a more nuanced understanding of the
considered metrics. Furthermore, we propose and incorporate a modified approach
of FedAvg, designated as Federated Learning with Individualized Updates (FLIU),
extending the algorithm by a straightforward individualization step with an
adaptive personalization factor. We evaluate and compare the approaches
empirically using MNIST and CIFAR-10 under various distributional conditions,
including benchmark IID and pathological non-IID, as well as additional novel
test environments with Dirichlet distribution specifically developed to stress
the algorithms on complex data heterogeneity.

</details>


### [95] [LoRA-DA: Data-Aware Initialization for Low-Rank Adaptation via Asymptotic Analysis](https://arxiv.org/abs/2510.24561)
*Qingyue Zhang,Chang Chu,Tianren Peng,Qi Li,Xiangyang Luo,Zhihao Jiang,Shao-Lun Huang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the widespread adoption of LLMs, LoRA has become a dominant method for
PEFT, and its initialization methods have attracted increasing attention.
However, existing methods have notable limitations: many methods do not
incorporate target-domain data, while gradient-based methods exploit data only
at a shallow level by relying on one-step gradient decomposition, which remains
unsatisfactory due to the weak empirical performance of the one-step
fine-tuning model that serves as their basis, as well as the fact that these
methods either lack a rigorous theoretical foundation or depend heavily on
restrictive isotropic assumptions. In this paper, we establish a theoretical
framework for data-aware LoRA initialization based on asymptotic analysis.
Starting from a general optimization objective that minimizes the expectation
of the parameter discrepancy between the fine-tuned and target models, we
derive an optimization problem with two components: a bias term, which is
related to the parameter distance between the fine-tuned and target models, and
is approximated using a Fisher-gradient formulation to preserve anisotropy; and
a variance term, which accounts for the uncertainty introduced by sampling
stochasticity through the Fisher information. By solving this problem, we
obtain an optimal initialization strategy for LoRA. Building on this
theoretical framework, we develop an efficient algorithm, LoRA-DA, which
estimates the terms in the optimization problem from a small set of target
domain samples and obtains the optimal LoRA initialization. Empirical results
across multiple benchmarks demonstrate that LoRA-DA consistently improves final
accuracy over existing initialization methods. Additional studies show faster,
more stable convergence, robustness across ranks, and only a small
initialization overhead for LoRA-DA. The source code will be released upon
publication.

</details>


### [96] [DistDF: Time-Series Forecasting Needs Joint-Distribution Wasserstein Alignment](https://arxiv.org/abs/2510.24574)
*Hao Wang,Licheng Pan,Yuan Lu,Zhixuan Chu,Xiaoxi Li,Shuting He,Zhichao Chen,Haoxuan Li,Qingsong Wen,Zhouchen Lin*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Training time-series forecast models requires aligning the conditional
distribution of model forecasts with that of the label sequence. The standard
direct forecast (DF) approach resorts to minimize the conditional negative
log-likelihood of the label sequence, typically estimated using the mean
squared error. However, this estimation proves to be biased in the presence of
label autocorrelation. In this paper, we propose DistDF, which achieves
alignment by alternatively minimizing a discrepancy between the conditional
forecast and label distributions. Because conditional discrepancies are
difficult to estimate from finite time-series observations, we introduce a
newly proposed joint-distribution Wasserstein discrepancy for time-series
forecasting, which provably upper bounds the conditional discrepancy of
interest. This discrepancy admits tractable, differentiable estimation from
empirical samples and integrates seamlessly with gradient-based training.
Extensive experiments show that DistDF improves the performance diverse
forecast models and achieves the state-of-the-art forecasting performance. Code
is available at https://anonymous.4open.science/r/DistDF-F66B.

</details>


### [97] [Physics-Informed Extreme Learning Machine (PIELM): Opportunities and Challenges](https://arxiv.org/abs/2510.24577)
*He Yang,Fei Ren,Hai-Sui Yu,Xiaohui Chen,Pei-Zhi Zhuang*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We are very delighted to see the fast development of physics-informed extreme
learning machine (PIELM) in recent years for higher computation efficiency and
accuracy in physics-informed machine learning. As a summary or review on PIELM
is currently not available, we would like to take this opportunity to show our
perspective and experience for this promising research direction. We can see
many efforts are made to solve PDEs with sharp gradients, nonlinearities,
high-frequency behavior, hard constraints, uncertainty, multiphysics coupling.
Despite the success, many urgent challenges remain to be tackled, which also
provides us opportunities to develop more robust, interpretable, and
generalizable PIELM frameworks with applications in science and engineering.

</details>


### [98] [Semi-supervised and unsupervised learning for health indicator extraction from guided waves in aerospace composite structures](https://arxiv.org/abs/2510.24614)
*James Josep Perry,Pablo Garcia-Conde Ortiz,George Konstantinou,Cornelie Vergouwen,Edlyn Santha Kumaran,Morteza Moradi*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Health indicators (HIs) are central to diagnosing and prognosing the
condition of aerospace composite structures, enabling efficient maintenance and
operational safety. However, extracting reliable HIs remains challenging due to
variability in material properties, stochastic damage evolution, and diverse
damage modes. Manufacturing defects (e.g., disbonds) and in-service incidents
(e.g., bird strikes) further complicate this process. This study presents a
comprehensive data-driven framework that learns HIs via two learning approaches
integrated with multi-domain signal processing. Because ground-truth HIs are
unavailable, a semi-supervised and an unsupervised approach are proposed: (i) a
diversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approach
augmented with continuous auxiliary labels used as hypothetical damage proxies,
which overcomes the limitation of prior binary labels that only distinguish
healthy and failed states while neglecting intermediate degradation, and (ii) a
degradation-trend-constrained variational autoencoder (DTC-VAE), in which the
monotonicity criterion is embedded via an explicit trend constraint. Guided
waves with multiple excitation frequencies are used to monitor single-stiffener
composite structures under fatigue loading. Time, frequency, and time-frequency
representations are explored, and per-frequency HIs are fused via unsupervised
ensemble learning to mitigate frequency dependence and reduce variance. Using
fast Fourier transform features, the augmented Diversity-DeepSAD model achieved
81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%
performance, outperforming existing baselines.

</details>


### [99] [Symbolic Snapshot Ensembles](https://arxiv.org/abs/2510.24633)
*Mingyue Liu,Andrew Cropper*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. Most
ILP algorithms learn a single hypothesis from a single training run. Ensemble
methods train an ILP algorithm multiple times to learn multiple hypotheses. In
this paper, we train an ILP algorithm only once and save intermediate
hypotheses. We then combine the hypotheses using a minimum description length
weighting scheme. Our experiments on multiple benchmarks, including game
playing and visual reasoning, show that our approach improves predictive
accuracy by 4% with less than 1% computational overhead.

</details>


### [100] [Causal Ordering for Structure Learning From Time Series](https://arxiv.org/abs/2510.24639)
*Pedro P. Sanchez,Damian Machlanski,Steven McDonagh,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Predicting causal structure from time series data is crucial for
understanding complex phenomena in physiology, brain connectivity, climate
dynamics, and socio-economic behaviour. Causal discovery in time series is
hindered by the combinatorial complexity of identifying true causal
relationships, especially as the number of variables and time points grow. A
common approach to simplify the task is the so-called ordering-based methods.
Traditional ordering methods inherently limit the representational capacity of
the resulting model. In this work, we fix this issue by leveraging multiple
valid causal orderings, instead of a single one as standard practice. We
propose DOTS (Diffusion Ordered Temporal Structure), using diffusion-based
causal discovery for temporal data. By integrating multiple orderings, DOTS
effectively recovers the transitive closure of the underlying directed acyclic
graph, mitigating spurious artifacts inherent in single-ordering approaches. We
formalise the problem under standard assumptions such as stationarity and the
additive noise model, and leverage score matching with diffusion processes to
enable efficient Hessian estimation. Extensive experiments validate the
approach. Empirical evaluations on synthetic and real-world datasets
demonstrate that DOTS outperforms state-of-the-art baselines, offering a
scalable and robust approach to temporal causal discovery. On synthetic
benchmarks ($d{=}\!3-\!6$ variables, $T{=}200\!-\!5{,}000$ samples), DOTS
improves mean window-graph $F1$ from $0.63$ (best baseline) to $0.81$. On the
CausalTime real-world benchmark ($d{=}20\!-\!36$), while baselines remain the
best on individual datasets, DOTS attains the highest average summary-graph
$F1$ while halving runtime relative to graph-optimisation methods. These
results establish DOTS as a scalable and accurate solution for temporal causal
discovery.

</details>


### [101] [Pearl: A Foundation Model for Placing Every Atom in the Right Location](https://arxiv.org/abs/2510.24670)
*Genesis Research Team,Alejandro Dobles,Nina Jovic,Kenneth Leidal,Pranav Murugan,David C. Williams,Drausin Wulsin,Nate Gruver,Christina X. Ji,Korrawat Pruegsanusak,Gianluca Scarpellini,Ansh Sharma,Wojciech Swiderski,Andrea Bootsma,Richard Strong Bowen,Charlotte Chen,Jamin Chen,Marc André Dämgen,Roy Tal Dew,Benjamin DiFrancesco,J. D. Fishman,Alla Ivanova,Zach Kagin,David Li-Bland,Zuli Liu,Igor Morozov,Jeffrey Ouyang-Zhang,Frank C. Pickard IV,Kushal S. Shah,Ben Shor,Gabriel Monteiro da Silva,Maxx Tessmer,Carl Tilbury,Cyr Vetcher,Daniel Zeng,Maruan Al-Shedivat,Aleksandra Faust,Evan N. Feinberg,Michael V. LeVine,Matteus Pan*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurately predicting the three-dimensional structures of protein-ligand
complexes remains a fundamental challenge in computational drug discovery that
limits the pace and success of therapeutic design. Deep learning methods have
recently shown strong potential as structural prediction tools, achieving
promising accuracy across diverse biomolecular systems. However, their
performance and utility are constrained by scarce experimental data,
inefficient architectures, physically invalid poses, and the limited ability to
exploit auxiliary information available at inference. To address these issues,
we introduce Pearl (Placing Every Atom in the Right Location), a foundation
model for protein-ligand cofolding at scale. Pearl addresses these challenges
with three key innovations: (1) training recipes that include large-scale
synthetic data to overcome data scarcity; (2) architectures that incorporate an
SO(3)-equivariant diffusion module to inherently respect 3D rotational
symmetries, improving generalization and sample efficiency, and (3)
controllable inference, including a generalized multi-chain templating system
supporting both protein and non-polymeric components as well as dual
unconditional/conditional modes. Pearl establishes a new state-of-the-art
performance in protein-ligand cofolding. On the key metric of generating
accurate (RMSD < 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold
3 and other open source baselines on the public Runs N' Poses and PoseBusters
benchmarks, delivering 14.5% and 14.2% improvements, respectively, over the
next best model. In the pocket-conditional cofolding regime, Pearl delivers
$3.6\times$ improvement on a proprietary set of challenging, real-world drug
targets at the more rigorous RMSD < 1 \r{A} threshold. Finally, we demonstrate
that model performance correlates directly with synthetic dataset size used in
training.

</details>


### [102] [Learning to Drive Safely with Hybrid Options](https://arxiv.org/abs/2510.24674)
*Bram De Cooman,Johan Suykens*

Main category: cs.LG

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Out of the many deep reinforcement learning approaches for autonomous
driving, only few make use of the options (or skills) framework. That is
surprising, as this framework is naturally suited for hierarchical control
applications in general, and autonomous driving tasks in specific. Therefore,
in this work the options framework is applied and tailored to autonomous
driving tasks on highways. More specifically, we define dedicated options for
longitudinal and lateral manoeuvres with embedded safety and comfort
constraints. This way, prior domain knowledge can be incorporated into the
learning process and the learned driving behaviour can be constrained more
easily. We propose several setups for hierarchical control with options and
derive practical algorithms following state-of-the-art reinforcement learning
techniques. By separately selecting actions for longitudinal and lateral
control, the introduced policies over combined and hybrid options obtain the
same expressiveness and flexibility that human drivers have, while being easier
to interpret than classical policies over continuous actions. Of all the
investigated approaches, these flexible policies over hybrid options perform
the best under varying traffic conditions, outperforming the baseline policies
over actions.

</details>
