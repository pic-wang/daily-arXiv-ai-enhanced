<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem](https://arxiv.org/abs/2511.10619)
*Avrim Blum,Marten Garicano,Kavya Ravichandran,Dravyansh Sharma*

Main category: cs.LG

TL;DR: 提出两类参数化的改进型 bandit 算法族，并基于离线数据学习近似最优算法以获得数据相关的样本复杂度保证；在额外的凹性假设下，第一族算法实现对 k 的最优依赖；第二族在良性实例下可实现最佳臂识别，在困难实例下退化到最坏情形，并给出从统计学习视角的强数据相关保证。


<details>
  <summary>Details</summary>
Motivation: 解决在不确定性下分配研究与资源的问题（如新技术研究投入、临床试验、超参数调优等），现有确定性/随机算法存在 Ω(k) 与 Ω(√k) 的下界，使得最坏情形下的近似强度受限。通过参数化算法族并利用离线数据进行学习，可在数据条件充分时获得更强的依赖于问题参数的性能保证。

Method: 提出两类参数化的 bandit 算法族，并对从离线数据中学习靠近最优算法的样本复杂度进行界定：第一族包含此前工作中的最优随机算法，给出在对收益曲线具备额外凹性强度假设时可获得对 k 的更好依赖的算法；第二族在良好实例下可保证最佳臂识别，在差劲实例下退回到最坏情形保证。以统计学习的角度分析 bandit 奖励优化问题，在不需要实际验证假设的情况下实现更强的数据相关保证。

Result: 在离线数据驱动下，能够选择出在不同实例条件下具有更强数据依赖性的近似最优算法；第一族在对收益曲线的凹性属性满足时，对 k 的依赖达到最优；第二族实现对良性与恶性实例的自适应，结合数据驱动的学习可获得更好的样本复杂度界限。

Conclusion: 提供了一种参数化、数据驱动的改进型 bandit 框架，能够在离线数据基础上选择具有更好数据相关保证的算法，并在不同实例条件下自适应地在最佳臂识别与最坏情形保证之间切换，提升了对 improving bandits 的理论与实际应用的适用性与鲁棒性。

Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $Ω(k)$ and $Ω(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.

</details>
